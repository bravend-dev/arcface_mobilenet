{
  "best_global_step": 124000,
  "best_metric": 0.6640577965081277,
  "best_model_checkpoint": "./checkpoints/checkpoint-124000",
  "epoch": 64.71816283924844,
  "eval_steps": 500,
  "global_step": 124000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05219206680584551,
      "grad_norm": 12.981678009033203,
      "learning_rate": 1.4762898896510589e-07,
      "loss": 25.501,
      "step": 100
    },
    {
      "epoch": 0.10438413361169102,
      "grad_norm": 12.856566429138184,
      "learning_rate": 2.9674917983895023e-07,
      "loss": 25.3194,
      "step": 200
    },
    {
      "epoch": 0.15657620041753653,
      "grad_norm": 12.896894454956055,
      "learning_rate": 4.458693707127946e-07,
      "loss": 25.0184,
      "step": 300
    },
    {
      "epoch": 0.20876826722338204,
      "grad_norm": 12.8238525390625,
      "learning_rate": 5.949895615866389e-07,
      "loss": 24.5848,
      "step": 400
    },
    {
      "epoch": 0.2609603340292276,
      "grad_norm": 13.044146537780762,
      "learning_rate": 7.441097524604831e-07,
      "loss": 24.0678,
      "step": 500
    },
    {
      "epoch": 0.31315240083507306,
      "grad_norm": 13.029045104980469,
      "learning_rate": 8.932299433343275e-07,
      "loss": 23.2962,
      "step": 600
    },
    {
      "epoch": 0.3653444676409186,
      "grad_norm": 13.302227020263672,
      "learning_rate": 1.0423501342081718e-06,
      "loss": 22.4483,
      "step": 700
    },
    {
      "epoch": 0.4175365344467641,
      "grad_norm": 13.366641998291016,
      "learning_rate": 1.191470325082016e-06,
      "loss": 21.494,
      "step": 800
    },
    {
      "epoch": 0.4697286012526096,
      "grad_norm": 13.095589637756348,
      "learning_rate": 1.3405905159558605e-06,
      "loss": 20.3598,
      "step": 900
    },
    {
      "epoch": 0.5219206680584552,
      "grad_norm": 13.49484634399414,
      "learning_rate": 1.4897107068297048e-06,
      "loss": 19.2794,
      "step": 1000
    },
    {
      "epoch": 0.5741127348643006,
      "grad_norm": 13.474152565002441,
      "learning_rate": 1.6388308977035492e-06,
      "loss": 18.171,
      "step": 1100
    },
    {
      "epoch": 0.6263048016701461,
      "grad_norm": 13.561841011047363,
      "learning_rate": 1.7879510885773935e-06,
      "loss": 17.1172,
      "step": 1200
    },
    {
      "epoch": 0.6784968684759917,
      "grad_norm": 13.425657272338867,
      "learning_rate": 1.937071279451238e-06,
      "loss": 16.0405,
      "step": 1300
    },
    {
      "epoch": 0.7306889352818372,
      "grad_norm": 13.530472755432129,
      "learning_rate": 2.086191470325082e-06,
      "loss": 15.0367,
      "step": 1400
    },
    {
      "epoch": 0.7828810020876826,
      "grad_norm": 13.315572738647461,
      "learning_rate": 2.2353116611989264e-06,
      "loss": 14.2326,
      "step": 1500
    },
    {
      "epoch": 0.8350730688935282,
      "grad_norm": 13.276067733764648,
      "learning_rate": 2.3844318520727706e-06,
      "loss": 13.4406,
      "step": 1600
    },
    {
      "epoch": 0.8872651356993737,
      "grad_norm": 13.389967918395996,
      "learning_rate": 2.5335520429466153e-06,
      "loss": 12.6964,
      "step": 1700
    },
    {
      "epoch": 0.9394572025052192,
      "grad_norm": 13.317703247070312,
      "learning_rate": 2.6826722338204595e-06,
      "loss": 11.8837,
      "step": 1800
    },
    {
      "epoch": 0.9916492693110647,
      "grad_norm": 12.918367385864258,
      "learning_rate": 2.831792424694304e-06,
      "loss": 11.2477,
      "step": 1900
    },
    {
      "epoch": 1.0438413361169103,
      "grad_norm": 12.918482780456543,
      "learning_rate": 2.9809126155681484e-06,
      "loss": 10.4141,
      "step": 2000
    },
    {
      "epoch": 1.0960334029227556,
      "grad_norm": 12.738448143005371,
      "learning_rate": 3.1300328064419927e-06,
      "loss": 9.8588,
      "step": 2100
    },
    {
      "epoch": 1.1482254697286012,
      "grad_norm": 12.655555725097656,
      "learning_rate": 3.279152997315837e-06,
      "loss": 9.3647,
      "step": 2200
    },
    {
      "epoch": 1.2004175365344467,
      "grad_norm": 12.271310806274414,
      "learning_rate": 3.428273188189681e-06,
      "loss": 9.1114,
      "step": 2300
    },
    {
      "epoch": 1.2526096033402923,
      "grad_norm": 12.29343032836914,
      "learning_rate": 3.577393379063526e-06,
      "loss": 8.704,
      "step": 2400
    },
    {
      "epoch": 1.3048016701461378,
      "grad_norm": 12.176202774047852,
      "learning_rate": 3.72651356993737e-06,
      "loss": 8.3438,
      "step": 2500
    },
    {
      "epoch": 1.3569937369519833,
      "grad_norm": 12.12769889831543,
      "learning_rate": 3.875633760811215e-06,
      "loss": 8.1061,
      "step": 2600
    },
    {
      "epoch": 1.4091858037578289,
      "grad_norm": 12.280659675598145,
      "learning_rate": 4.0247539516850586e-06,
      "loss": 7.8274,
      "step": 2700
    },
    {
      "epoch": 1.4613778705636742,
      "grad_norm": 11.667861938476562,
      "learning_rate": 4.173874142558903e-06,
      "loss": 7.569,
      "step": 2800
    },
    {
      "epoch": 1.5135699373695197,
      "grad_norm": 11.847161293029785,
      "learning_rate": 4.322994333432747e-06,
      "loss": 7.4173,
      "step": 2900
    },
    {
      "epoch": 1.5657620041753653,
      "grad_norm": 11.6265869140625,
      "learning_rate": 4.472114524306592e-06,
      "loss": 7.2885,
      "step": 3000
    },
    {
      "epoch": 1.6179540709812108,
      "grad_norm": 11.485748291015625,
      "learning_rate": 4.621234715180436e-06,
      "loss": 7.044,
      "step": 3100
    },
    {
      "epoch": 1.6701461377870563,
      "grad_norm": 11.168266296386719,
      "learning_rate": 4.77035490605428e-06,
      "loss": 6.923,
      "step": 3200
    },
    {
      "epoch": 1.7223382045929019,
      "grad_norm": 11.075411796569824,
      "learning_rate": 4.919475096928125e-06,
      "loss": 6.7769,
      "step": 3300
    },
    {
      "epoch": 1.7745302713987474,
      "grad_norm": 11.023847579956055,
      "learning_rate": 5.068595287801969e-06,
      "loss": 6.7574,
      "step": 3400
    },
    {
      "epoch": 1.826722338204593,
      "grad_norm": 10.568507194519043,
      "learning_rate": 5.217715478675813e-06,
      "loss": 6.6183,
      "step": 3500
    },
    {
      "epoch": 1.8789144050104385,
      "grad_norm": 11.074904441833496,
      "learning_rate": 5.366835669549657e-06,
      "loss": 6.6488,
      "step": 3600
    },
    {
      "epoch": 1.931106471816284,
      "grad_norm": 10.676702499389648,
      "learning_rate": 5.515955860423502e-06,
      "loss": 6.4667,
      "step": 3700
    },
    {
      "epoch": 1.9832985386221296,
      "grad_norm": 10.696776390075684,
      "learning_rate": 5.665076051297346e-06,
      "loss": 6.48,
      "step": 3800
    },
    {
      "epoch": 2.035490605427975,
      "grad_norm": 10.391487121582031,
      "learning_rate": 5.81419624217119e-06,
      "loss": 6.0825,
      "step": 3900
    },
    {
      "epoch": 2.0876826722338206,
      "grad_norm": 10.269753456115723,
      "learning_rate": 5.963316433045034e-06,
      "loss": 5.9513,
      "step": 4000
    },
    {
      "epoch": 2.1398747390396657,
      "grad_norm": 9.92873477935791,
      "learning_rate": 6.11243662391888e-06,
      "loss": 6.0429,
      "step": 4100
    },
    {
      "epoch": 2.1920668058455113,
      "grad_norm": 9.406721115112305,
      "learning_rate": 6.261556814792723e-06,
      "loss": 6.072,
      "step": 4200
    },
    {
      "epoch": 2.244258872651357,
      "grad_norm": 9.798189163208008,
      "learning_rate": 6.410677005666568e-06,
      "loss": 6.1629,
      "step": 4300
    },
    {
      "epoch": 2.2964509394572024,
      "grad_norm": 9.623802185058594,
      "learning_rate": 6.559797196540412e-06,
      "loss": 6.1551,
      "step": 4400
    },
    {
      "epoch": 2.348643006263048,
      "grad_norm": 9.661754608154297,
      "learning_rate": 6.708917387414257e-06,
      "loss": 6.0664,
      "step": 4500
    },
    {
      "epoch": 2.4008350730688934,
      "grad_norm": 9.632540702819824,
      "learning_rate": 6.8580375782881004e-06,
      "loss": 6.1062,
      "step": 4600
    },
    {
      "epoch": 2.453027139874739,
      "grad_norm": 9.295202255249023,
      "learning_rate": 7.007157769161945e-06,
      "loss": 6.1539,
      "step": 4700
    },
    {
      "epoch": 2.5052192066805845,
      "grad_norm": 9.02694034576416,
      "learning_rate": 7.156277960035789e-06,
      "loss": 6.1592,
      "step": 4800
    },
    {
      "epoch": 2.55741127348643,
      "grad_norm": 8.921063423156738,
      "learning_rate": 7.305398150909634e-06,
      "loss": 6.124,
      "step": 4900
    },
    {
      "epoch": 2.6096033402922756,
      "grad_norm": 8.756376266479492,
      "learning_rate": 7.454518341783477e-06,
      "loss": 6.1366,
      "step": 5000
    },
    {
      "epoch": 2.661795407098121,
      "grad_norm": 8.646078109741211,
      "learning_rate": 7.603638532657323e-06,
      "loss": 6.102,
      "step": 5100
    },
    {
      "epoch": 2.7139874739039667,
      "grad_norm": 8.757345199584961,
      "learning_rate": 7.752758723531166e-06,
      "loss": 6.1553,
      "step": 5200
    },
    {
      "epoch": 2.766179540709812,
      "grad_norm": 8.447406768798828,
      "learning_rate": 7.90187891440501e-06,
      "loss": 6.1431,
      "step": 5300
    },
    {
      "epoch": 2.8183716075156577,
      "grad_norm": 8.55689811706543,
      "learning_rate": 8.050999105278855e-06,
      "loss": 6.0892,
      "step": 5400
    },
    {
      "epoch": 2.8705636743215033,
      "grad_norm": 8.492989540100098,
      "learning_rate": 8.2001192961527e-06,
      "loss": 6.0953,
      "step": 5500
    },
    {
      "epoch": 2.9227557411273484,
      "grad_norm": 8.120549201965332,
      "learning_rate": 8.349239487026543e-06,
      "loss": 6.0863,
      "step": 5600
    },
    {
      "epoch": 2.9749478079331944,
      "grad_norm": 8.03798770904541,
      "learning_rate": 8.49835967790039e-06,
      "loss": 6.1017,
      "step": 5700
    },
    {
      "epoch": 3.0271398747390394,
      "grad_norm": 7.873332500457764,
      "learning_rate": 8.647479868774232e-06,
      "loss": 5.8431,
      "step": 5800
    },
    {
      "epoch": 3.079331941544885,
      "grad_norm": 7.928519248962402,
      "learning_rate": 8.796600059648077e-06,
      "loss": 5.694,
      "step": 5900
    },
    {
      "epoch": 3.1315240083507305,
      "grad_norm": 7.631398677825928,
      "learning_rate": 8.945720250521922e-06,
      "loss": 5.739,
      "step": 6000
    },
    {
      "epoch": 3.183716075156576,
      "grad_norm": 7.560294151306152,
      "learning_rate": 9.094840441395766e-06,
      "loss": 5.8378,
      "step": 6100
    },
    {
      "epoch": 3.2359081419624216,
      "grad_norm": 7.497657775878906,
      "learning_rate": 9.24396063226961e-06,
      "loss": 5.8468,
      "step": 6200
    },
    {
      "epoch": 3.288100208768267,
      "grad_norm": 7.146864891052246,
      "learning_rate": 9.393080823143454e-06,
      "loss": 5.9562,
      "step": 6300
    },
    {
      "epoch": 3.3402922755741127,
      "grad_norm": 7.169593811035156,
      "learning_rate": 9.542201014017298e-06,
      "loss": 5.9896,
      "step": 6400
    },
    {
      "epoch": 3.392484342379958,
      "grad_norm": 7.030746936798096,
      "learning_rate": 9.691321204891143e-06,
      "loss": 5.9188,
      "step": 6500
    },
    {
      "epoch": 3.4446764091858038,
      "grad_norm": 7.242572784423828,
      "learning_rate": 9.840441395764986e-06,
      "loss": 5.9544,
      "step": 6600
    },
    {
      "epoch": 3.4968684759916493,
      "grad_norm": 7.283881187438965,
      "learning_rate": 9.989561586638832e-06,
      "loss": 5.9355,
      "step": 6700
    },
    {
      "epoch": 3.549060542797495,
      "grad_norm": 6.945225715637207,
      "learning_rate": 1.0138681777512675e-05,
      "loss": 5.8708,
      "step": 6800
    },
    {
      "epoch": 3.6012526096033404,
      "grad_norm": 7.036522388458252,
      "learning_rate": 1.028780196838652e-05,
      "loss": 5.9124,
      "step": 6900
    },
    {
      "epoch": 3.653444676409186,
      "grad_norm": 6.610386371612549,
      "learning_rate": 1.0436922159260365e-05,
      "loss": 5.979,
      "step": 7000
    },
    {
      "epoch": 3.7056367432150314,
      "grad_norm": 6.713845729827881,
      "learning_rate": 1.058604235013421e-05,
      "loss": 6.0076,
      "step": 7100
    },
    {
      "epoch": 3.757828810020877,
      "grad_norm": 6.571114540100098,
      "learning_rate": 1.0735162541008052e-05,
      "loss": 5.9933,
      "step": 7200
    },
    {
      "epoch": 3.810020876826722,
      "grad_norm": 6.414210796356201,
      "learning_rate": 1.0884282731881897e-05,
      "loss": 5.9713,
      "step": 7300
    },
    {
      "epoch": 3.862212943632568,
      "grad_norm": 6.701316833496094,
      "learning_rate": 1.1033402922755743e-05,
      "loss": 5.9615,
      "step": 7400
    },
    {
      "epoch": 3.914405010438413,
      "grad_norm": 6.265986442565918,
      "learning_rate": 1.1182523113629586e-05,
      "loss": 5.9601,
      "step": 7500
    },
    {
      "epoch": 3.966597077244259,
      "grad_norm": 6.393284320831299,
      "learning_rate": 1.133164330450343e-05,
      "loss": 5.9657,
      "step": 7600
    },
    {
      "epoch": 4.018789144050104,
      "grad_norm": 6.027726173400879,
      "learning_rate": 1.1480763495377274e-05,
      "loss": 5.793,
      "step": 7700
    },
    {
      "epoch": 4.07098121085595,
      "grad_norm": 6.320893287658691,
      "learning_rate": 1.162988368625112e-05,
      "loss": 5.502,
      "step": 7800
    },
    {
      "epoch": 4.123173277661795,
      "grad_norm": 6.180007457733154,
      "learning_rate": 1.1779003877124963e-05,
      "loss": 5.5809,
      "step": 7900
    },
    {
      "epoch": 4.175365344467641,
      "grad_norm": 5.962803840637207,
      "learning_rate": 1.1928124067998808e-05,
      "loss": 5.7361,
      "step": 8000
    },
    {
      "epoch": 4.227557411273486,
      "grad_norm": 5.835709571838379,
      "learning_rate": 1.2077244258872651e-05,
      "loss": 5.7564,
      "step": 8100
    },
    {
      "epoch": 4.2797494780793315,
      "grad_norm": 5.91887092590332,
      "learning_rate": 1.2226364449746497e-05,
      "loss": 5.7897,
      "step": 8200
    },
    {
      "epoch": 4.3319415448851775,
      "grad_norm": 6.000433444976807,
      "learning_rate": 1.237548464062034e-05,
      "loss": 5.8697,
      "step": 8300
    },
    {
      "epoch": 4.384133611691023,
      "grad_norm": 5.8859381675720215,
      "learning_rate": 1.2524604831494185e-05,
      "loss": 5.812,
      "step": 8400
    },
    {
      "epoch": 4.4363256784968685,
      "grad_norm": 5.656550407409668,
      "learning_rate": 1.2673725022368028e-05,
      "loss": 5.8311,
      "step": 8500
    },
    {
      "epoch": 4.488517745302714,
      "grad_norm": 5.878974437713623,
      "learning_rate": 1.2822845213241874e-05,
      "loss": 5.8665,
      "step": 8600
    },
    {
      "epoch": 4.54070981210856,
      "grad_norm": 5.707660675048828,
      "learning_rate": 1.2971965404115719e-05,
      "loss": 5.9071,
      "step": 8700
    },
    {
      "epoch": 4.592901878914405,
      "grad_norm": 5.507828235626221,
      "learning_rate": 1.3121085594989562e-05,
      "loss": 5.911,
      "step": 8800
    },
    {
      "epoch": 4.645093945720251,
      "grad_norm": 5.32956075668335,
      "learning_rate": 1.3270205785863407e-05,
      "loss": 5.7867,
      "step": 8900
    },
    {
      "epoch": 4.697286012526096,
      "grad_norm": 5.642925262451172,
      "learning_rate": 1.3419325976737251e-05,
      "loss": 5.8775,
      "step": 9000
    },
    {
      "epoch": 4.749478079331942,
      "grad_norm": 5.3360724449157715,
      "learning_rate": 1.3568446167611096e-05,
      "loss": 5.8422,
      "step": 9100
    },
    {
      "epoch": 4.801670146137787,
      "grad_norm": 5.595920085906982,
      "learning_rate": 1.3717566358484939e-05,
      "loss": 5.8248,
      "step": 9200
    },
    {
      "epoch": 4.853862212943633,
      "grad_norm": 5.5924530029296875,
      "learning_rate": 1.3866686549358784e-05,
      "loss": 5.8267,
      "step": 9300
    },
    {
      "epoch": 4.906054279749478,
      "grad_norm": 5.4772419929504395,
      "learning_rate": 1.401580674023263e-05,
      "loss": 5.7712,
      "step": 9400
    },
    {
      "epoch": 4.958246346555324,
      "grad_norm": 5.361322402954102,
      "learning_rate": 1.4164926931106473e-05,
      "loss": 5.8002,
      "step": 9500
    },
    {
      "epoch": 5.010438413361169,
      "grad_norm": 5.092629432678223,
      "learning_rate": 1.4314047121980316e-05,
      "loss": 5.7392,
      "step": 9600
    },
    {
      "epoch": 5.062630480167015,
      "grad_norm": 5.074234485626221,
      "learning_rate": 1.446316731285416e-05,
      "loss": 5.4324,
      "step": 9700
    },
    {
      "epoch": 5.11482254697286,
      "grad_norm": 5.040637016296387,
      "learning_rate": 1.4612287503728007e-05,
      "loss": 5.4797,
      "step": 9800
    },
    {
      "epoch": 5.167014613778706,
      "grad_norm": 5.033182621002197,
      "learning_rate": 1.476140769460185e-05,
      "loss": 5.4987,
      "step": 9900
    },
    {
      "epoch": 5.219206680584551,
      "grad_norm": 5.023143291473389,
      "learning_rate": 1.4910527885475695e-05,
      "loss": 5.7063,
      "step": 10000
    },
    {
      "epoch": 5.271398747390396,
      "grad_norm": 5.257872581481934,
      "learning_rate": 1.505964807634954e-05,
      "loss": 5.6631,
      "step": 10100
    },
    {
      "epoch": 5.323590814196242,
      "grad_norm": 5.075551509857178,
      "learning_rate": 1.5208768267223384e-05,
      "loss": 5.7123,
      "step": 10200
    },
    {
      "epoch": 5.375782881002087,
      "grad_norm": 4.687784194946289,
      "learning_rate": 1.535788845809723e-05,
      "loss": 5.7218,
      "step": 10300
    },
    {
      "epoch": 5.427974947807933,
      "grad_norm": 4.950658798217773,
      "learning_rate": 1.550700864897107e-05,
      "loss": 5.6885,
      "step": 10400
    },
    {
      "epoch": 5.480167014613778,
      "grad_norm": 5.175008296966553,
      "learning_rate": 1.5656128839844918e-05,
      "loss": 5.7951,
      "step": 10500
    },
    {
      "epoch": 5.532359081419624,
      "grad_norm": 4.92156457901001,
      "learning_rate": 1.580524903071876e-05,
      "loss": 5.6653,
      "step": 10600
    },
    {
      "epoch": 5.5845511482254695,
      "grad_norm": 5.001760959625244,
      "learning_rate": 1.5954369221592604e-05,
      "loss": 5.7609,
      "step": 10700
    },
    {
      "epoch": 5.6367432150313155,
      "grad_norm": 4.763769149780273,
      "learning_rate": 1.610348941246645e-05,
      "loss": 5.7746,
      "step": 10800
    },
    {
      "epoch": 5.688935281837161,
      "grad_norm": 4.912633419036865,
      "learning_rate": 1.6252609603340293e-05,
      "loss": 5.7771,
      "step": 10900
    },
    {
      "epoch": 5.741127348643007,
      "grad_norm": 4.825681209564209,
      "learning_rate": 1.640172979421414e-05,
      "loss": 5.7807,
      "step": 11000
    },
    {
      "epoch": 5.793319415448852,
      "grad_norm": 4.629530906677246,
      "learning_rate": 1.6550849985087983e-05,
      "loss": 5.7403,
      "step": 11100
    },
    {
      "epoch": 5.845511482254698,
      "grad_norm": 4.866567134857178,
      "learning_rate": 1.6699970175961826e-05,
      "loss": 5.7591,
      "step": 11200
    },
    {
      "epoch": 5.897703549060543,
      "grad_norm": 4.984760284423828,
      "learning_rate": 1.6849090366835672e-05,
      "loss": 5.7648,
      "step": 11300
    },
    {
      "epoch": 5.949895615866389,
      "grad_norm": 4.792060852050781,
      "learning_rate": 1.6998210557709515e-05,
      "loss": 5.683,
      "step": 11400
    },
    {
      "epoch": 6.002087682672234,
      "grad_norm": 4.77677059173584,
      "learning_rate": 1.714733074858336e-05,
      "loss": 5.7607,
      "step": 11500
    },
    {
      "epoch": 6.054279749478079,
      "grad_norm": 4.6850056648254395,
      "learning_rate": 1.7296450939457204e-05,
      "loss": 5.2996,
      "step": 11600
    },
    {
      "epoch": 6.106471816283925,
      "grad_norm": 4.622418403625488,
      "learning_rate": 1.744557113033105e-05,
      "loss": 5.3835,
      "step": 11700
    },
    {
      "epoch": 6.15866388308977,
      "grad_norm": 4.764825344085693,
      "learning_rate": 1.7594691321204893e-05,
      "loss": 5.5179,
      "step": 11800
    },
    {
      "epoch": 6.210855949895616,
      "grad_norm": 4.48654842376709,
      "learning_rate": 1.7743811512078736e-05,
      "loss": 5.5355,
      "step": 11900
    },
    {
      "epoch": 6.263048016701461,
      "grad_norm": 4.508426666259766,
      "learning_rate": 1.789293170295258e-05,
      "loss": 5.5556,
      "step": 12000
    },
    {
      "epoch": 6.315240083507307,
      "grad_norm": 4.612165927886963,
      "learning_rate": 1.8042051893826426e-05,
      "loss": 5.5874,
      "step": 12100
    },
    {
      "epoch": 6.367432150313152,
      "grad_norm": 4.438450813293457,
      "learning_rate": 1.819117208470027e-05,
      "loss": 5.5627,
      "step": 12200
    },
    {
      "epoch": 6.419624217118998,
      "grad_norm": 4.375000476837158,
      "learning_rate": 1.8340292275574115e-05,
      "loss": 5.5894,
      "step": 12300
    },
    {
      "epoch": 6.471816283924843,
      "grad_norm": 4.3320441246032715,
      "learning_rate": 1.8489412466447958e-05,
      "loss": 5.6106,
      "step": 12400
    },
    {
      "epoch": 6.524008350730689,
      "grad_norm": 4.477361679077148,
      "learning_rate": 1.8638532657321804e-05,
      "loss": 5.6012,
      "step": 12500
    },
    {
      "epoch": 6.576200417536534,
      "grad_norm": 4.454993724822998,
      "learning_rate": 1.8787652848195647e-05,
      "loss": 5.6181,
      "step": 12600
    },
    {
      "epoch": 6.62839248434238,
      "grad_norm": 4.500024795532227,
      "learning_rate": 1.893677303906949e-05,
      "loss": 5.6821,
      "step": 12700
    },
    {
      "epoch": 6.680584551148225,
      "grad_norm": 4.506542682647705,
      "learning_rate": 1.9085893229943337e-05,
      "loss": 5.6864,
      "step": 12800
    },
    {
      "epoch": 6.732776617954071,
      "grad_norm": 4.2323808670043945,
      "learning_rate": 1.923501342081718e-05,
      "loss": 5.7299,
      "step": 12900
    },
    {
      "epoch": 6.784968684759916,
      "grad_norm": 4.30665922164917,
      "learning_rate": 1.9384133611691026e-05,
      "loss": 5.7199,
      "step": 13000
    },
    {
      "epoch": 6.8371607515657615,
      "grad_norm": 4.279403209686279,
      "learning_rate": 1.953325380256487e-05,
      "loss": 5.6944,
      "step": 13100
    },
    {
      "epoch": 6.8893528183716075,
      "grad_norm": 4.192182540893555,
      "learning_rate": 1.9682373993438712e-05,
      "loss": 5.6863,
      "step": 13200
    },
    {
      "epoch": 6.9415448851774535,
      "grad_norm": 4.341867923736572,
      "learning_rate": 1.983149418431256e-05,
      "loss": 5.6706,
      "step": 13300
    },
    {
      "epoch": 6.993736951983299,
      "grad_norm": 4.552614212036133,
      "learning_rate": 1.99806143751864e-05,
      "loss": 5.6798,
      "step": 13400
    },
    {
      "epoch": 7.045929018789144,
      "grad_norm": 4.104211807250977,
      "learning_rate": 1.9999974364844216e-05,
      "loss": 5.2004,
      "step": 13500
    },
    {
      "epoch": 7.09812108559499,
      "grad_norm": 4.078258514404297,
      "learning_rate": 1.999988156501834e-05,
      "loss": 5.2388,
      "step": 13600
    },
    {
      "epoch": 7.150313152400835,
      "grad_norm": 4.206387996673584,
      "learning_rate": 1.9999721028771116e-05,
      "loss": 5.3878,
      "step": 13700
    },
    {
      "epoch": 7.202505219206681,
      "grad_norm": 4.065128326416016,
      "learning_rate": 1.9999492757189977e-05,
      "loss": 5.3691,
      "step": 13800
    },
    {
      "epoch": 7.254697286012526,
      "grad_norm": 3.980229616165161,
      "learning_rate": 1.9999196751821164e-05,
      "loss": 5.4006,
      "step": 13900
    },
    {
      "epoch": 7.306889352818372,
      "grad_norm": 4.072607040405273,
      "learning_rate": 1.999883301466974e-05,
      "loss": 5.4548,
      "step": 14000
    },
    {
      "epoch": 7.359081419624217,
      "grad_norm": 4.0047502517700195,
      "learning_rate": 1.999840154819956e-05,
      "loss": 5.5793,
      "step": 14100
    },
    {
      "epoch": 7.411273486430063,
      "grad_norm": 4.048254013061523,
      "learning_rate": 1.9997902355333255e-05,
      "loss": 5.5228,
      "step": 14200
    },
    {
      "epoch": 7.463465553235908,
      "grad_norm": 4.098287582397461,
      "learning_rate": 1.999733543945222e-05,
      "loss": 5.5395,
      "step": 14300
    },
    {
      "epoch": 7.515657620041754,
      "grad_norm": 4.040717124938965,
      "learning_rate": 1.9996700804396586e-05,
      "loss": 5.6014,
      "step": 14400
    },
    {
      "epoch": 7.567849686847599,
      "grad_norm": 4.24561882019043,
      "learning_rate": 1.9995998454465198e-05,
      "loss": 5.551,
      "step": 14500
    },
    {
      "epoch": 7.620041753653445,
      "grad_norm": 3.9179282188415527,
      "learning_rate": 1.9995228394415574e-05,
      "loss": 5.5959,
      "step": 14600
    },
    {
      "epoch": 7.67223382045929,
      "grad_norm": 4.248225688934326,
      "learning_rate": 1.999439062946389e-05,
      "loss": 5.5583,
      "step": 14700
    },
    {
      "epoch": 7.724425887265136,
      "grad_norm": 4.12637186050415,
      "learning_rate": 1.999348516528493e-05,
      "loss": 5.5513,
      "step": 14800
    },
    {
      "epoch": 7.776617954070981,
      "grad_norm": 3.954927444458008,
      "learning_rate": 1.9992512008012063e-05,
      "loss": 5.5638,
      "step": 14900
    },
    {
      "epoch": 7.828810020876826,
      "grad_norm": 4.185195446014404,
      "learning_rate": 1.999147116423718e-05,
      "loss": 5.5312,
      "step": 15000
    },
    {
      "epoch": 7.881002087682672,
      "grad_norm": 4.060211181640625,
      "learning_rate": 1.999036264101067e-05,
      "loss": 5.5512,
      "step": 15100
    },
    {
      "epoch": 7.933194154488517,
      "grad_norm": 3.9983317852020264,
      "learning_rate": 1.9989186445841362e-05,
      "loss": 5.5674,
      "step": 15200
    },
    {
      "epoch": 7.985386221294363,
      "grad_norm": 3.9316799640655518,
      "learning_rate": 1.9987942586696477e-05,
      "loss": 5.6184,
      "step": 15300
    },
    {
      "epoch": 8.037578288100208,
      "grad_norm": 3.8505656719207764,
      "learning_rate": 1.998663107200157e-05,
      "loss": 5.2412,
      "step": 15400
    },
    {
      "epoch": 8.089770354906054,
      "grad_norm": 4.026348114013672,
      "learning_rate": 1.9985251910640472e-05,
      "loss": 5.1481,
      "step": 15500
    },
    {
      "epoch": 8.1419624217119,
      "grad_norm": 4.1329779624938965,
      "learning_rate": 1.998380511195525e-05,
      "loss": 5.2309,
      "step": 15600
    },
    {
      "epoch": 8.194154488517745,
      "grad_norm": 3.861410140991211,
      "learning_rate": 1.998229068574611e-05,
      "loss": 5.3347,
      "step": 15700
    },
    {
      "epoch": 8.24634655532359,
      "grad_norm": 3.7857775688171387,
      "learning_rate": 1.9980708642271357e-05,
      "loss": 5.2843,
      "step": 15800
    },
    {
      "epoch": 8.298538622129437,
      "grad_norm": 3.939924716949463,
      "learning_rate": 1.997905899224731e-05,
      "loss": 5.3296,
      "step": 15900
    },
    {
      "epoch": 8.350730688935283,
      "grad_norm": 3.9529213905334473,
      "learning_rate": 1.9977341746848243e-05,
      "loss": 5.4404,
      "step": 16000
    },
    {
      "epoch": 8.402922755741127,
      "grad_norm": 3.7908318042755127,
      "learning_rate": 1.9975556917706302e-05,
      "loss": 5.4242,
      "step": 16100
    },
    {
      "epoch": 8.455114822546973,
      "grad_norm": 3.9864349365234375,
      "learning_rate": 1.997370451691142e-05,
      "loss": 5.4089,
      "step": 16200
    },
    {
      "epoch": 8.507306889352819,
      "grad_norm": 3.988471269607544,
      "learning_rate": 1.997178455701125e-05,
      "loss": 5.3967,
      "step": 16300
    },
    {
      "epoch": 8.559498956158663,
      "grad_norm": 3.916606903076172,
      "learning_rate": 1.9969797051011063e-05,
      "loss": 5.4426,
      "step": 16400
    },
    {
      "epoch": 8.611691022964509,
      "grad_norm": 3.999656915664673,
      "learning_rate": 1.9967742012373678e-05,
      "loss": 5.4009,
      "step": 16500
    },
    {
      "epoch": 8.663883089770355,
      "grad_norm": 4.020442485809326,
      "learning_rate": 1.9965619455019346e-05,
      "loss": 5.4257,
      "step": 16600
    },
    {
      "epoch": 8.716075156576201,
      "grad_norm": 3.6934468746185303,
      "learning_rate": 1.9963429393325694e-05,
      "loss": 5.4762,
      "step": 16700
    },
    {
      "epoch": 8.768267223382045,
      "grad_norm": 3.8777658939361572,
      "learning_rate": 1.996117184212758e-05,
      "loss": 5.4253,
      "step": 16800
    },
    {
      "epoch": 8.820459290187891,
      "grad_norm": 3.822453737258911,
      "learning_rate": 1.995884681671704e-05,
      "loss": 5.4187,
      "step": 16900
    },
    {
      "epoch": 8.872651356993737,
      "grad_norm": 3.9409408569335938,
      "learning_rate": 1.9956454332843144e-05,
      "loss": 5.4975,
      "step": 17000
    },
    {
      "epoch": 8.924843423799583,
      "grad_norm": 3.8137366771698,
      "learning_rate": 1.9953994406711915e-05,
      "loss": 5.4219,
      "step": 17100
    },
    {
      "epoch": 8.977035490605427,
      "grad_norm": 3.838078260421753,
      "learning_rate": 1.995146705498621e-05,
      "loss": 5.399,
      "step": 17200
    },
    {
      "epoch": 9.029227557411273,
      "grad_norm": 3.819807529449463,
      "learning_rate": 1.994887229478561e-05,
      "loss": 5.2115,
      "step": 17300
    },
    {
      "epoch": 9.08141962421712,
      "grad_norm": 3.9525067806243896,
      "learning_rate": 1.9946210143686295e-05,
      "loss": 5.0752,
      "step": 17400
    },
    {
      "epoch": 9.133611691022965,
      "grad_norm": 3.708817481994629,
      "learning_rate": 1.994348061972094e-05,
      "loss": 5.1424,
      "step": 17500
    },
    {
      "epoch": 9.18580375782881,
      "grad_norm": 3.810025215148926,
      "learning_rate": 1.9940683741378584e-05,
      "loss": 5.2403,
      "step": 17600
    },
    {
      "epoch": 9.237995824634655,
      "grad_norm": 3.7460761070251465,
      "learning_rate": 1.9937819527604503e-05,
      "loss": 5.1916,
      "step": 17700
    },
    {
      "epoch": 9.290187891440501,
      "grad_norm": 3.6302034854888916,
      "learning_rate": 1.9934887997800084e-05,
      "loss": 5.1966,
      "step": 17800
    },
    {
      "epoch": 9.342379958246347,
      "grad_norm": 3.816066026687622,
      "learning_rate": 1.99318891718227e-05,
      "loss": 5.3334,
      "step": 17900
    },
    {
      "epoch": 9.394572025052192,
      "grad_norm": 3.682215690612793,
      "learning_rate": 1.9928823069985563e-05,
      "loss": 5.2435,
      "step": 18000
    },
    {
      "epoch": 9.446764091858038,
      "grad_norm": 3.8391947746276855,
      "learning_rate": 1.9925689713057595e-05,
      "loss": 5.2433,
      "step": 18100
    },
    {
      "epoch": 9.498956158663884,
      "grad_norm": 3.5803885459899902,
      "learning_rate": 1.9922489122263288e-05,
      "loss": 5.3186,
      "step": 18200
    },
    {
      "epoch": 9.551148225469728,
      "grad_norm": 3.7201175689697266,
      "learning_rate": 1.9919221319282552e-05,
      "loss": 5.3336,
      "step": 18300
    },
    {
      "epoch": 9.603340292275574,
      "grad_norm": 3.7663462162017822,
      "learning_rate": 1.9915886326250584e-05,
      "loss": 5.3557,
      "step": 18400
    },
    {
      "epoch": 9.65553235908142,
      "grad_norm": 3.6835172176361084,
      "learning_rate": 1.9912484165757693e-05,
      "loss": 5.3934,
      "step": 18500
    },
    {
      "epoch": 9.707724425887266,
      "grad_norm": 3.644395351409912,
      "learning_rate": 1.9909014860849173e-05,
      "loss": 5.3457,
      "step": 18600
    },
    {
      "epoch": 9.75991649269311,
      "grad_norm": 3.8388986587524414,
      "learning_rate": 1.9905478435025135e-05,
      "loss": 5.3447,
      "step": 18700
    },
    {
      "epoch": 9.812108559498956,
      "grad_norm": 3.699249267578125,
      "learning_rate": 1.990187491224034e-05,
      "loss": 5.3686,
      "step": 18800
    },
    {
      "epoch": 9.864300626304802,
      "grad_norm": 3.8171231746673584,
      "learning_rate": 1.9898204316904058e-05,
      "loss": 5.3999,
      "step": 18900
    },
    {
      "epoch": 9.916492693110648,
      "grad_norm": 3.737452507019043,
      "learning_rate": 1.9894466673879875e-05,
      "loss": 5.36,
      "step": 19000
    },
    {
      "epoch": 9.968684759916492,
      "grad_norm": 3.6523876190185547,
      "learning_rate": 1.989066200848555e-05,
      "loss": 5.2848,
      "step": 19100
    },
    {
      "epoch": 10.020876826722338,
      "grad_norm": 3.5477209091186523,
      "learning_rate": 1.988679034649283e-05,
      "loss": 5.1561,
      "step": 19200
    },
    {
      "epoch": 10.073068893528184,
      "grad_norm": 3.4637887477874756,
      "learning_rate": 1.988285171412728e-05,
      "loss": 4.9473,
      "step": 19300
    },
    {
      "epoch": 10.12526096033403,
      "grad_norm": 3.5920908451080322,
      "learning_rate": 1.98788461380681e-05,
      "loss": 5.0769,
      "step": 19400
    },
    {
      "epoch": 10.177453027139874,
      "grad_norm": 3.674114465713501,
      "learning_rate": 1.9874773645447957e-05,
      "loss": 5.1179,
      "step": 19500
    },
    {
      "epoch": 10.22964509394572,
      "grad_norm": 3.8836584091186523,
      "learning_rate": 1.9870634263852774e-05,
      "loss": 5.1465,
      "step": 19600
    },
    {
      "epoch": 10.281837160751566,
      "grad_norm": 3.7260982990264893,
      "learning_rate": 1.986642802132158e-05,
      "loss": 5.1491,
      "step": 19700
    },
    {
      "epoch": 10.334029227557412,
      "grad_norm": 3.6823365688323975,
      "learning_rate": 1.986215494634629e-05,
      "loss": 5.1972,
      "step": 19800
    },
    {
      "epoch": 10.386221294363256,
      "grad_norm": 3.7173891067504883,
      "learning_rate": 1.9857815067871536e-05,
      "loss": 5.2125,
      "step": 19900
    },
    {
      "epoch": 10.438413361169102,
      "grad_norm": 3.613659381866455,
      "learning_rate": 1.9853408415294437e-05,
      "loss": 5.1892,
      "step": 20000
    },
    {
      "epoch": 10.490605427974948,
      "grad_norm": 3.7297816276550293,
      "learning_rate": 1.9848935018464442e-05,
      "loss": 5.2572,
      "step": 20100
    },
    {
      "epoch": 10.542797494780793,
      "grad_norm": 3.6758131980895996,
      "learning_rate": 1.98443949076831e-05,
      "loss": 5.2547,
      "step": 20200
    },
    {
      "epoch": 10.594989561586639,
      "grad_norm": 3.5820205211639404,
      "learning_rate": 1.9839788113703856e-05,
      "loss": 5.2789,
      "step": 20300
    },
    {
      "epoch": 10.647181628392484,
      "grad_norm": 3.6975722312927246,
      "learning_rate": 1.9835114667731862e-05,
      "loss": 5.3123,
      "step": 20400
    },
    {
      "epoch": 10.69937369519833,
      "grad_norm": 3.6453685760498047,
      "learning_rate": 1.983037460142373e-05,
      "loss": 5.2297,
      "step": 20500
    },
    {
      "epoch": 10.751565762004175,
      "grad_norm": 3.911433696746826,
      "learning_rate": 1.9825567946887366e-05,
      "loss": 5.2465,
      "step": 20600
    },
    {
      "epoch": 10.80375782881002,
      "grad_norm": 3.7368955612182617,
      "learning_rate": 1.9820694736681707e-05,
      "loss": 5.2735,
      "step": 20700
    },
    {
      "epoch": 10.855949895615867,
      "grad_norm": 3.589111089706421,
      "learning_rate": 1.981575500381653e-05,
      "loss": 5.304,
      "step": 20800
    },
    {
      "epoch": 10.908141962421713,
      "grad_norm": 3.618086099624634,
      "learning_rate": 1.9810748781752207e-05,
      "loss": 5.3806,
      "step": 20900
    },
    {
      "epoch": 10.960334029227557,
      "grad_norm": 3.596123456954956,
      "learning_rate": 1.98056761043995e-05,
      "loss": 5.2865,
      "step": 21000
    },
    {
      "epoch": 11.012526096033403,
      "grad_norm": 3.51802659034729,
      "learning_rate": 1.9800537006119317e-05,
      "loss": 5.1707,
      "step": 21100
    },
    {
      "epoch": 11.064718162839249,
      "grad_norm": 3.608466148376465,
      "learning_rate": 1.9795331521722488e-05,
      "loss": 4.8943,
      "step": 21200
    },
    {
      "epoch": 11.116910229645095,
      "grad_norm": 3.5289905071258545,
      "learning_rate": 1.979005968646951e-05,
      "loss": 4.9703,
      "step": 21300
    },
    {
      "epoch": 11.169102296450939,
      "grad_norm": 3.620490312576294,
      "learning_rate": 1.9784721536070335e-05,
      "loss": 5.0182,
      "step": 21400
    },
    {
      "epoch": 11.221294363256785,
      "grad_norm": 3.479278802871704,
      "learning_rate": 1.9779317106684113e-05,
      "loss": 5.1311,
      "step": 21500
    },
    {
      "epoch": 11.273486430062631,
      "grad_norm": 3.4901363849639893,
      "learning_rate": 1.977384643491895e-05,
      "loss": 5.0574,
      "step": 21600
    },
    {
      "epoch": 11.325678496868475,
      "grad_norm": 3.7101495265960693,
      "learning_rate": 1.9768309557831653e-05,
      "loss": 5.1042,
      "step": 21700
    },
    {
      "epoch": 11.377870563674321,
      "grad_norm": 3.4603891372680664,
      "learning_rate": 1.9762706512927496e-05,
      "loss": 5.118,
      "step": 21800
    },
    {
      "epoch": 11.430062630480167,
      "grad_norm": 3.54091739654541,
      "learning_rate": 1.9757037338159942e-05,
      "loss": 5.1702,
      "step": 21900
    },
    {
      "epoch": 11.482254697286013,
      "grad_norm": 3.6271049976348877,
      "learning_rate": 1.975130207193041e-05,
      "loss": 5.1575,
      "step": 22000
    },
    {
      "epoch": 11.534446764091857,
      "grad_norm": 3.660062551498413,
      "learning_rate": 1.9745500753088004e-05,
      "loss": 5.2023,
      "step": 22100
    },
    {
      "epoch": 11.586638830897703,
      "grad_norm": 3.7034692764282227,
      "learning_rate": 1.9739633420929246e-05,
      "loss": 5.2433,
      "step": 22200
    },
    {
      "epoch": 11.63883089770355,
      "grad_norm": 3.6290132999420166,
      "learning_rate": 1.9733700115197813e-05,
      "loss": 5.1743,
      "step": 22300
    },
    {
      "epoch": 11.691022964509395,
      "grad_norm": 3.6781492233276367,
      "learning_rate": 1.972770087608427e-05,
      "loss": 5.329,
      "step": 22400
    },
    {
      "epoch": 11.74321503131524,
      "grad_norm": 3.565035343170166,
      "learning_rate": 1.97216357442258e-05,
      "loss": 5.2181,
      "step": 22500
    },
    {
      "epoch": 11.795407098121085,
      "grad_norm": 3.610320806503296,
      "learning_rate": 1.971550476070592e-05,
      "loss": 5.2886,
      "step": 22600
    },
    {
      "epoch": 11.847599164926931,
      "grad_norm": 3.490565299987793,
      "learning_rate": 1.9709307967054213e-05,
      "loss": 5.2303,
      "step": 22700
    },
    {
      "epoch": 11.899791231732777,
      "grad_norm": 3.4970502853393555,
      "learning_rate": 1.9703045405246038e-05,
      "loss": 5.2791,
      "step": 22800
    },
    {
      "epoch": 11.951983298538622,
      "grad_norm": 3.5137133598327637,
      "learning_rate": 1.9696717117702245e-05,
      "loss": 5.2211,
      "step": 22900
    },
    {
      "epoch": 12.004175365344468,
      "grad_norm": 3.4568824768066406,
      "learning_rate": 1.9690323147288906e-05,
      "loss": 5.2151,
      "step": 23000
    },
    {
      "epoch": 12.056367432150314,
      "grad_norm": 3.4455153942108154,
      "learning_rate": 1.9683863537316995e-05,
      "loss": 4.8017,
      "step": 23100
    },
    {
      "epoch": 12.108559498956158,
      "grad_norm": 3.7171237468719482,
      "learning_rate": 1.9677338331542115e-05,
      "loss": 4.8887,
      "step": 23200
    },
    {
      "epoch": 12.160751565762004,
      "grad_norm": 3.566678047180176,
      "learning_rate": 1.96707475741642e-05,
      "loss": 4.9753,
      "step": 23300
    },
    {
      "epoch": 12.21294363256785,
      "grad_norm": 3.664321184158325,
      "learning_rate": 1.9664091309827212e-05,
      "loss": 5.0017,
      "step": 23400
    },
    {
      "epoch": 12.265135699373696,
      "grad_norm": 3.634293794631958,
      "learning_rate": 1.965736958361884e-05,
      "loss": 5.0584,
      "step": 23500
    },
    {
      "epoch": 12.31732776617954,
      "grad_norm": 3.8407375812530518,
      "learning_rate": 1.9650582441070187e-05,
      "loss": 5.0636,
      "step": 23600
    },
    {
      "epoch": 12.369519832985386,
      "grad_norm": 3.7361397743225098,
      "learning_rate": 1.9643729928155474e-05,
      "loss": 5.0909,
      "step": 23700
    },
    {
      "epoch": 12.421711899791232,
      "grad_norm": 3.4868340492248535,
      "learning_rate": 1.9636812091291718e-05,
      "loss": 5.0983,
      "step": 23800
    },
    {
      "epoch": 12.473903966597078,
      "grad_norm": 3.541808843612671,
      "learning_rate": 1.962982897733843e-05,
      "loss": 5.1813,
      "step": 23900
    },
    {
      "epoch": 12.526096033402922,
      "grad_norm": 3.580082416534424,
      "learning_rate": 1.9622780633597284e-05,
      "loss": 5.1495,
      "step": 24000
    },
    {
      "epoch": 12.578288100208768,
      "grad_norm": 3.5834007263183594,
      "learning_rate": 1.96156671078118e-05,
      "loss": 5.1783,
      "step": 24100
    },
    {
      "epoch": 12.630480167014614,
      "grad_norm": 3.61822509765625,
      "learning_rate": 1.9608488448167024e-05,
      "loss": 5.1881,
      "step": 24200
    },
    {
      "epoch": 12.68267223382046,
      "grad_norm": 3.6698434352874756,
      "learning_rate": 1.9601244703289215e-05,
      "loss": 5.1432,
      "step": 24300
    },
    {
      "epoch": 12.734864300626304,
      "grad_norm": 3.6293442249298096,
      "learning_rate": 1.9593935922245483e-05,
      "loss": 5.2361,
      "step": 24400
    },
    {
      "epoch": 12.78705636743215,
      "grad_norm": 3.3849985599517822,
      "learning_rate": 1.9586562154543478e-05,
      "loss": 5.1586,
      "step": 24500
    },
    {
      "epoch": 12.839248434237996,
      "grad_norm": 3.4114201068878174,
      "learning_rate": 1.9579123450131062e-05,
      "loss": 5.2026,
      "step": 24600
    },
    {
      "epoch": 12.891440501043842,
      "grad_norm": 3.524522542953491,
      "learning_rate": 1.9571619859395948e-05,
      "loss": 5.2571,
      "step": 24700
    },
    {
      "epoch": 12.943632567849686,
      "grad_norm": 3.5871801376342773,
      "learning_rate": 1.956405143316538e-05,
      "loss": 5.2309,
      "step": 24800
    },
    {
      "epoch": 12.995824634655532,
      "grad_norm": 3.4311468601226807,
      "learning_rate": 1.9556418222705772e-05,
      "loss": 5.1492,
      "step": 24900
    },
    {
      "epoch": 13.048016701461378,
      "grad_norm": 3.4234180450439453,
      "learning_rate": 1.9548720279722374e-05,
      "loss": 4.8567,
      "step": 25000
    },
    {
      "epoch": 13.100208768267223,
      "grad_norm": 3.534191370010376,
      "learning_rate": 1.954095765635892e-05,
      "loss": 4.9019,
      "step": 25100
    },
    {
      "epoch": 13.152400835073069,
      "grad_norm": 3.5908589363098145,
      "learning_rate": 1.9533130405197258e-05,
      "loss": 4.8805,
      "step": 25200
    },
    {
      "epoch": 13.204592901878915,
      "grad_norm": 3.545030117034912,
      "learning_rate": 1.9525238579257014e-05,
      "loss": 4.9746,
      "step": 25300
    },
    {
      "epoch": 13.25678496868476,
      "grad_norm": 3.510617256164551,
      "learning_rate": 1.9517282231995237e-05,
      "loss": 4.9567,
      "step": 25400
    },
    {
      "epoch": 13.308977035490605,
      "grad_norm": 3.4603991508483887,
      "learning_rate": 1.9509261417305997e-05,
      "loss": 4.9337,
      "step": 25500
    },
    {
      "epoch": 13.36116910229645,
      "grad_norm": 3.4743010997772217,
      "learning_rate": 1.950117618952008e-05,
      "loss": 4.9979,
      "step": 25600
    },
    {
      "epoch": 13.413361169102297,
      "grad_norm": 3.6439220905303955,
      "learning_rate": 1.949302660340457e-05,
      "loss": 5.0803,
      "step": 25700
    },
    {
      "epoch": 13.465553235908143,
      "grad_norm": 3.51778244972229,
      "learning_rate": 1.9484812714162497e-05,
      "loss": 5.0498,
      "step": 25800
    },
    {
      "epoch": 13.517745302713987,
      "grad_norm": 3.574378252029419,
      "learning_rate": 1.947653457743247e-05,
      "loss": 5.0572,
      "step": 25900
    },
    {
      "epoch": 13.569937369519833,
      "grad_norm": 3.4539599418640137,
      "learning_rate": 1.9468192249288287e-05,
      "loss": 5.175,
      "step": 26000
    },
    {
      "epoch": 13.622129436325679,
      "grad_norm": 3.492335081100464,
      "learning_rate": 1.9459785786238565e-05,
      "loss": 5.0647,
      "step": 26100
    },
    {
      "epoch": 13.674321503131525,
      "grad_norm": 3.4639532566070557,
      "learning_rate": 1.9451315245226347e-05,
      "loss": 5.0861,
      "step": 26200
    },
    {
      "epoch": 13.726513569937369,
      "grad_norm": 3.5798416137695312,
      "learning_rate": 1.944278068362873e-05,
      "loss": 5.1548,
      "step": 26300
    },
    {
      "epoch": 13.778705636743215,
      "grad_norm": 3.3666727542877197,
      "learning_rate": 1.9434182159256463e-05,
      "loss": 5.2299,
      "step": 26400
    },
    {
      "epoch": 13.830897703549061,
      "grad_norm": 3.5617361068725586,
      "learning_rate": 1.9425519730353563e-05,
      "loss": 5.1262,
      "step": 26500
    },
    {
      "epoch": 13.883089770354907,
      "grad_norm": 3.6348655223846436,
      "learning_rate": 1.941679345559692e-05,
      "loss": 5.1887,
      "step": 26600
    },
    {
      "epoch": 13.935281837160751,
      "grad_norm": 3.5462701320648193,
      "learning_rate": 1.9408003394095895e-05,
      "loss": 5.1725,
      "step": 26700
    },
    {
      "epoch": 13.987473903966597,
      "grad_norm": 3.351278066635132,
      "learning_rate": 1.939914960539192e-05,
      "loss": 5.202,
      "step": 26800
    },
    {
      "epoch": 14.039665970772443,
      "grad_norm": 3.423273801803589,
      "learning_rate": 1.9390232149458107e-05,
      "loss": 4.9065,
      "step": 26900
    },
    {
      "epoch": 14.091858037578287,
      "grad_norm": 3.480562448501587,
      "learning_rate": 1.9381251086698826e-05,
      "loss": 4.814,
      "step": 27000
    },
    {
      "epoch": 14.144050104384133,
      "grad_norm": 3.541778326034546,
      "learning_rate": 1.9372206477949298e-05,
      "loss": 4.8643,
      "step": 27100
    },
    {
      "epoch": 14.19624217118998,
      "grad_norm": 3.4362151622772217,
      "learning_rate": 1.93630983844752e-05,
      "loss": 4.9094,
      "step": 27200
    },
    {
      "epoch": 14.248434237995825,
      "grad_norm": 3.441768169403076,
      "learning_rate": 1.935392686797222e-05,
      "loss": 4.9659,
      "step": 27300
    },
    {
      "epoch": 14.30062630480167,
      "grad_norm": 3.3904051780700684,
      "learning_rate": 1.934469199056567e-05,
      "loss": 4.9265,
      "step": 27400
    },
    {
      "epoch": 14.352818371607516,
      "grad_norm": 3.3917737007141113,
      "learning_rate": 1.933539381481004e-05,
      "loss": 4.9657,
      "step": 27500
    },
    {
      "epoch": 14.405010438413361,
      "grad_norm": 3.314000129699707,
      "learning_rate": 1.9326032403688596e-05,
      "loss": 4.9827,
      "step": 27600
    },
    {
      "epoch": 14.457202505219207,
      "grad_norm": 3.5258679389953613,
      "learning_rate": 1.931660782061294e-05,
      "loss": 5.0235,
      "step": 27700
    },
    {
      "epoch": 14.509394572025052,
      "grad_norm": 3.242063283920288,
      "learning_rate": 1.9307120129422574e-05,
      "loss": 5.0096,
      "step": 27800
    },
    {
      "epoch": 14.561586638830898,
      "grad_norm": 3.416931390762329,
      "learning_rate": 1.9297569394384488e-05,
      "loss": 5.1291,
      "step": 27900
    },
    {
      "epoch": 14.613778705636744,
      "grad_norm": 3.4820737838745117,
      "learning_rate": 1.9287955680192708e-05,
      "loss": 5.1116,
      "step": 28000
    },
    {
      "epoch": 14.665970772442588,
      "grad_norm": 3.3237838745117188,
      "learning_rate": 1.9278279051967867e-05,
      "loss": 5.0568,
      "step": 28100
    },
    {
      "epoch": 14.718162839248434,
      "grad_norm": 3.6318092346191406,
      "learning_rate": 1.9268539575256755e-05,
      "loss": 5.0901,
      "step": 28200
    },
    {
      "epoch": 14.77035490605428,
      "grad_norm": 3.3846030235290527,
      "learning_rate": 1.9258737316031886e-05,
      "loss": 5.1496,
      "step": 28300
    },
    {
      "epoch": 14.822546972860126,
      "grad_norm": 3.587296485900879,
      "learning_rate": 1.9248872340691042e-05,
      "loss": 5.1083,
      "step": 28400
    },
    {
      "epoch": 14.87473903966597,
      "grad_norm": 3.512610912322998,
      "learning_rate": 1.9238944716056825e-05,
      "loss": 5.1541,
      "step": 28500
    },
    {
      "epoch": 14.926931106471816,
      "grad_norm": 3.3003993034362793,
      "learning_rate": 1.9228954509376208e-05,
      "loss": 5.164,
      "step": 28600
    },
    {
      "epoch": 14.979123173277662,
      "grad_norm": 3.451629400253296,
      "learning_rate": 1.9218901788320082e-05,
      "loss": 5.0951,
      "step": 28700
    },
    {
      "epoch": 15.031315240083508,
      "grad_norm": 3.422368288040161,
      "learning_rate": 1.920878662098278e-05,
      "loss": 4.9314,
      "step": 28800
    },
    {
      "epoch": 15.083507306889352,
      "grad_norm": 3.292945623397827,
      "learning_rate": 1.9198609075881647e-05,
      "loss": 4.7542,
      "step": 28900
    },
    {
      "epoch": 15.135699373695198,
      "grad_norm": 3.3153529167175293,
      "learning_rate": 1.918836922195654e-05,
      "loss": 4.7715,
      "step": 29000
    },
    {
      "epoch": 15.187891440501044,
      "grad_norm": 3.4769413471221924,
      "learning_rate": 1.917806712856939e-05,
      "loss": 4.8879,
      "step": 29100
    },
    {
      "epoch": 15.24008350730689,
      "grad_norm": 3.3227767944335938,
      "learning_rate": 1.916770286550372e-05,
      "loss": 4.8928,
      "step": 29200
    },
    {
      "epoch": 15.292275574112734,
      "grad_norm": 3.350118637084961,
      "learning_rate": 1.9157276502964164e-05,
      "loss": 4.8912,
      "step": 29300
    },
    {
      "epoch": 15.34446764091858,
      "grad_norm": 3.450441837310791,
      "learning_rate": 1.9146788111576013e-05,
      "loss": 4.9321,
      "step": 29400
    },
    {
      "epoch": 15.396659707724426,
      "grad_norm": 3.431027412414551,
      "learning_rate": 1.913623776238471e-05,
      "loss": 4.9947,
      "step": 29500
    },
    {
      "epoch": 15.448851774530272,
      "grad_norm": 3.3936643600463867,
      "learning_rate": 1.91256255268554e-05,
      "loss": 5.0122,
      "step": 29600
    },
    {
      "epoch": 15.501043841336116,
      "grad_norm": 3.420715808868408,
      "learning_rate": 1.9114951476872416e-05,
      "loss": 5.0502,
      "step": 29700
    },
    {
      "epoch": 15.553235908141962,
      "grad_norm": 3.408820390701294,
      "learning_rate": 1.9104215684738812e-05,
      "loss": 5.0185,
      "step": 29800
    },
    {
      "epoch": 15.605427974947808,
      "grad_norm": 3.3062551021575928,
      "learning_rate": 1.909341822317586e-05,
      "loss": 5.0942,
      "step": 29900
    },
    {
      "epoch": 15.657620041753653,
      "grad_norm": 3.354543685913086,
      "learning_rate": 1.908255916532257e-05,
      "loss": 5.0235,
      "step": 30000
    },
    {
      "epoch": 15.709812108559499,
      "grad_norm": 3.4149839878082275,
      "learning_rate": 1.9071638584735178e-05,
      "loss": 5.0811,
      "step": 30100
    },
    {
      "epoch": 15.762004175365345,
      "grad_norm": 3.540463447570801,
      "learning_rate": 1.9060656555386674e-05,
      "loss": 5.0377,
      "step": 30200
    },
    {
      "epoch": 15.81419624217119,
      "grad_norm": 3.376540184020996,
      "learning_rate": 1.904961315166627e-05,
      "loss": 5.1128,
      "step": 30300
    },
    {
      "epoch": 15.866388308977035,
      "grad_norm": 3.270016670227051,
      "learning_rate": 1.903850844837892e-05,
      "loss": 5.1325,
      "step": 30400
    },
    {
      "epoch": 15.91858037578288,
      "grad_norm": 3.555612802505493,
      "learning_rate": 1.9027342520744795e-05,
      "loss": 5.1147,
      "step": 30500
    },
    {
      "epoch": 15.970772442588727,
      "grad_norm": 3.469038724899292,
      "learning_rate": 1.9016115444398797e-05,
      "loss": 5.0907,
      "step": 30600
    },
    {
      "epoch": 16.022964509394573,
      "grad_norm": 3.4838173389434814,
      "learning_rate": 1.9004827295390016e-05,
      "loss": 4.9319,
      "step": 30700
    },
    {
      "epoch": 16.075156576200417,
      "grad_norm": 3.433464527130127,
      "learning_rate": 1.899347815018125e-05,
      "loss": 4.7633,
      "step": 30800
    },
    {
      "epoch": 16.127348643006265,
      "grad_norm": 3.3748583793640137,
      "learning_rate": 1.8982068085648446e-05,
      "loss": 4.7361,
      "step": 30900
    },
    {
      "epoch": 16.17954070981211,
      "grad_norm": 3.4580962657928467,
      "learning_rate": 1.8970597179080217e-05,
      "loss": 4.8214,
      "step": 31000
    },
    {
      "epoch": 16.231732776617953,
      "grad_norm": 3.4493627548217773,
      "learning_rate": 1.8959065508177302e-05,
      "loss": 4.9306,
      "step": 31100
    },
    {
      "epoch": 16.2839248434238,
      "grad_norm": 3.4860270023345947,
      "learning_rate": 1.8947473151052037e-05,
      "loss": 4.8646,
      "step": 31200
    },
    {
      "epoch": 16.336116910229645,
      "grad_norm": 3.425762414932251,
      "learning_rate": 1.8935820186227828e-05,
      "loss": 4.924,
      "step": 31300
    },
    {
      "epoch": 16.38830897703549,
      "grad_norm": 3.4885222911834717,
      "learning_rate": 1.892410669263863e-05,
      "loss": 4.9039,
      "step": 31400
    },
    {
      "epoch": 16.440501043841337,
      "grad_norm": 3.4967739582061768,
      "learning_rate": 1.891233274962839e-05,
      "loss": 5.0267,
      "step": 31500
    },
    {
      "epoch": 16.49269311064718,
      "grad_norm": 3.288821220397949,
      "learning_rate": 1.890049843695053e-05,
      "loss": 4.9701,
      "step": 31600
    },
    {
      "epoch": 16.544885177453025,
      "grad_norm": 3.444493055343628,
      "learning_rate": 1.88886038347674e-05,
      "loss": 4.9467,
      "step": 31700
    },
    {
      "epoch": 16.597077244258873,
      "grad_norm": 3.286592483520508,
      "learning_rate": 1.887664902364973e-05,
      "loss": 5.0139,
      "step": 31800
    },
    {
      "epoch": 16.649269311064717,
      "grad_norm": 3.3732564449310303,
      "learning_rate": 1.88646340845761e-05,
      "loss": 5.0037,
      "step": 31900
    },
    {
      "epoch": 16.701461377870565,
      "grad_norm": 3.2916159629821777,
      "learning_rate": 1.8852559098932358e-05,
      "loss": 5.0378,
      "step": 32000
    },
    {
      "epoch": 16.75365344467641,
      "grad_norm": 3.4820339679718018,
      "learning_rate": 1.8840424148511112e-05,
      "loss": 4.9874,
      "step": 32100
    },
    {
      "epoch": 16.805845511482254,
      "grad_norm": 3.347585439682007,
      "learning_rate": 1.882822931551115e-05,
      "loss": 5.0952,
      "step": 32200
    },
    {
      "epoch": 16.8580375782881,
      "grad_norm": 3.5641520023345947,
      "learning_rate": 1.8815974682536882e-05,
      "loss": 5.0864,
      "step": 32300
    },
    {
      "epoch": 16.910229645093946,
      "grad_norm": 3.5270729064941406,
      "learning_rate": 1.8803660332597785e-05,
      "loss": 5.0456,
      "step": 32400
    },
    {
      "epoch": 16.96242171189979,
      "grad_norm": 3.4096744060516357,
      "learning_rate": 1.879128634910785e-05,
      "loss": 5.1163,
      "step": 32500
    },
    {
      "epoch": 17.014613778705638,
      "grad_norm": 3.3388381004333496,
      "learning_rate": 1.8778852815885007e-05,
      "loss": 4.918,
      "step": 32600
    },
    {
      "epoch": 17.06680584551148,
      "grad_norm": 3.4799869060516357,
      "learning_rate": 1.8766359817150558e-05,
      "loss": 4.7937,
      "step": 32700
    },
    {
      "epoch": 17.11899791231733,
      "grad_norm": 3.296919584274292,
      "learning_rate": 1.8753807437528603e-05,
      "loss": 4.7191,
      "step": 32800
    },
    {
      "epoch": 17.171189979123174,
      "grad_norm": 3.3981728553771973,
      "learning_rate": 1.8741195762045482e-05,
      "loss": 4.7864,
      "step": 32900
    },
    {
      "epoch": 17.223382045929018,
      "grad_norm": 3.5032761096954346,
      "learning_rate": 1.872852487612918e-05,
      "loss": 4.8494,
      "step": 33000
    },
    {
      "epoch": 17.275574112734866,
      "grad_norm": 3.4718000888824463,
      "learning_rate": 1.8715794865608764e-05,
      "loss": 4.896,
      "step": 33100
    },
    {
      "epoch": 17.32776617954071,
      "grad_norm": 3.4434258937835693,
      "learning_rate": 1.870300581671379e-05,
      "loss": 4.8623,
      "step": 33200
    },
    {
      "epoch": 17.379958246346554,
      "grad_norm": 3.378950834274292,
      "learning_rate": 1.869015781607372e-05,
      "loss": 4.9484,
      "step": 33300
    },
    {
      "epoch": 17.432150313152402,
      "grad_norm": 3.667534112930298,
      "learning_rate": 1.8677250950717344e-05,
      "loss": 4.9505,
      "step": 33400
    },
    {
      "epoch": 17.484342379958246,
      "grad_norm": 3.5255720615386963,
      "learning_rate": 1.866428530807219e-05,
      "loss": 4.9733,
      "step": 33500
    },
    {
      "epoch": 17.53653444676409,
      "grad_norm": 3.3470497131347656,
      "learning_rate": 1.8651260975963918e-05,
      "loss": 4.943,
      "step": 33600
    },
    {
      "epoch": 17.588726513569938,
      "grad_norm": 3.4839298725128174,
      "learning_rate": 1.8638178042615736e-05,
      "loss": 5.0087,
      "step": 33700
    },
    {
      "epoch": 17.640918580375782,
      "grad_norm": 3.3350486755371094,
      "learning_rate": 1.8625036596647803e-05,
      "loss": 5.0331,
      "step": 33800
    },
    {
      "epoch": 17.69311064718163,
      "grad_norm": 3.1829493045806885,
      "learning_rate": 1.8611836727076628e-05,
      "loss": 5.0094,
      "step": 33900
    },
    {
      "epoch": 17.745302713987474,
      "grad_norm": 3.466740846633911,
      "learning_rate": 1.859857852331446e-05,
      "loss": 5.0449,
      "step": 34000
    },
    {
      "epoch": 17.79749478079332,
      "grad_norm": 3.43501615524292,
      "learning_rate": 1.8585262075168695e-05,
      "loss": 4.9602,
      "step": 34100
    },
    {
      "epoch": 17.849686847599166,
      "grad_norm": 3.6916067600250244,
      "learning_rate": 1.8571887472841244e-05,
      "loss": 5.0094,
      "step": 34200
    },
    {
      "epoch": 17.90187891440501,
      "grad_norm": 3.4461112022399902,
      "learning_rate": 1.8558454806927963e-05,
      "loss": 4.9954,
      "step": 34300
    },
    {
      "epoch": 17.954070981210855,
      "grad_norm": 3.300931692123413,
      "learning_rate": 1.8544964168417995e-05,
      "loss": 4.9694,
      "step": 34400
    },
    {
      "epoch": 18.006263048016702,
      "grad_norm": 3.2222344875335693,
      "learning_rate": 1.853141564869318e-05,
      "loss": 4.9803,
      "step": 34500
    },
    {
      "epoch": 18.058455114822547,
      "grad_norm": 3.3785834312438965,
      "learning_rate": 1.851780933952743e-05,
      "loss": 4.6618,
      "step": 34600
    },
    {
      "epoch": 18.110647181628394,
      "grad_norm": 3.399691581726074,
      "learning_rate": 1.8504145333086106e-05,
      "loss": 4.7,
      "step": 34700
    },
    {
      "epoch": 18.16283924843424,
      "grad_norm": 3.315654993057251,
      "learning_rate": 1.849042372192539e-05,
      "loss": 4.7939,
      "step": 34800
    },
    {
      "epoch": 18.215031315240083,
      "grad_norm": 3.2705891132354736,
      "learning_rate": 1.8476644598991674e-05,
      "loss": 4.7773,
      "step": 34900
    },
    {
      "epoch": 18.26722338204593,
      "grad_norm": 3.306627035140991,
      "learning_rate": 1.84628080576209e-05,
      "loss": 4.8516,
      "step": 35000
    },
    {
      "epoch": 18.319415448851775,
      "grad_norm": 3.4620232582092285,
      "learning_rate": 1.8448914191537972e-05,
      "loss": 4.8861,
      "step": 35100
    },
    {
      "epoch": 18.37160751565762,
      "grad_norm": 3.355099678039551,
      "learning_rate": 1.843496309485607e-05,
      "loss": 4.9413,
      "step": 35200
    },
    {
      "epoch": 18.423799582463467,
      "grad_norm": 3.656018018722534,
      "learning_rate": 1.8420954862076052e-05,
      "loss": 4.8514,
      "step": 35300
    },
    {
      "epoch": 18.47599164926931,
      "grad_norm": 3.3231770992279053,
      "learning_rate": 1.8406889588085802e-05,
      "loss": 4.8834,
      "step": 35400
    },
    {
      "epoch": 18.528183716075155,
      "grad_norm": 3.5412232875823975,
      "learning_rate": 1.8392767368159575e-05,
      "loss": 4.9486,
      "step": 35500
    },
    {
      "epoch": 18.580375782881003,
      "grad_norm": 3.4337539672851562,
      "learning_rate": 1.8378588297957372e-05,
      "loss": 4.9198,
      "step": 35600
    },
    {
      "epoch": 18.632567849686847,
      "grad_norm": 3.527585744857788,
      "learning_rate": 1.8364352473524274e-05,
      "loss": 4.9819,
      "step": 35700
    },
    {
      "epoch": 18.684759916492695,
      "grad_norm": 3.103686571121216,
      "learning_rate": 1.8350059991289807e-05,
      "loss": 4.9308,
      "step": 35800
    },
    {
      "epoch": 18.73695198329854,
      "grad_norm": 3.4181110858917236,
      "learning_rate": 1.833571094806728e-05,
      "loss": 4.9565,
      "step": 35900
    },
    {
      "epoch": 18.789144050104383,
      "grad_norm": 3.346856117248535,
      "learning_rate": 1.832130544105312e-05,
      "loss": 4.9823,
      "step": 36000
    },
    {
      "epoch": 18.84133611691023,
      "grad_norm": 3.234431505203247,
      "learning_rate": 1.8306843567826236e-05,
      "loss": 4.9922,
      "step": 36100
    },
    {
      "epoch": 18.893528183716075,
      "grad_norm": 3.425056219100952,
      "learning_rate": 1.8292325426347345e-05,
      "loss": 5.0074,
      "step": 36200
    },
    {
      "epoch": 18.94572025052192,
      "grad_norm": 3.314201593399048,
      "learning_rate": 1.82777511149583e-05,
      "loss": 5.0898,
      "step": 36300
    },
    {
      "epoch": 18.997912317327767,
      "grad_norm": 3.370657444000244,
      "learning_rate": 1.8263120732381447e-05,
      "loss": 4.9762,
      "step": 36400
    },
    {
      "epoch": 19.05010438413361,
      "grad_norm": 3.158978223800659,
      "learning_rate": 1.8248434377718926e-05,
      "loss": 4.7132,
      "step": 36500
    },
    {
      "epoch": 19.102296450939455,
      "grad_norm": 3.290355682373047,
      "learning_rate": 1.8233692150452038e-05,
      "loss": 4.7351,
      "step": 36600
    },
    {
      "epoch": 19.154488517745303,
      "grad_norm": 3.463367223739624,
      "learning_rate": 1.821889415044053e-05,
      "loss": 4.7553,
      "step": 36700
    },
    {
      "epoch": 19.206680584551147,
      "grad_norm": 3.390824556350708,
      "learning_rate": 1.820404047792195e-05,
      "loss": 4.7562,
      "step": 36800
    },
    {
      "epoch": 19.258872651356995,
      "grad_norm": 3.4200849533081055,
      "learning_rate": 1.818913123351094e-05,
      "loss": 4.7996,
      "step": 36900
    },
    {
      "epoch": 19.31106471816284,
      "grad_norm": 3.298551321029663,
      "learning_rate": 1.8174166518198596e-05,
      "loss": 4.8456,
      "step": 37000
    },
    {
      "epoch": 19.363256784968684,
      "grad_norm": 3.3061890602111816,
      "learning_rate": 1.8159146433351745e-05,
      "loss": 4.9362,
      "step": 37100
    },
    {
      "epoch": 19.41544885177453,
      "grad_norm": 3.3644604682922363,
      "learning_rate": 1.814407108071226e-05,
      "loss": 4.845,
      "step": 37200
    },
    {
      "epoch": 19.467640918580376,
      "grad_norm": 3.3406927585601807,
      "learning_rate": 1.8128940562396404e-05,
      "loss": 4.8988,
      "step": 37300
    },
    {
      "epoch": 19.51983298538622,
      "grad_norm": 3.267392873764038,
      "learning_rate": 1.8113754980894097e-05,
      "loss": 4.9105,
      "step": 37400
    },
    {
      "epoch": 19.572025052192068,
      "grad_norm": 3.3123831748962402,
      "learning_rate": 1.8098514439068264e-05,
      "loss": 4.9032,
      "step": 37500
    },
    {
      "epoch": 19.62421711899791,
      "grad_norm": 3.4771745204925537,
      "learning_rate": 1.80832190401541e-05,
      "loss": 4.9677,
      "step": 37600
    },
    {
      "epoch": 19.67640918580376,
      "grad_norm": 3.3067874908447266,
      "learning_rate": 1.8067868887758386e-05,
      "loss": 4.8477,
      "step": 37700
    },
    {
      "epoch": 19.728601252609604,
      "grad_norm": 3.2225699424743652,
      "learning_rate": 1.8052464085858795e-05,
      "loss": 4.9595,
      "step": 37800
    },
    {
      "epoch": 19.780793319415448,
      "grad_norm": 3.4322588443756104,
      "learning_rate": 1.8037004738803185e-05,
      "loss": 4.8823,
      "step": 37900
    },
    {
      "epoch": 19.832985386221296,
      "grad_norm": 3.361478567123413,
      "learning_rate": 1.8021490951308866e-05,
      "loss": 5.0055,
      "step": 38000
    },
    {
      "epoch": 19.88517745302714,
      "grad_norm": 3.4251294136047363,
      "learning_rate": 1.8005922828461943e-05,
      "loss": 4.9799,
      "step": 38100
    },
    {
      "epoch": 19.937369519832984,
      "grad_norm": 3.2375199794769287,
      "learning_rate": 1.7990300475716545e-05,
      "loss": 4.9158,
      "step": 38200
    },
    {
      "epoch": 19.989561586638832,
      "grad_norm": 3.5239574909210205,
      "learning_rate": 1.797462399889416e-05,
      "loss": 4.951,
      "step": 38300
    },
    {
      "epoch": 20.041753653444676,
      "grad_norm": 3.167942762374878,
      "learning_rate": 1.7958893504182887e-05,
      "loss": 4.7518,
      "step": 38400
    },
    {
      "epoch": 20.09394572025052,
      "grad_norm": 3.382749557495117,
      "learning_rate": 1.794310909813673e-05,
      "loss": 4.656,
      "step": 38500
    },
    {
      "epoch": 20.146137787056368,
      "grad_norm": 3.519723653793335,
      "learning_rate": 1.7927270887674874e-05,
      "loss": 4.761,
      "step": 38600
    },
    {
      "epoch": 20.198329853862212,
      "grad_norm": 3.3015642166137695,
      "learning_rate": 1.791137898008096e-05,
      "loss": 4.803,
      "step": 38700
    },
    {
      "epoch": 20.25052192066806,
      "grad_norm": 3.4405808448791504,
      "learning_rate": 1.7895433483002356e-05,
      "loss": 4.7522,
      "step": 38800
    },
    {
      "epoch": 20.302713987473904,
      "grad_norm": 3.63765549659729,
      "learning_rate": 1.787943450444943e-05,
      "loss": 4.835,
      "step": 38900
    },
    {
      "epoch": 20.35490605427975,
      "grad_norm": 3.1389262676239014,
      "learning_rate": 1.7863382152794826e-05,
      "loss": 4.7644,
      "step": 39000
    },
    {
      "epoch": 20.407098121085596,
      "grad_norm": 3.3782644271850586,
      "learning_rate": 1.7847276536772712e-05,
      "loss": 4.8005,
      "step": 39100
    },
    {
      "epoch": 20.45929018789144,
      "grad_norm": 3.3790512084960938,
      "learning_rate": 1.7831117765478063e-05,
      "loss": 4.8286,
      "step": 39200
    },
    {
      "epoch": 20.511482254697285,
      "grad_norm": 3.2730531692504883,
      "learning_rate": 1.781490594836591e-05,
      "loss": 4.8343,
      "step": 39300
    },
    {
      "epoch": 20.563674321503132,
      "grad_norm": 3.407341241836548,
      "learning_rate": 1.7798641195250596e-05,
      "loss": 4.8802,
      "step": 39400
    },
    {
      "epoch": 20.615866388308977,
      "grad_norm": 3.3671720027923584,
      "learning_rate": 1.778232361630505e-05,
      "loss": 4.902,
      "step": 39500
    },
    {
      "epoch": 20.668058455114824,
      "grad_norm": 3.2564964294433594,
      "learning_rate": 1.7765953322060013e-05,
      "loss": 4.9158,
      "step": 39600
    },
    {
      "epoch": 20.72025052192067,
      "grad_norm": 3.2774691581726074,
      "learning_rate": 1.774953042340332e-05,
      "loss": 4.8971,
      "step": 39700
    },
    {
      "epoch": 20.772442588726513,
      "grad_norm": 3.3085968494415283,
      "learning_rate": 1.7733055031579125e-05,
      "loss": 4.9233,
      "step": 39800
    },
    {
      "epoch": 20.82463465553236,
      "grad_norm": 3.4113824367523193,
      "learning_rate": 1.7716527258187155e-05,
      "loss": 4.9658,
      "step": 39900
    },
    {
      "epoch": 20.876826722338205,
      "grad_norm": 3.4444849491119385,
      "learning_rate": 1.7699947215181963e-05,
      "loss": 5.016,
      "step": 40000
    },
    {
      "epoch": 20.92901878914405,
      "grad_norm": 3.4898579120635986,
      "learning_rate": 1.7683315014872152e-05,
      "loss": 5.0029,
      "step": 40100
    },
    {
      "epoch": 20.981210855949897,
      "grad_norm": 3.53027081489563,
      "learning_rate": 1.7666630769919634e-05,
      "loss": 5.0535,
      "step": 40200
    },
    {
      "epoch": 21.03340292275574,
      "grad_norm": 3.399956226348877,
      "learning_rate": 1.7649894593338853e-05,
      "loss": 4.7419,
      "step": 40300
    },
    {
      "epoch": 21.085594989561585,
      "grad_norm": 3.372777223587036,
      "learning_rate": 1.7633106598496013e-05,
      "loss": 4.691,
      "step": 40400
    },
    {
      "epoch": 21.137787056367433,
      "grad_norm": 3.378377676010132,
      "learning_rate": 1.761626689910834e-05,
      "loss": 4.652,
      "step": 40500
    },
    {
      "epoch": 21.189979123173277,
      "grad_norm": 3.374513864517212,
      "learning_rate": 1.7599375609243282e-05,
      "loss": 4.7395,
      "step": 40600
    },
    {
      "epoch": 21.242171189979125,
      "grad_norm": 3.3401315212249756,
      "learning_rate": 1.758243284331774e-05,
      "loss": 4.8073,
      "step": 40700
    },
    {
      "epoch": 21.29436325678497,
      "grad_norm": 3.3711957931518555,
      "learning_rate": 1.756543871609731e-05,
      "loss": 4.7314,
      "step": 40800
    },
    {
      "epoch": 21.346555323590813,
      "grad_norm": 3.3223793506622314,
      "learning_rate": 1.754839334269549e-05,
      "loss": 4.8172,
      "step": 40900
    },
    {
      "epoch": 21.39874739039666,
      "grad_norm": 3.371060848236084,
      "learning_rate": 1.753129683857291e-05,
      "loss": 4.8375,
      "step": 41000
    },
    {
      "epoch": 21.450939457202505,
      "grad_norm": 3.3652820587158203,
      "learning_rate": 1.7514149319536538e-05,
      "loss": 4.7958,
      "step": 41100
    },
    {
      "epoch": 21.50313152400835,
      "grad_norm": 3.3720860481262207,
      "learning_rate": 1.7496950901738914e-05,
      "loss": 4.8715,
      "step": 41200
    },
    {
      "epoch": 21.555323590814197,
      "grad_norm": 3.537747859954834,
      "learning_rate": 1.7479701701677337e-05,
      "loss": 4.8666,
      "step": 41300
    },
    {
      "epoch": 21.60751565762004,
      "grad_norm": 3.626697301864624,
      "learning_rate": 1.7462401836193108e-05,
      "loss": 4.896,
      "step": 41400
    },
    {
      "epoch": 21.659707724425886,
      "grad_norm": 3.3902695178985596,
      "learning_rate": 1.7445051422470706e-05,
      "loss": 4.8951,
      "step": 41500
    },
    {
      "epoch": 21.711899791231733,
      "grad_norm": 3.3878097534179688,
      "learning_rate": 1.7427650578037014e-05,
      "loss": 4.8984,
      "step": 41600
    },
    {
      "epoch": 21.764091858037578,
      "grad_norm": 3.4461145401000977,
      "learning_rate": 1.741019942076053e-05,
      "loss": 4.9248,
      "step": 41700
    },
    {
      "epoch": 21.816283924843425,
      "grad_norm": 3.3563270568847656,
      "learning_rate": 1.739269806885054e-05,
      "loss": 4.8523,
      "step": 41800
    },
    {
      "epoch": 21.86847599164927,
      "grad_norm": 3.2246034145355225,
      "learning_rate": 1.7375146640856347e-05,
      "loss": 4.9486,
      "step": 41900
    },
    {
      "epoch": 21.920668058455114,
      "grad_norm": 3.3267526626586914,
      "learning_rate": 1.735754525566645e-05,
      "loss": 4.9033,
      "step": 42000
    },
    {
      "epoch": 21.97286012526096,
      "grad_norm": 3.5866458415985107,
      "learning_rate": 1.7339894032507746e-05,
      "loss": 4.9343,
      "step": 42100
    },
    {
      "epoch": 22.025052192066806,
      "grad_norm": 3.161191701889038,
      "learning_rate": 1.732219309094472e-05,
      "loss": 4.7311,
      "step": 42200
    },
    {
      "epoch": 22.07724425887265,
      "grad_norm": 3.2794406414031982,
      "learning_rate": 1.7304442550878636e-05,
      "loss": 4.6065,
      "step": 42300
    },
    {
      "epoch": 22.129436325678498,
      "grad_norm": 3.4205472469329834,
      "learning_rate": 1.7286642532546724e-05,
      "loss": 4.6866,
      "step": 42400
    },
    {
      "epoch": 22.181628392484342,
      "grad_norm": 3.436525583267212,
      "learning_rate": 1.7268793156521362e-05,
      "loss": 4.6958,
      "step": 42500
    },
    {
      "epoch": 22.23382045929019,
      "grad_norm": 3.359814167022705,
      "learning_rate": 1.7250894543709275e-05,
      "loss": 4.7392,
      "step": 42600
    },
    {
      "epoch": 22.286012526096034,
      "grad_norm": 3.1500244140625,
      "learning_rate": 1.723294681535069e-05,
      "loss": 4.7752,
      "step": 42700
    },
    {
      "epoch": 22.338204592901878,
      "grad_norm": 3.337022304534912,
      "learning_rate": 1.721495009301854e-05,
      "loss": 4.771,
      "step": 42800
    },
    {
      "epoch": 22.390396659707726,
      "grad_norm": 3.259995460510254,
      "learning_rate": 1.7196904498617626e-05,
      "loss": 4.8256,
      "step": 42900
    },
    {
      "epoch": 22.44258872651357,
      "grad_norm": 3.226658344268799,
      "learning_rate": 1.717881015438379e-05,
      "loss": 4.8002,
      "step": 43000
    },
    {
      "epoch": 22.494780793319414,
      "grad_norm": 3.1655852794647217,
      "learning_rate": 1.71606671828831e-05,
      "loss": 4.8285,
      "step": 43100
    },
    {
      "epoch": 22.546972860125262,
      "grad_norm": 3.2486917972564697,
      "learning_rate": 1.7142475707011007e-05,
      "loss": 4.8232,
      "step": 43200
    },
    {
      "epoch": 22.599164926931106,
      "grad_norm": 3.3779540061950684,
      "learning_rate": 1.712423584999151e-05,
      "loss": 4.9254,
      "step": 43300
    },
    {
      "epoch": 22.65135699373695,
      "grad_norm": 3.155477523803711,
      "learning_rate": 1.7105947735376353e-05,
      "loss": 4.9029,
      "step": 43400
    },
    {
      "epoch": 22.703549060542798,
      "grad_norm": 3.461657762527466,
      "learning_rate": 1.7087611487044133e-05,
      "loss": 4.8198,
      "step": 43500
    },
    {
      "epoch": 22.755741127348642,
      "grad_norm": 3.3835558891296387,
      "learning_rate": 1.7069227229199512e-05,
      "loss": 4.8992,
      "step": 43600
    },
    {
      "epoch": 22.80793319415449,
      "grad_norm": 3.3458340167999268,
      "learning_rate": 1.7050795086372345e-05,
      "loss": 4.8506,
      "step": 43700
    },
    {
      "epoch": 22.860125260960334,
      "grad_norm": 3.391472816467285,
      "learning_rate": 1.703231518341685e-05,
      "loss": 4.9057,
      "step": 43800
    },
    {
      "epoch": 22.91231732776618,
      "grad_norm": 3.3418052196502686,
      "learning_rate": 1.7013787645510766e-05,
      "loss": 4.9224,
      "step": 43900
    },
    {
      "epoch": 22.964509394572026,
      "grad_norm": 3.391741991043091,
      "learning_rate": 1.6995212598154476e-05,
      "loss": 4.8636,
      "step": 44000
    },
    {
      "epoch": 23.01670146137787,
      "grad_norm": 3.229424238204956,
      "learning_rate": 1.6976590167170208e-05,
      "loss": 4.7386,
      "step": 44100
    },
    {
      "epoch": 23.068893528183715,
      "grad_norm": 3.204946517944336,
      "learning_rate": 1.695792047870113e-05,
      "loss": 4.6231,
      "step": 44200
    },
    {
      "epoch": 23.121085594989562,
      "grad_norm": 3.424361228942871,
      "learning_rate": 1.6939203659210534e-05,
      "loss": 4.5938,
      "step": 44300
    },
    {
      "epoch": 23.173277661795407,
      "grad_norm": 3.1529605388641357,
      "learning_rate": 1.6920439835480954e-05,
      "loss": 4.6651,
      "step": 44400
    },
    {
      "epoch": 23.225469728601254,
      "grad_norm": 3.327277183532715,
      "learning_rate": 1.690162913461333e-05,
      "loss": 4.72,
      "step": 44500
    },
    {
      "epoch": 23.2776617954071,
      "grad_norm": 3.407095193862915,
      "learning_rate": 1.688277168402612e-05,
      "loss": 4.6998,
      "step": 44600
    },
    {
      "epoch": 23.329853862212943,
      "grad_norm": 3.313267707824707,
      "learning_rate": 1.6863867611454456e-05,
      "loss": 4.763,
      "step": 44700
    },
    {
      "epoch": 23.38204592901879,
      "grad_norm": 3.2226951122283936,
      "learning_rate": 1.6844917044949282e-05,
      "loss": 4.809,
      "step": 44800
    },
    {
      "epoch": 23.434237995824635,
      "grad_norm": 3.2073323726654053,
      "learning_rate": 1.682592011287648e-05,
      "loss": 4.7442,
      "step": 44900
    },
    {
      "epoch": 23.48643006263048,
      "grad_norm": 3.3558857440948486,
      "learning_rate": 1.680687694391598e-05,
      "loss": 4.8625,
      "step": 45000
    },
    {
      "epoch": 23.538622129436327,
      "grad_norm": 3.260936975479126,
      "learning_rate": 1.6787787667060933e-05,
      "loss": 4.8224,
      "step": 45100
    },
    {
      "epoch": 23.59081419624217,
      "grad_norm": 3.1588680744171143,
      "learning_rate": 1.6768652411616795e-05,
      "loss": 4.8638,
      "step": 45200
    },
    {
      "epoch": 23.643006263048015,
      "grad_norm": 3.208120107650757,
      "learning_rate": 1.6749471307200473e-05,
      "loss": 4.8509,
      "step": 45300
    },
    {
      "epoch": 23.695198329853863,
      "grad_norm": 3.5670981407165527,
      "learning_rate": 1.6730244483739447e-05,
      "loss": 4.8768,
      "step": 45400
    },
    {
      "epoch": 23.747390396659707,
      "grad_norm": 3.3604788780212402,
      "learning_rate": 1.6710972071470878e-05,
      "loss": 4.8519,
      "step": 45500
    },
    {
      "epoch": 23.799582463465555,
      "grad_norm": 3.435948371887207,
      "learning_rate": 1.6691654200940734e-05,
      "loss": 4.8993,
      "step": 45600
    },
    {
      "epoch": 23.8517745302714,
      "grad_norm": 3.306457757949829,
      "learning_rate": 1.6672291003002913e-05,
      "loss": 4.9572,
      "step": 45700
    },
    {
      "epoch": 23.903966597077243,
      "grad_norm": 3.1640427112579346,
      "learning_rate": 1.6652882608818333e-05,
      "loss": 4.7666,
      "step": 45800
    },
    {
      "epoch": 23.95615866388309,
      "grad_norm": 3.3086395263671875,
      "learning_rate": 1.6633429149854076e-05,
      "loss": 4.8781,
      "step": 45900
    },
    {
      "epoch": 24.008350730688935,
      "grad_norm": 3.286233425140381,
      "learning_rate": 1.6613930757882468e-05,
      "loss": 4.8925,
      "step": 46000
    },
    {
      "epoch": 24.06054279749478,
      "grad_norm": 3.191985845565796,
      "learning_rate": 1.65943875649802e-05,
      "loss": 4.6586,
      "step": 46100
    },
    {
      "epoch": 24.112734864300627,
      "grad_norm": 3.364009380340576,
      "learning_rate": 1.657479970352744e-05,
      "loss": 4.6268,
      "step": 46200
    },
    {
      "epoch": 24.16492693110647,
      "grad_norm": 3.3079073429107666,
      "learning_rate": 1.655516730620692e-05,
      "loss": 4.6325,
      "step": 46300
    },
    {
      "epoch": 24.217118997912316,
      "grad_norm": 3.274810791015625,
      "learning_rate": 1.653549050600305e-05,
      "loss": 4.6045,
      "step": 46400
    },
    {
      "epoch": 24.269311064718163,
      "grad_norm": 3.3194141387939453,
      "learning_rate": 1.6515769436201013e-05,
      "loss": 4.7052,
      "step": 46500
    },
    {
      "epoch": 24.321503131524008,
      "grad_norm": 3.5387659072875977,
      "learning_rate": 1.6496004230385857e-05,
      "loss": 4.7198,
      "step": 46600
    },
    {
      "epoch": 24.373695198329855,
      "grad_norm": 3.2475192546844482,
      "learning_rate": 1.64761950224416e-05,
      "loss": 4.7482,
      "step": 46700
    },
    {
      "epoch": 24.4258872651357,
      "grad_norm": 3.4773285388946533,
      "learning_rate": 1.6456341946550315e-05,
      "loss": 4.7863,
      "step": 46800
    },
    {
      "epoch": 24.478079331941544,
      "grad_norm": 3.4153735637664795,
      "learning_rate": 1.6436445137191227e-05,
      "loss": 4.755,
      "step": 46900
    },
    {
      "epoch": 24.53027139874739,
      "grad_norm": 3.3571252822875977,
      "learning_rate": 1.6416504729139804e-05,
      "loss": 4.8213,
      "step": 47000
    },
    {
      "epoch": 24.582463465553236,
      "grad_norm": 3.256282329559326,
      "learning_rate": 1.6396520857466824e-05,
      "loss": 4.7656,
      "step": 47100
    },
    {
      "epoch": 24.63465553235908,
      "grad_norm": 3.268573760986328,
      "learning_rate": 1.6376493657537497e-05,
      "loss": 4.796,
      "step": 47200
    },
    {
      "epoch": 24.686847599164928,
      "grad_norm": 3.3340020179748535,
      "learning_rate": 1.63564232650105e-05,
      "loss": 4.8158,
      "step": 47300
    },
    {
      "epoch": 24.739039665970772,
      "grad_norm": 3.438063383102417,
      "learning_rate": 1.6336309815837116e-05,
      "loss": 4.8793,
      "step": 47400
    },
    {
      "epoch": 24.79123173277662,
      "grad_norm": 3.5457818508148193,
      "learning_rate": 1.6316153446260253e-05,
      "loss": 4.8487,
      "step": 47500
    },
    {
      "epoch": 24.843423799582464,
      "grad_norm": 3.3959121704101562,
      "learning_rate": 1.629595429281357e-05,
      "loss": 4.8754,
      "step": 47600
    },
    {
      "epoch": 24.895615866388308,
      "grad_norm": 3.3408429622650146,
      "learning_rate": 1.6275712492320516e-05,
      "loss": 4.8866,
      "step": 47700
    },
    {
      "epoch": 24.947807933194156,
      "grad_norm": 3.3476479053497314,
      "learning_rate": 1.6255428181893438e-05,
      "loss": 4.8926,
      "step": 47800
    },
    {
      "epoch": 25.0,
      "grad_norm": 3.4151175022125244,
      "learning_rate": 1.6235101498932614e-05,
      "loss": 4.9173,
      "step": 47900
    },
    {
      "epoch": 25.052192066805844,
      "grad_norm": 3.1499733924865723,
      "learning_rate": 1.6214732581125358e-05,
      "loss": 4.5672,
      "step": 48000
    },
    {
      "epoch": 25.104384133611692,
      "grad_norm": 3.680316209793091,
      "learning_rate": 1.619432156644506e-05,
      "loss": 4.6543,
      "step": 48100
    },
    {
      "epoch": 25.156576200417536,
      "grad_norm": 3.2111740112304688,
      "learning_rate": 1.6173868593150265e-05,
      "loss": 4.61,
      "step": 48200
    },
    {
      "epoch": 25.20876826722338,
      "grad_norm": 3.3080365657806396,
      "learning_rate": 1.615337379978374e-05,
      "loss": 4.6146,
      "step": 48300
    },
    {
      "epoch": 25.260960334029228,
      "grad_norm": 3.2674458026885986,
      "learning_rate": 1.613283732517152e-05,
      "loss": 4.6983,
      "step": 48400
    },
    {
      "epoch": 25.313152400835072,
      "grad_norm": 3.2715141773223877,
      "learning_rate": 1.611225930842199e-05,
      "loss": 4.7196,
      "step": 48500
    },
    {
      "epoch": 25.36534446764092,
      "grad_norm": 3.198162317276001,
      "learning_rate": 1.609163988892491e-05,
      "loss": 4.7364,
      "step": 48600
    },
    {
      "epoch": 25.417536534446764,
      "grad_norm": 3.2079074382781982,
      "learning_rate": 1.6070979206350516e-05,
      "loss": 4.6905,
      "step": 48700
    },
    {
      "epoch": 25.46972860125261,
      "grad_norm": 3.2628490924835205,
      "learning_rate": 1.6050277400648527e-05,
      "loss": 4.7855,
      "step": 48800
    },
    {
      "epoch": 25.521920668058456,
      "grad_norm": 3.2863047122955322,
      "learning_rate": 1.602953461204723e-05,
      "loss": 4.7642,
      "step": 48900
    },
    {
      "epoch": 25.5741127348643,
      "grad_norm": 3.4375483989715576,
      "learning_rate": 1.6008750981052514e-05,
      "loss": 4.7863,
      "step": 49000
    },
    {
      "epoch": 25.626304801670145,
      "grad_norm": 3.2922163009643555,
      "learning_rate": 1.598792664844693e-05,
      "loss": 4.8504,
      "step": 49100
    },
    {
      "epoch": 25.678496868475992,
      "grad_norm": 3.4539387226104736,
      "learning_rate": 1.5967061755288723e-05,
      "loss": 4.8212,
      "step": 49200
    },
    {
      "epoch": 25.730688935281837,
      "grad_norm": 3.5203371047973633,
      "learning_rate": 1.594615644291088e-05,
      "loss": 4.8385,
      "step": 49300
    },
    {
      "epoch": 25.782881002087684,
      "grad_norm": 3.77875018119812,
      "learning_rate": 1.592521085292019e-05,
      "loss": 4.8461,
      "step": 49400
    },
    {
      "epoch": 25.83507306889353,
      "grad_norm": 3.4378135204315186,
      "learning_rate": 1.5904225127196265e-05,
      "loss": 4.8229,
      "step": 49500
    },
    {
      "epoch": 25.887265135699373,
      "grad_norm": 3.317199468612671,
      "learning_rate": 1.588319940789058e-05,
      "loss": 4.8381,
      "step": 49600
    },
    {
      "epoch": 25.93945720250522,
      "grad_norm": 3.2496087551116943,
      "learning_rate": 1.5862133837425526e-05,
      "loss": 4.8423,
      "step": 49700
    },
    {
      "epoch": 25.991649269311065,
      "grad_norm": 3.202678918838501,
      "learning_rate": 1.584102855849342e-05,
      "loss": 4.9023,
      "step": 49800
    },
    {
      "epoch": 26.04384133611691,
      "grad_norm": 3.2272984981536865,
      "learning_rate": 1.5819883714055565e-05,
      "loss": 4.6295,
      "step": 49900
    },
    {
      "epoch": 26.096033402922757,
      "grad_norm": 3.201411008834839,
      "learning_rate": 1.5798699447341273e-05,
      "loss": 4.5632,
      "step": 50000
    },
    {
      "epoch": 26.1482254697286,
      "grad_norm": 3.174661874771118,
      "learning_rate": 1.577747590184688e-05,
      "loss": 4.6178,
      "step": 50100
    },
    {
      "epoch": 26.200417536534445,
      "grad_norm": 3.23684024810791,
      "learning_rate": 1.5756213221334787e-05,
      "loss": 4.6482,
      "step": 50200
    },
    {
      "epoch": 26.252609603340293,
      "grad_norm": 3.256967782974243,
      "learning_rate": 1.5734911549832492e-05,
      "loss": 4.6312,
      "step": 50300
    },
    {
      "epoch": 26.304801670146137,
      "grad_norm": 3.2521631717681885,
      "learning_rate": 1.5713571031631607e-05,
      "loss": 4.7603,
      "step": 50400
    },
    {
      "epoch": 26.356993736951985,
      "grad_norm": 3.4985740184783936,
      "learning_rate": 1.5692191811286873e-05,
      "loss": 4.6869,
      "step": 50500
    },
    {
      "epoch": 26.40918580375783,
      "grad_norm": 3.0450949668884277,
      "learning_rate": 1.567077403361519e-05,
      "loss": 4.7587,
      "step": 50600
    },
    {
      "epoch": 26.461377870563673,
      "grad_norm": 3.360532522201538,
      "learning_rate": 1.5649317843694647e-05,
      "loss": 4.7691,
      "step": 50700
    },
    {
      "epoch": 26.51356993736952,
      "grad_norm": 3.241459369659424,
      "learning_rate": 1.562782338686351e-05,
      "loss": 4.7524,
      "step": 50800
    },
    {
      "epoch": 26.565762004175365,
      "grad_norm": 3.2961058616638184,
      "learning_rate": 1.560629080871926e-05,
      "loss": 4.8011,
      "step": 50900
    },
    {
      "epoch": 26.61795407098121,
      "grad_norm": 3.355480909347534,
      "learning_rate": 1.5584720255117614e-05,
      "loss": 4.7635,
      "step": 51000
    },
    {
      "epoch": 26.670146137787057,
      "grad_norm": 3.3422327041625977,
      "learning_rate": 1.5563111872171503e-05,
      "loss": 4.8042,
      "step": 51100
    },
    {
      "epoch": 26.7223382045929,
      "grad_norm": 3.6112658977508545,
      "learning_rate": 1.5541465806250114e-05,
      "loss": 4.8022,
      "step": 51200
    },
    {
      "epoch": 26.774530271398746,
      "grad_norm": 3.2128145694732666,
      "learning_rate": 1.5519782203977894e-05,
      "loss": 4.8372,
      "step": 51300
    },
    {
      "epoch": 26.826722338204593,
      "grad_norm": 3.340045690536499,
      "learning_rate": 1.549806121223354e-05,
      "loss": 4.8397,
      "step": 51400
    },
    {
      "epoch": 26.878914405010438,
      "grad_norm": 3.307807683944702,
      "learning_rate": 1.5476302978149018e-05,
      "loss": 4.8459,
      "step": 51500
    },
    {
      "epoch": 26.931106471816285,
      "grad_norm": 3.3427062034606934,
      "learning_rate": 1.5454507649108566e-05,
      "loss": 4.7691,
      "step": 51600
    },
    {
      "epoch": 26.98329853862213,
      "grad_norm": 3.239816904067993,
      "learning_rate": 1.5432675372747698e-05,
      "loss": 4.8669,
      "step": 51700
    },
    {
      "epoch": 27.035490605427974,
      "grad_norm": 3.2262961864471436,
      "learning_rate": 1.5410806296952182e-05,
      "loss": 4.559,
      "step": 51800
    },
    {
      "epoch": 27.08768267223382,
      "grad_norm": 3.312995433807373,
      "learning_rate": 1.5388900569857076e-05,
      "loss": 4.6115,
      "step": 51900
    },
    {
      "epoch": 27.139874739039666,
      "grad_norm": 3.445101737976074,
      "learning_rate": 1.5366958339845684e-05,
      "loss": 4.6175,
      "step": 52000
    },
    {
      "epoch": 27.19206680584551,
      "grad_norm": 3.830345630645752,
      "learning_rate": 1.5344979755548587e-05,
      "loss": 4.5727,
      "step": 52100
    },
    {
      "epoch": 27.244258872651358,
      "grad_norm": 3.3602466583251953,
      "learning_rate": 1.5322964965842607e-05,
      "loss": 4.6585,
      "step": 52200
    },
    {
      "epoch": 27.296450939457202,
      "grad_norm": 3.3544692993164062,
      "learning_rate": 1.5300914119849822e-05,
      "loss": 4.7341,
      "step": 52300
    },
    {
      "epoch": 27.34864300626305,
      "grad_norm": 3.2830655574798584,
      "learning_rate": 1.527882736693654e-05,
      "loss": 4.6478,
      "step": 52400
    },
    {
      "epoch": 27.400835073068894,
      "grad_norm": 3.274656295776367,
      "learning_rate": 1.5256704856712294e-05,
      "loss": 4.6783,
      "step": 52500
    },
    {
      "epoch": 27.453027139874738,
      "grad_norm": 3.281418561935425,
      "learning_rate": 1.5234546739028822e-05,
      "loss": 4.7353,
      "step": 52600
    },
    {
      "epoch": 27.505219206680586,
      "grad_norm": 3.267838716506958,
      "learning_rate": 1.5212353163979062e-05,
      "loss": 4.7837,
      "step": 52700
    },
    {
      "epoch": 27.55741127348643,
      "grad_norm": 3.253873348236084,
      "learning_rate": 1.5190124281896132e-05,
      "loss": 4.7375,
      "step": 52800
    },
    {
      "epoch": 27.609603340292274,
      "grad_norm": 3.231304168701172,
      "learning_rate": 1.5167860243352308e-05,
      "loss": 4.7148,
      "step": 52900
    },
    {
      "epoch": 27.661795407098122,
      "grad_norm": 3.39961838722229,
      "learning_rate": 1.5145561199158003e-05,
      "loss": 4.7805,
      "step": 53000
    },
    {
      "epoch": 27.713987473903966,
      "grad_norm": 3.245112419128418,
      "learning_rate": 1.5123227300360753e-05,
      "loss": 4.7222,
      "step": 53100
    },
    {
      "epoch": 27.76617954070981,
      "grad_norm": 3.5294787883758545,
      "learning_rate": 1.5100858698244188e-05,
      "loss": 4.8379,
      "step": 53200
    },
    {
      "epoch": 27.818371607515658,
      "grad_norm": 3.3459630012512207,
      "learning_rate": 1.5078455544327004e-05,
      "loss": 4.8481,
      "step": 53300
    },
    {
      "epoch": 27.870563674321502,
      "grad_norm": 3.4766006469726562,
      "learning_rate": 1.5056017990361955e-05,
      "loss": 4.7901,
      "step": 53400
    },
    {
      "epoch": 27.92275574112735,
      "grad_norm": 3.403146505355835,
      "learning_rate": 1.5033546188334791e-05,
      "loss": 4.935,
      "step": 53500
    },
    {
      "epoch": 27.974947807933194,
      "grad_norm": 3.3788180351257324,
      "learning_rate": 1.5011040290463266e-05,
      "loss": 4.8092,
      "step": 53600
    },
    {
      "epoch": 28.02713987473904,
      "grad_norm": 3.281763792037964,
      "learning_rate": 1.4988500449196084e-05,
      "loss": 4.6258,
      "step": 53700
    },
    {
      "epoch": 28.079331941544886,
      "grad_norm": 3.286660671234131,
      "learning_rate": 1.496592681721187e-05,
      "loss": 4.5894,
      "step": 53800
    },
    {
      "epoch": 28.13152400835073,
      "grad_norm": 3.294203758239746,
      "learning_rate": 1.4943319547418136e-05,
      "loss": 4.5522,
      "step": 53900
    },
    {
      "epoch": 28.183716075156575,
      "grad_norm": 3.230210542678833,
      "learning_rate": 1.4920678792950259e-05,
      "loss": 4.5527,
      "step": 54000
    },
    {
      "epoch": 28.235908141962422,
      "grad_norm": 3.3285655975341797,
      "learning_rate": 1.489800470717042e-05,
      "loss": 4.587,
      "step": 54100
    },
    {
      "epoch": 28.288100208768267,
      "grad_norm": 3.1679084300994873,
      "learning_rate": 1.4875297443666581e-05,
      "loss": 4.6744,
      "step": 54200
    },
    {
      "epoch": 28.340292275574114,
      "grad_norm": 3.485074996948242,
      "learning_rate": 1.4852557156251437e-05,
      "loss": 4.6477,
      "step": 54300
    },
    {
      "epoch": 28.39248434237996,
      "grad_norm": 3.3585853576660156,
      "learning_rate": 1.4829783998961385e-05,
      "loss": 4.6555,
      "step": 54400
    },
    {
      "epoch": 28.444676409185803,
      "grad_norm": 3.168107509613037,
      "learning_rate": 1.4806978126055467e-05,
      "loss": 4.7648,
      "step": 54500
    },
    {
      "epoch": 28.49686847599165,
      "grad_norm": 3.24229097366333,
      "learning_rate": 1.478413969201434e-05,
      "loss": 4.7123,
      "step": 54600
    },
    {
      "epoch": 28.549060542797495,
      "grad_norm": 3.3163487911224365,
      "learning_rate": 1.4761268851539213e-05,
      "loss": 4.7248,
      "step": 54700
    },
    {
      "epoch": 28.60125260960334,
      "grad_norm": 3.3601255416870117,
      "learning_rate": 1.4738365759550802e-05,
      "loss": 4.7459,
      "step": 54800
    },
    {
      "epoch": 28.653444676409187,
      "grad_norm": 3.1711578369140625,
      "learning_rate": 1.4715430571188305e-05,
      "loss": 4.7557,
      "step": 54900
    },
    {
      "epoch": 28.70563674321503,
      "grad_norm": 3.242227792739868,
      "learning_rate": 1.4692463441808312e-05,
      "loss": 4.7675,
      "step": 55000
    },
    {
      "epoch": 28.757828810020875,
      "grad_norm": 3.2806527614593506,
      "learning_rate": 1.4669464526983785e-05,
      "loss": 4.8535,
      "step": 55100
    },
    {
      "epoch": 28.810020876826723,
      "grad_norm": 3.300790309906006,
      "learning_rate": 1.4646433982502983e-05,
      "loss": 4.7452,
      "step": 55200
    },
    {
      "epoch": 28.862212943632567,
      "grad_norm": 3.1704630851745605,
      "learning_rate": 1.4623371964368425e-05,
      "loss": 4.8585,
      "step": 55300
    },
    {
      "epoch": 28.914405010438415,
      "grad_norm": 3.181467294692993,
      "learning_rate": 1.460027862879582e-05,
      "loss": 4.806,
      "step": 55400
    },
    {
      "epoch": 28.96659707724426,
      "grad_norm": 3.279395818710327,
      "learning_rate": 1.4577154132213003e-05,
      "loss": 4.7914,
      "step": 55500
    },
    {
      "epoch": 29.018789144050103,
      "grad_norm": 3.3721120357513428,
      "learning_rate": 1.4553998631258901e-05,
      "loss": 4.697,
      "step": 55600
    },
    {
      "epoch": 29.07098121085595,
      "grad_norm": 3.2187442779541016,
      "learning_rate": 1.4530812282782447e-05,
      "loss": 4.5809,
      "step": 55700
    },
    {
      "epoch": 29.123173277661795,
      "grad_norm": 3.647679328918457,
      "learning_rate": 1.4507595243841525e-05,
      "loss": 4.6806,
      "step": 55800
    },
    {
      "epoch": 29.17536534446764,
      "grad_norm": 3.3001418113708496,
      "learning_rate": 1.4484347671701918e-05,
      "loss": 4.629,
      "step": 55900
    },
    {
      "epoch": 29.227557411273487,
      "grad_norm": 3.2512636184692383,
      "learning_rate": 1.4461069723836218e-05,
      "loss": 4.6099,
      "step": 56000
    },
    {
      "epoch": 29.27974947807933,
      "grad_norm": 3.3235666751861572,
      "learning_rate": 1.4437761557922785e-05,
      "loss": 4.6867,
      "step": 56100
    },
    {
      "epoch": 29.331941544885176,
      "grad_norm": 3.0737521648406982,
      "learning_rate": 1.4414423331844663e-05,
      "loss": 4.6136,
      "step": 56200
    },
    {
      "epoch": 29.384133611691023,
      "grad_norm": 3.3673839569091797,
      "learning_rate": 1.439105520368851e-05,
      "loss": 4.6521,
      "step": 56300
    },
    {
      "epoch": 29.436325678496868,
      "grad_norm": 3.054788112640381,
      "learning_rate": 1.436765733174355e-05,
      "loss": 4.6747,
      "step": 56400
    },
    {
      "epoch": 29.488517745302715,
      "grad_norm": 3.302170753479004,
      "learning_rate": 1.4344229874500459e-05,
      "loss": 4.6727,
      "step": 56500
    },
    {
      "epoch": 29.54070981210856,
      "grad_norm": 3.2896792888641357,
      "learning_rate": 1.4320772990650336e-05,
      "loss": 4.6521,
      "step": 56600
    },
    {
      "epoch": 29.592901878914404,
      "grad_norm": 3.4240005016326904,
      "learning_rate": 1.4297286839083597e-05,
      "loss": 4.6629,
      "step": 56700
    },
    {
      "epoch": 29.64509394572025,
      "grad_norm": 3.1116909980773926,
      "learning_rate": 1.4273771578888912e-05,
      "loss": 4.7567,
      "step": 56800
    },
    {
      "epoch": 29.697286012526096,
      "grad_norm": 3.300784111022949,
      "learning_rate": 1.4250227369352123e-05,
      "loss": 4.7951,
      "step": 56900
    },
    {
      "epoch": 29.74947807933194,
      "grad_norm": 3.0672972202301025,
      "learning_rate": 1.4226654369955175e-05,
      "loss": 4.7704,
      "step": 57000
    },
    {
      "epoch": 29.801670146137788,
      "grad_norm": 3.2481637001037598,
      "learning_rate": 1.4203052740375013e-05,
      "loss": 4.791,
      "step": 57100
    },
    {
      "epoch": 29.853862212943632,
      "grad_norm": 3.259096145629883,
      "learning_rate": 1.4179422640482527e-05,
      "loss": 4.7367,
      "step": 57200
    },
    {
      "epoch": 29.90605427974948,
      "grad_norm": 3.2426178455352783,
      "learning_rate": 1.4155764230341453e-05,
      "loss": 4.7517,
      "step": 57300
    },
    {
      "epoch": 29.958246346555324,
      "grad_norm": 3.162259340286255,
      "learning_rate": 1.4132077670207291e-05,
      "loss": 4.7657,
      "step": 57400
    },
    {
      "epoch": 30.010438413361168,
      "grad_norm": 3.050096035003662,
      "learning_rate": 1.4108363120526228e-05,
      "loss": 4.7254,
      "step": 57500
    },
    {
      "epoch": 30.062630480167016,
      "grad_norm": 3.087245464324951,
      "learning_rate": 1.4084620741934031e-05,
      "loss": 4.5332,
      "step": 57600
    },
    {
      "epoch": 30.11482254697286,
      "grad_norm": 3.4037933349609375,
      "learning_rate": 1.4060850695254988e-05,
      "loss": 4.5966,
      "step": 57700
    },
    {
      "epoch": 30.167014613778704,
      "grad_norm": 3.2350988388061523,
      "learning_rate": 1.4037053141500794e-05,
      "loss": 4.5797,
      "step": 57800
    },
    {
      "epoch": 30.219206680584552,
      "grad_norm": 4.035472869873047,
      "learning_rate": 1.4013228241869467e-05,
      "loss": 4.5422,
      "step": 57900
    },
    {
      "epoch": 30.271398747390396,
      "grad_norm": 3.18192195892334,
      "learning_rate": 1.3989376157744268e-05,
      "loss": 4.639,
      "step": 58000
    },
    {
      "epoch": 30.32359081419624,
      "grad_norm": 3.213860273361206,
      "learning_rate": 1.396549705069259e-05,
      "loss": 4.6157,
      "step": 58100
    },
    {
      "epoch": 30.37578288100209,
      "grad_norm": 3.1657397747039795,
      "learning_rate": 1.3941591082464872e-05,
      "loss": 4.6128,
      "step": 58200
    },
    {
      "epoch": 30.427974947807932,
      "grad_norm": 3.2854936122894287,
      "learning_rate": 1.3917658414993509e-05,
      "loss": 4.6523,
      "step": 58300
    },
    {
      "epoch": 30.48016701461378,
      "grad_norm": 3.3434956073760986,
      "learning_rate": 1.3893699210391745e-05,
      "loss": 4.7417,
      "step": 58400
    },
    {
      "epoch": 30.532359081419624,
      "grad_norm": 3.1274633407592773,
      "learning_rate": 1.386971363095258e-05,
      "loss": 4.6961,
      "step": 58500
    },
    {
      "epoch": 30.58455114822547,
      "grad_norm": 3.2591280937194824,
      "learning_rate": 1.3845701839147665e-05,
      "loss": 4.6963,
      "step": 58600
    },
    {
      "epoch": 30.636743215031316,
      "grad_norm": 3.295398235321045,
      "learning_rate": 1.382166399762622e-05,
      "loss": 4.6968,
      "step": 58700
    },
    {
      "epoch": 30.68893528183716,
      "grad_norm": 3.2566256523132324,
      "learning_rate": 1.3797600269213904e-05,
      "loss": 4.7113,
      "step": 58800
    },
    {
      "epoch": 30.741127348643005,
      "grad_norm": 3.3022825717926025,
      "learning_rate": 1.3773510816911732e-05,
      "loss": 4.6917,
      "step": 58900
    },
    {
      "epoch": 30.793319415448853,
      "grad_norm": 3.300440788269043,
      "learning_rate": 1.3749395803894968e-05,
      "loss": 4.6918,
      "step": 59000
    },
    {
      "epoch": 30.845511482254697,
      "grad_norm": 3.2550876140594482,
      "learning_rate": 1.3725255393512014e-05,
      "loss": 4.7906,
      "step": 59100
    },
    {
      "epoch": 30.897703549060545,
      "grad_norm": 3.1898200511932373,
      "learning_rate": 1.3701089749283305e-05,
      "loss": 4.8129,
      "step": 59200
    },
    {
      "epoch": 30.94989561586639,
      "grad_norm": 3.2243921756744385,
      "learning_rate": 1.367689903490021e-05,
      "loss": 4.8125,
      "step": 59300
    },
    {
      "epoch": 31.002087682672233,
      "grad_norm": 3.1578896045684814,
      "learning_rate": 1.3652683414223906e-05,
      "loss": 4.7746,
      "step": 59400
    },
    {
      "epoch": 31.05427974947808,
      "grad_norm": 3.277087688446045,
      "learning_rate": 1.362844305128429e-05,
      "loss": 4.5597,
      "step": 59500
    },
    {
      "epoch": 31.106471816283925,
      "grad_norm": 3.3431832790374756,
      "learning_rate": 1.3604178110278849e-05,
      "loss": 4.5106,
      "step": 59600
    },
    {
      "epoch": 31.15866388308977,
      "grad_norm": 3.222294807434082,
      "learning_rate": 1.3579888755571555e-05,
      "loss": 4.5433,
      "step": 59700
    },
    {
      "epoch": 31.210855949895617,
      "grad_norm": 3.1872806549072266,
      "learning_rate": 1.3555575151691755e-05,
      "loss": 4.6333,
      "step": 59800
    },
    {
      "epoch": 31.26304801670146,
      "grad_norm": 3.378706455230713,
      "learning_rate": 1.353123746333305e-05,
      "loss": 4.5839,
      "step": 59900
    },
    {
      "epoch": 31.315240083507305,
      "grad_norm": 3.2470548152923584,
      "learning_rate": 1.3506875855352184e-05,
      "loss": 4.6098,
      "step": 60000
    },
    {
      "epoch": 31.367432150313153,
      "grad_norm": 3.2006661891937256,
      "learning_rate": 1.3482490492767925e-05,
      "loss": 4.7005,
      "step": 60100
    },
    {
      "epoch": 31.419624217118997,
      "grad_norm": 3.252150535583496,
      "learning_rate": 1.3458081540759949e-05,
      "loss": 4.591,
      "step": 60200
    },
    {
      "epoch": 31.471816283924845,
      "grad_norm": 3.287257432937622,
      "learning_rate": 1.343364916466772e-05,
      "loss": 4.6806,
      "step": 60300
    },
    {
      "epoch": 31.52400835073069,
      "grad_norm": 3.245002508163452,
      "learning_rate": 1.340919352998937e-05,
      "loss": 4.6508,
      "step": 60400
    },
    {
      "epoch": 31.576200417536533,
      "grad_norm": 3.3661410808563232,
      "learning_rate": 1.3384714802380581e-05,
      "loss": 4.7206,
      "step": 60500
    },
    {
      "epoch": 31.62839248434238,
      "grad_norm": 3.38451886177063,
      "learning_rate": 1.3360213147653455e-05,
      "loss": 4.6974,
      "step": 60600
    },
    {
      "epoch": 31.680584551148225,
      "grad_norm": 3.24922776222229,
      "learning_rate": 1.3335688731775398e-05,
      "loss": 4.6982,
      "step": 60700
    },
    {
      "epoch": 31.73277661795407,
      "grad_norm": 3.3620798587799072,
      "learning_rate": 1.3311141720867997e-05,
      "loss": 4.673,
      "step": 60800
    },
    {
      "epoch": 31.784968684759917,
      "grad_norm": 3.1562869548797607,
      "learning_rate": 1.3286572281205885e-05,
      "loss": 4.6827,
      "step": 60900
    },
    {
      "epoch": 31.83716075156576,
      "grad_norm": 3.30698299407959,
      "learning_rate": 1.3261980579215627e-05,
      "loss": 4.771,
      "step": 61000
    },
    {
      "epoch": 31.889352818371606,
      "grad_norm": 3.1533875465393066,
      "learning_rate": 1.3237366781474586e-05,
      "loss": 4.8166,
      "step": 61100
    },
    {
      "epoch": 31.941544885177453,
      "grad_norm": 3.51755952835083,
      "learning_rate": 1.3212731054709788e-05,
      "loss": 4.7663,
      "step": 61200
    },
    {
      "epoch": 31.993736951983298,
      "grad_norm": 3.300166130065918,
      "learning_rate": 1.3188073565796815e-05,
      "loss": 4.7291,
      "step": 61300
    },
    {
      "epoch": 32.045929018789145,
      "grad_norm": 3.276634931564331,
      "learning_rate": 1.3163394481758647e-05,
      "loss": 4.4687,
      "step": 61400
    },
    {
      "epoch": 32.09812108559499,
      "grad_norm": 3.1161975860595703,
      "learning_rate": 1.3138693969764544e-05,
      "loss": 4.4772,
      "step": 61500
    },
    {
      "epoch": 32.150313152400834,
      "grad_norm": 3.2653729915618896,
      "learning_rate": 1.311397219712892e-05,
      "loss": 4.5563,
      "step": 61600
    },
    {
      "epoch": 32.20250521920668,
      "grad_norm": 3.3562538623809814,
      "learning_rate": 1.3089229331310196e-05,
      "loss": 4.6239,
      "step": 61700
    },
    {
      "epoch": 32.25469728601253,
      "grad_norm": 3.4241111278533936,
      "learning_rate": 1.306446553990968e-05,
      "loss": 4.5811,
      "step": 61800
    },
    {
      "epoch": 32.306889352818374,
      "grad_norm": 3.2031216621398926,
      "learning_rate": 1.3039680990670418e-05,
      "loss": 4.6453,
      "step": 61900
    },
    {
      "epoch": 32.35908141962422,
      "grad_norm": 3.2036476135253906,
      "learning_rate": 1.3014875851476063e-05,
      "loss": 4.6345,
      "step": 62000
    },
    {
      "epoch": 32.41127348643006,
      "grad_norm": 3.3781917095184326,
      "learning_rate": 1.2990050290349744e-05,
      "loss": 4.595,
      "step": 62100
    },
    {
      "epoch": 32.463465553235906,
      "grad_norm": 3.3216378688812256,
      "learning_rate": 1.2965204475452917e-05,
      "loss": 4.6442,
      "step": 62200
    },
    {
      "epoch": 32.51565762004175,
      "grad_norm": 3.366269826889038,
      "learning_rate": 1.2940338575084237e-05,
      "loss": 4.6875,
      "step": 62300
    },
    {
      "epoch": 32.5678496868476,
      "grad_norm": 3.2968029975891113,
      "learning_rate": 1.2915452757678404e-05,
      "loss": 4.7188,
      "step": 62400
    },
    {
      "epoch": 32.620041753653446,
      "grad_norm": 3.091388463973999,
      "learning_rate": 1.289054719180504e-05,
      "loss": 4.6288,
      "step": 62500
    },
    {
      "epoch": 32.67223382045929,
      "grad_norm": 3.3910133838653564,
      "learning_rate": 1.2865622046167537e-05,
      "loss": 4.7427,
      "step": 62600
    },
    {
      "epoch": 32.724425887265134,
      "grad_norm": 3.3865480422973633,
      "learning_rate": 1.2840677489601902e-05,
      "loss": 4.6918,
      "step": 62700
    },
    {
      "epoch": 32.77661795407098,
      "grad_norm": 3.4918899536132812,
      "learning_rate": 1.2815713691075639e-05,
      "loss": 4.7379,
      "step": 62800
    },
    {
      "epoch": 32.82881002087683,
      "grad_norm": 3.3368425369262695,
      "learning_rate": 1.2790730819686595e-05,
      "loss": 4.7352,
      "step": 62900
    },
    {
      "epoch": 32.881002087682674,
      "grad_norm": 3.2637085914611816,
      "learning_rate": 1.2765729044661794e-05,
      "loss": 4.7405,
      "step": 63000
    },
    {
      "epoch": 32.93319415448852,
      "grad_norm": 3.226433038711548,
      "learning_rate": 1.2740708535356327e-05,
      "loss": 4.7044,
      "step": 63100
    },
    {
      "epoch": 32.98538622129436,
      "grad_norm": 3.1397268772125244,
      "learning_rate": 1.2715669461252174e-05,
      "loss": 4.6971,
      "step": 63200
    },
    {
      "epoch": 33.03757828810021,
      "grad_norm": 3.170895576477051,
      "learning_rate": 1.2690611991957068e-05,
      "loss": 4.4959,
      "step": 63300
    },
    {
      "epoch": 33.08977035490605,
      "grad_norm": 3.1459670066833496,
      "learning_rate": 1.2665536297203356e-05,
      "loss": 4.4724,
      "step": 63400
    },
    {
      "epoch": 33.1419624217119,
      "grad_norm": 3.12652325630188,
      "learning_rate": 1.2640442546846826e-05,
      "loss": 4.4981,
      "step": 63500
    },
    {
      "epoch": 33.194154488517746,
      "grad_norm": 3.2287497520446777,
      "learning_rate": 1.2615330910865577e-05,
      "loss": 4.6443,
      "step": 63600
    },
    {
      "epoch": 33.24634655532359,
      "grad_norm": 3.2553882598876953,
      "learning_rate": 1.259020155935886e-05,
      "loss": 4.596,
      "step": 63700
    },
    {
      "epoch": 33.298538622129435,
      "grad_norm": 3.1355438232421875,
      "learning_rate": 1.2565054662545927e-05,
      "loss": 4.5424,
      "step": 63800
    },
    {
      "epoch": 33.35073068893528,
      "grad_norm": 3.3883886337280273,
      "learning_rate": 1.2539890390764877e-05,
      "loss": 4.5777,
      "step": 63900
    },
    {
      "epoch": 33.40292275574113,
      "grad_norm": 3.310699462890625,
      "learning_rate": 1.2514708914471492e-05,
      "loss": 4.5917,
      "step": 64000
    },
    {
      "epoch": 33.455114822546975,
      "grad_norm": 3.260485887527466,
      "learning_rate": 1.2489510404238107e-05,
      "loss": 4.6632,
      "step": 64100
    },
    {
      "epoch": 33.50730688935282,
      "grad_norm": 3.2479395866394043,
      "learning_rate": 1.2464295030752436e-05,
      "loss": 4.6371,
      "step": 64200
    },
    {
      "epoch": 33.55949895615866,
      "grad_norm": 3.39591121673584,
      "learning_rate": 1.2439062964816413e-05,
      "loss": 4.6607,
      "step": 64300
    },
    {
      "epoch": 33.61169102296451,
      "grad_norm": 3.2715907096862793,
      "learning_rate": 1.2413814377345053e-05,
      "loss": 4.7041,
      "step": 64400
    },
    {
      "epoch": 33.66388308977035,
      "grad_norm": 3.0680136680603027,
      "learning_rate": 1.2388549439365269e-05,
      "loss": 4.6453,
      "step": 64500
    },
    {
      "epoch": 33.7160751565762,
      "grad_norm": 3.221161127090454,
      "learning_rate": 1.2363268322014743e-05,
      "loss": 4.6985,
      "step": 64600
    },
    {
      "epoch": 33.76826722338205,
      "grad_norm": 3.148623466491699,
      "learning_rate": 1.2337971196540748e-05,
      "loss": 4.7225,
      "step": 64700
    },
    {
      "epoch": 33.82045929018789,
      "grad_norm": 3.2404849529266357,
      "learning_rate": 1.2312658234298978e-05,
      "loss": 4.7368,
      "step": 64800
    },
    {
      "epoch": 33.872651356993735,
      "grad_norm": 3.1868934631347656,
      "learning_rate": 1.2287329606752419e-05,
      "loss": 4.7478,
      "step": 64900
    },
    {
      "epoch": 33.92484342379958,
      "grad_norm": 3.3667187690734863,
      "learning_rate": 1.226198548547016e-05,
      "loss": 4.759,
      "step": 65000
    },
    {
      "epoch": 33.97703549060543,
      "grad_norm": 3.1725594997406006,
      "learning_rate": 1.2236626042126244e-05,
      "loss": 4.6797,
      "step": 65100
    },
    {
      "epoch": 34.029227557411275,
      "grad_norm": 3.3216142654418945,
      "learning_rate": 1.2211251448498496e-05,
      "loss": 4.5928,
      "step": 65200
    },
    {
      "epoch": 34.08141962421712,
      "grad_norm": 3.260887622833252,
      "learning_rate": 1.2185861876467367e-05,
      "loss": 4.5332,
      "step": 65300
    },
    {
      "epoch": 34.13361169102296,
      "grad_norm": 3.26438307762146,
      "learning_rate": 1.2160457498014772e-05,
      "loss": 4.5305,
      "step": 65400
    },
    {
      "epoch": 34.18580375782881,
      "grad_norm": 3.152700901031494,
      "learning_rate": 1.213503848522292e-05,
      "loss": 4.5099,
      "step": 65500
    },
    {
      "epoch": 34.23799582463466,
      "grad_norm": 3.129380702972412,
      "learning_rate": 1.2109605010273138e-05,
      "loss": 4.5657,
      "step": 65600
    },
    {
      "epoch": 34.2901878914405,
      "grad_norm": 3.230746030807495,
      "learning_rate": 1.2084157245444731e-05,
      "loss": 4.5763,
      "step": 65700
    },
    {
      "epoch": 34.34237995824635,
      "grad_norm": 4.747929096221924,
      "learning_rate": 1.2058695363113788e-05,
      "loss": 4.6227,
      "step": 65800
    },
    {
      "epoch": 34.39457202505219,
      "grad_norm": 3.193235397338867,
      "learning_rate": 1.2033219535752036e-05,
      "loss": 4.6112,
      "step": 65900
    },
    {
      "epoch": 34.446764091858036,
      "grad_norm": 3.5097146034240723,
      "learning_rate": 1.2007729935925656e-05,
      "loss": 4.6079,
      "step": 66000
    },
    {
      "epoch": 34.49895615866388,
      "grad_norm": 3.331873893737793,
      "learning_rate": 1.1982226736294114e-05,
      "loss": 4.6019,
      "step": 66100
    },
    {
      "epoch": 34.55114822546973,
      "grad_norm": 3.358361005783081,
      "learning_rate": 1.1956710109609007e-05,
      "loss": 4.5801,
      "step": 66200
    },
    {
      "epoch": 34.603340292275576,
      "grad_norm": 3.3239822387695312,
      "learning_rate": 1.1931180228712882e-05,
      "loss": 4.6296,
      "step": 66300
    },
    {
      "epoch": 34.65553235908142,
      "grad_norm": 3.313788414001465,
      "learning_rate": 1.1905637266538061e-05,
      "loss": 4.6942,
      "step": 66400
    },
    {
      "epoch": 34.707724425887264,
      "grad_norm": 3.2589914798736572,
      "learning_rate": 1.188008139610548e-05,
      "loss": 4.6727,
      "step": 66500
    },
    {
      "epoch": 34.75991649269311,
      "grad_norm": 3.1271538734436035,
      "learning_rate": 1.1854512790523508e-05,
      "loss": 4.6857,
      "step": 66600
    },
    {
      "epoch": 34.81210855949896,
      "grad_norm": 3.335641384124756,
      "learning_rate": 1.182893162298678e-05,
      "loss": 4.6916,
      "step": 66700
    },
    {
      "epoch": 34.864300626304804,
      "grad_norm": 3.374749183654785,
      "learning_rate": 1.1803338066775027e-05,
      "loss": 4.6366,
      "step": 66800
    },
    {
      "epoch": 34.91649269311065,
      "grad_norm": 3.1592061519622803,
      "learning_rate": 1.1777732295251887e-05,
      "loss": 4.733,
      "step": 66900
    },
    {
      "epoch": 34.96868475991649,
      "grad_norm": 3.1046595573425293,
      "learning_rate": 1.1752114481863753e-05,
      "loss": 4.6948,
      "step": 67000
    },
    {
      "epoch": 35.020876826722336,
      "grad_norm": 3.399660110473633,
      "learning_rate": 1.1726484800138574e-05,
      "loss": 4.5933,
      "step": 67100
    },
    {
      "epoch": 35.07306889352818,
      "grad_norm": 3.1465399265289307,
      "learning_rate": 1.1700843423684702e-05,
      "loss": 4.519,
      "step": 67200
    },
    {
      "epoch": 35.12526096033403,
      "grad_norm": 3.0754127502441406,
      "learning_rate": 1.1675190526189705e-05,
      "loss": 4.4953,
      "step": 67300
    },
    {
      "epoch": 35.177453027139876,
      "grad_norm": 3.3446431159973145,
      "learning_rate": 1.164952628141918e-05,
      "loss": 4.5479,
      "step": 67400
    },
    {
      "epoch": 35.22964509394572,
      "grad_norm": 3.2645325660705566,
      "learning_rate": 1.1623850863215602e-05,
      "loss": 4.6154,
      "step": 67500
    },
    {
      "epoch": 35.281837160751564,
      "grad_norm": 3.356982707977295,
      "learning_rate": 1.1598164445497124e-05,
      "loss": 4.5496,
      "step": 67600
    },
    {
      "epoch": 35.33402922755741,
      "grad_norm": 3.38261079788208,
      "learning_rate": 1.157246720225641e-05,
      "loss": 4.581,
      "step": 67700
    },
    {
      "epoch": 35.38622129436326,
      "grad_norm": 3.3356404304504395,
      "learning_rate": 1.154675930755945e-05,
      "loss": 4.5979,
      "step": 67800
    },
    {
      "epoch": 35.438413361169104,
      "grad_norm": 3.2252750396728516,
      "learning_rate": 1.1521040935544376e-05,
      "loss": 4.6371,
      "step": 67900
    },
    {
      "epoch": 35.49060542797495,
      "grad_norm": 3.35910964012146,
      "learning_rate": 1.1495312260420315e-05,
      "loss": 4.5504,
      "step": 68000
    },
    {
      "epoch": 35.54279749478079,
      "grad_norm": 3.2039496898651123,
      "learning_rate": 1.1469573456466161e-05,
      "loss": 4.6192,
      "step": 68100
    },
    {
      "epoch": 35.59498956158664,
      "grad_norm": 3.4853830337524414,
      "learning_rate": 1.1443824698029424e-05,
      "loss": 4.6671,
      "step": 68200
    },
    {
      "epoch": 35.64718162839248,
      "grad_norm": 3.4002256393432617,
      "learning_rate": 1.1418066159525048e-05,
      "loss": 4.6346,
      "step": 68300
    },
    {
      "epoch": 35.69937369519833,
      "grad_norm": 3.289015293121338,
      "learning_rate": 1.139229801543422e-05,
      "loss": 4.6473,
      "step": 68400
    },
    {
      "epoch": 35.75156576200418,
      "grad_norm": 3.27998685836792,
      "learning_rate": 1.1366520440303195e-05,
      "loss": 4.671,
      "step": 68500
    },
    {
      "epoch": 35.80375782881002,
      "grad_norm": 3.418801784515381,
      "learning_rate": 1.134073360874211e-05,
      "loss": 4.6759,
      "step": 68600
    },
    {
      "epoch": 35.855949895615865,
      "grad_norm": 3.3079190254211426,
      "learning_rate": 1.13149376954238e-05,
      "loss": 4.6456,
      "step": 68700
    },
    {
      "epoch": 35.90814196242171,
      "grad_norm": 3.3218986988067627,
      "learning_rate": 1.128913287508262e-05,
      "loss": 4.7623,
      "step": 68800
    },
    {
      "epoch": 35.96033402922756,
      "grad_norm": 3.2277584075927734,
      "learning_rate": 1.1263319322513263e-05,
      "loss": 4.6218,
      "step": 68900
    },
    {
      "epoch": 36.012526096033405,
      "grad_norm": 3.414780378341675,
      "learning_rate": 1.1237497212569563e-05,
      "loss": 4.6199,
      "step": 69000
    },
    {
      "epoch": 36.06471816283925,
      "grad_norm": 3.253614664077759,
      "learning_rate": 1.1211666720163326e-05,
      "loss": 4.4491,
      "step": 69100
    },
    {
      "epoch": 36.11691022964509,
      "grad_norm": 3.1524603366851807,
      "learning_rate": 1.1185828020263133e-05,
      "loss": 4.5081,
      "step": 69200
    },
    {
      "epoch": 36.16910229645094,
      "grad_norm": 3.1030824184417725,
      "learning_rate": 1.1159981287893166e-05,
      "loss": 4.5546,
      "step": 69300
    },
    {
      "epoch": 36.22129436325679,
      "grad_norm": 3.027806282043457,
      "learning_rate": 1.1134126698132013e-05,
      "loss": 4.5444,
      "step": 69400
    },
    {
      "epoch": 36.27348643006263,
      "grad_norm": 3.444779634475708,
      "learning_rate": 1.1108264426111486e-05,
      "loss": 4.5438,
      "step": 69500
    },
    {
      "epoch": 36.32567849686848,
      "grad_norm": 3.032133102416992,
      "learning_rate": 1.1082394647015435e-05,
      "loss": 4.556,
      "step": 69600
    },
    {
      "epoch": 36.37787056367432,
      "grad_norm": 3.294257879257202,
      "learning_rate": 1.1056517536078563e-05,
      "loss": 4.6158,
      "step": 69700
    },
    {
      "epoch": 36.430062630480165,
      "grad_norm": 3.6194393634796143,
      "learning_rate": 1.1030633268585236e-05,
      "loss": 4.5742,
      "step": 69800
    },
    {
      "epoch": 36.48225469728601,
      "grad_norm": 3.321892261505127,
      "learning_rate": 1.1004742019868293e-05,
      "loss": 4.5882,
      "step": 69900
    },
    {
      "epoch": 36.53444676409186,
      "grad_norm": 3.3977959156036377,
      "learning_rate": 1.097884396530786e-05,
      "loss": 4.588,
      "step": 70000
    },
    {
      "epoch": 36.586638830897705,
      "grad_norm": 3.48230242729187,
      "learning_rate": 1.0952939280330177e-05,
      "loss": 4.6352,
      "step": 70100
    },
    {
      "epoch": 36.63883089770355,
      "grad_norm": 3.156663179397583,
      "learning_rate": 1.0927028140406384e-05,
      "loss": 4.6615,
      "step": 70200
    },
    {
      "epoch": 36.69102296450939,
      "grad_norm": 3.23624849319458,
      "learning_rate": 1.0901110721051349e-05,
      "loss": 4.6506,
      "step": 70300
    },
    {
      "epoch": 36.74321503131524,
      "grad_norm": 3.12091064453125,
      "learning_rate": 1.0875187197822473e-05,
      "loss": 4.6342,
      "step": 70400
    },
    {
      "epoch": 36.79540709812109,
      "grad_norm": 3.275210380554199,
      "learning_rate": 1.0849257746318504e-05,
      "loss": 4.6456,
      "step": 70500
    },
    {
      "epoch": 36.84759916492693,
      "grad_norm": 3.3453874588012695,
      "learning_rate": 1.0823322542178356e-05,
      "loss": 4.6531,
      "step": 70600
    },
    {
      "epoch": 36.89979123173278,
      "grad_norm": 3.348649501800537,
      "learning_rate": 1.079738176107989e-05,
      "loss": 4.6171,
      "step": 70700
    },
    {
      "epoch": 36.95198329853862,
      "grad_norm": 3.299222230911255,
      "learning_rate": 1.0771435578738762e-05,
      "loss": 4.6735,
      "step": 70800
    },
    {
      "epoch": 37.004175365344466,
      "grad_norm": 3.210026502609253,
      "learning_rate": 1.0745484170907209e-05,
      "loss": 4.6932,
      "step": 70900
    },
    {
      "epoch": 37.05636743215031,
      "grad_norm": 3.438610076904297,
      "learning_rate": 1.0719527713372854e-05,
      "loss": 4.5005,
      "step": 71000
    },
    {
      "epoch": 37.10855949895616,
      "grad_norm": 3.265155076980591,
      "learning_rate": 1.0693566381957543e-05,
      "loss": 4.5629,
      "step": 71100
    },
    {
      "epoch": 37.160751565762006,
      "grad_norm": 3.101038694381714,
      "learning_rate": 1.0667600352516121e-05,
      "loss": 4.4272,
      "step": 71200
    },
    {
      "epoch": 37.21294363256785,
      "grad_norm": 3.0926833152770996,
      "learning_rate": 1.0641629800935262e-05,
      "loss": 4.5317,
      "step": 71300
    },
    {
      "epoch": 37.265135699373694,
      "grad_norm": 3.1746840476989746,
      "learning_rate": 1.0615654903132278e-05,
      "loss": 4.537,
      "step": 71400
    },
    {
      "epoch": 37.31732776617954,
      "grad_norm": 3.2280943393707275,
      "learning_rate": 1.0589675835053907e-05,
      "loss": 4.5549,
      "step": 71500
    },
    {
      "epoch": 37.36951983298539,
      "grad_norm": 3.3687491416931152,
      "learning_rate": 1.0563692772675154e-05,
      "loss": 4.5522,
      "step": 71600
    },
    {
      "epoch": 37.421711899791234,
      "grad_norm": 3.323941946029663,
      "learning_rate": 1.0537705891998055e-05,
      "loss": 4.6466,
      "step": 71700
    },
    {
      "epoch": 37.47390396659708,
      "grad_norm": 3.3726184368133545,
      "learning_rate": 1.0511715369050536e-05,
      "loss": 4.6121,
      "step": 71800
    },
    {
      "epoch": 37.52609603340292,
      "grad_norm": 3.314934492111206,
      "learning_rate": 1.0485721379885182e-05,
      "loss": 4.5807,
      "step": 71900
    },
    {
      "epoch": 37.578288100208766,
      "grad_norm": 3.3578920364379883,
      "learning_rate": 1.0459724100578054e-05,
      "loss": 4.5983,
      "step": 72000
    },
    {
      "epoch": 37.63048016701461,
      "grad_norm": 2.972816228866577,
      "learning_rate": 1.0433723707227507e-05,
      "loss": 4.6148,
      "step": 72100
    },
    {
      "epoch": 37.68267223382046,
      "grad_norm": 3.251574754714966,
      "learning_rate": 1.040772037595299e-05,
      "loss": 4.6451,
      "step": 72200
    },
    {
      "epoch": 37.734864300626306,
      "grad_norm": 3.3038039207458496,
      "learning_rate": 1.0381714282893842e-05,
      "loss": 4.5958,
      "step": 72300
    },
    {
      "epoch": 37.78705636743215,
      "grad_norm": 3.1653010845184326,
      "learning_rate": 1.0355705604208126e-05,
      "loss": 4.6526,
      "step": 72400
    },
    {
      "epoch": 37.839248434237994,
      "grad_norm": 4.951602458953857,
      "learning_rate": 1.03296945160714e-05,
      "loss": 4.6657,
      "step": 72500
    },
    {
      "epoch": 37.89144050104384,
      "grad_norm": 3.1527163982391357,
      "learning_rate": 1.0303681194675562e-05,
      "loss": 4.6339,
      "step": 72600
    },
    {
      "epoch": 37.94363256784969,
      "grad_norm": 3.3456993103027344,
      "learning_rate": 1.0277665816227624e-05,
      "loss": 4.6215,
      "step": 72700
    },
    {
      "epoch": 37.995824634655534,
      "grad_norm": 3.1652791500091553,
      "learning_rate": 1.0251648556948539e-05,
      "loss": 4.6298,
      "step": 72800
    },
    {
      "epoch": 38.04801670146138,
      "grad_norm": 3.0021884441375732,
      "learning_rate": 1.0225629593071997e-05,
      "loss": 4.4694,
      "step": 72900
    },
    {
      "epoch": 38.10020876826722,
      "grad_norm": 3.385500192642212,
      "learning_rate": 1.0199609100843236e-05,
      "loss": 4.4661,
      "step": 73000
    },
    {
      "epoch": 38.15240083507307,
      "grad_norm": 3.042306423187256,
      "learning_rate": 1.0173587256517844e-05,
      "loss": 4.4749,
      "step": 73100
    },
    {
      "epoch": 38.20459290187891,
      "grad_norm": 3.0617618560791016,
      "learning_rate": 1.0147564236360574e-05,
      "loss": 4.568,
      "step": 73200
    },
    {
      "epoch": 38.25678496868476,
      "grad_norm": 3.171006202697754,
      "learning_rate": 1.0121540216644135e-05,
      "loss": 4.5484,
      "step": 73300
    },
    {
      "epoch": 38.30897703549061,
      "grad_norm": 3.1113617420196533,
      "learning_rate": 1.0095515373648013e-05,
      "loss": 4.4946,
      "step": 73400
    },
    {
      "epoch": 38.36116910229645,
      "grad_norm": 3.3707447052001953,
      "learning_rate": 1.0069489883657268e-05,
      "loss": 4.6137,
      "step": 73500
    },
    {
      "epoch": 38.413361169102295,
      "grad_norm": 3.122220277786255,
      "learning_rate": 1.0043463922961345e-05,
      "loss": 4.525,
      "step": 73600
    },
    {
      "epoch": 38.46555323590814,
      "grad_norm": 3.4053547382354736,
      "learning_rate": 1.0017437667852876e-05,
      "loss": 4.511,
      "step": 73700
    },
    {
      "epoch": 38.51774530271399,
      "grad_norm": 3.126354455947876,
      "learning_rate": 9.991411294626484e-06,
      "loss": 4.5941,
      "step": 73800
    },
    {
      "epoch": 38.569937369519835,
      "grad_norm": 3.181536912918091,
      "learning_rate": 9.965384979577599e-06,
      "loss": 4.5706,
      "step": 73900
    },
    {
      "epoch": 38.62212943632568,
      "grad_norm": 3.218569040298462,
      "learning_rate": 9.939358899001256e-06,
      "loss": 4.623,
      "step": 74000
    },
    {
      "epoch": 38.67432150313152,
      "grad_norm": 3.4060873985290527,
      "learning_rate": 9.913333229190889e-06,
      "loss": 4.6137,
      "step": 74100
    },
    {
      "epoch": 38.72651356993737,
      "grad_norm": 3.5443062782287598,
      "learning_rate": 9.887308146437166e-06,
      "loss": 4.6452,
      "step": 74200
    },
    {
      "epoch": 38.77870563674321,
      "grad_norm": 3.195802927017212,
      "learning_rate": 9.861283827026775e-06,
      "loss": 4.5602,
      "step": 74300
    },
    {
      "epoch": 38.83089770354906,
      "grad_norm": 3.2999472618103027,
      "learning_rate": 9.835260447241222e-06,
      "loss": 4.5435,
      "step": 74400
    },
    {
      "epoch": 38.88308977035491,
      "grad_norm": 3.086296796798706,
      "learning_rate": 9.809238183355663e-06,
      "loss": 4.6525,
      "step": 74500
    },
    {
      "epoch": 38.93528183716075,
      "grad_norm": 3.2319579124450684,
      "learning_rate": 9.783217211637688e-06,
      "loss": 4.6776,
      "step": 74600
    },
    {
      "epoch": 38.987473903966595,
      "grad_norm": 3.2018883228302,
      "learning_rate": 9.75719770834613e-06,
      "loss": 4.7231,
      "step": 74700
    },
    {
      "epoch": 39.03966597077244,
      "grad_norm": 3.1608352661132812,
      "learning_rate": 9.731179849729887e-06,
      "loss": 4.5268,
      "step": 74800
    },
    {
      "epoch": 39.09185803757829,
      "grad_norm": 3.2538628578186035,
      "learning_rate": 9.705163812026707e-06,
      "loss": 4.4688,
      "step": 74900
    },
    {
      "epoch": 39.144050104384135,
      "grad_norm": 3.1077239513397217,
      "learning_rate": 9.679149771462002e-06,
      "loss": 4.4786,
      "step": 75000
    },
    {
      "epoch": 39.19624217118998,
      "grad_norm": 3.2314326763153076,
      "learning_rate": 9.653137904247665e-06,
      "loss": 4.5483,
      "step": 75100
    },
    {
      "epoch": 39.24843423799582,
      "grad_norm": 3.2182648181915283,
      "learning_rate": 9.627128386580861e-06,
      "loss": 4.5258,
      "step": 75200
    },
    {
      "epoch": 39.30062630480167,
      "grad_norm": 3.1577157974243164,
      "learning_rate": 9.601121394642848e-06,
      "loss": 4.484,
      "step": 75300
    },
    {
      "epoch": 39.35281837160752,
      "grad_norm": 3.2100231647491455,
      "learning_rate": 9.575117104597758e-06,
      "loss": 4.5576,
      "step": 75400
    },
    {
      "epoch": 39.40501043841336,
      "grad_norm": 3.4381067752838135,
      "learning_rate": 9.549115692591437e-06,
      "loss": 4.5467,
      "step": 75500
    },
    {
      "epoch": 39.45720250521921,
      "grad_norm": 3.3639650344848633,
      "learning_rate": 9.523117334750231e-06,
      "loss": 4.4953,
      "step": 75600
    },
    {
      "epoch": 39.50939457202505,
      "grad_norm": 3.2414073944091797,
      "learning_rate": 9.497122207179796e-06,
      "loss": 4.5453,
      "step": 75700
    },
    {
      "epoch": 39.561586638830896,
      "grad_norm": 3.1904661655426025,
      "learning_rate": 9.47113048596391e-06,
      "loss": 4.5474,
      "step": 75800
    },
    {
      "epoch": 39.61377870563674,
      "grad_norm": 3.300807237625122,
      "learning_rate": 9.445142347163279e-06,
      "loss": 4.6385,
      "step": 75900
    },
    {
      "epoch": 39.66597077244259,
      "grad_norm": 3.203571319580078,
      "learning_rate": 9.419157966814335e-06,
      "loss": 4.587,
      "step": 76000
    },
    {
      "epoch": 39.718162839248436,
      "grad_norm": 3.1631722450256348,
      "learning_rate": 9.393177520928058e-06,
      "loss": 4.5996,
      "step": 76100
    },
    {
      "epoch": 39.77035490605428,
      "grad_norm": 3.201151132583618,
      "learning_rate": 9.367201185488779e-06,
      "loss": 4.6702,
      "step": 76200
    },
    {
      "epoch": 39.822546972860124,
      "grad_norm": 3.1614573001861572,
      "learning_rate": 9.341229136452975e-06,
      "loss": 4.6081,
      "step": 76300
    },
    {
      "epoch": 39.87473903966597,
      "grad_norm": 3.1741483211517334,
      "learning_rate": 9.3152615497481e-06,
      "loss": 4.604,
      "step": 76400
    },
    {
      "epoch": 39.92693110647182,
      "grad_norm": 3.318838119506836,
      "learning_rate": 9.289298601271375e-06,
      "loss": 4.6736,
      "step": 76500
    },
    {
      "epoch": 39.979123173277664,
      "grad_norm": 3.4235291481018066,
      "learning_rate": 9.26334046688861e-06,
      "loss": 4.6224,
      "step": 76600
    },
    {
      "epoch": 40.03131524008351,
      "grad_norm": 3.3283779621124268,
      "learning_rate": 9.237387322432992e-06,
      "loss": 4.4637,
      "step": 76700
    },
    {
      "epoch": 40.08350730688935,
      "grad_norm": 3.228910207748413,
      "learning_rate": 9.211439343703915e-06,
      "loss": 4.4628,
      "step": 76800
    },
    {
      "epoch": 40.135699373695196,
      "grad_norm": 3.28928542137146,
      "learning_rate": 9.185496706465795e-06,
      "loss": 4.4625,
      "step": 76900
    },
    {
      "epoch": 40.18789144050104,
      "grad_norm": 3.4189014434814453,
      "learning_rate": 9.159559586446843e-06,
      "loss": 4.5222,
      "step": 77000
    },
    {
      "epoch": 40.24008350730689,
      "grad_norm": 3.2996366024017334,
      "learning_rate": 9.133628159337909e-06,
      "loss": 4.5399,
      "step": 77100
    },
    {
      "epoch": 40.292275574112736,
      "grad_norm": 3.124762535095215,
      "learning_rate": 9.107702600791289e-06,
      "loss": 4.4994,
      "step": 77200
    },
    {
      "epoch": 40.34446764091858,
      "grad_norm": 3.2379612922668457,
      "learning_rate": 9.081783086419509e-06,
      "loss": 4.5507,
      "step": 77300
    },
    {
      "epoch": 40.396659707724424,
      "grad_norm": 3.1568856239318848,
      "learning_rate": 9.055869791794167e-06,
      "loss": 4.4892,
      "step": 77400
    },
    {
      "epoch": 40.44885177453027,
      "grad_norm": 3.3654468059539795,
      "learning_rate": 9.029962892444727e-06,
      "loss": 4.5671,
      "step": 77500
    },
    {
      "epoch": 40.50104384133612,
      "grad_norm": 3.2595624923706055,
      "learning_rate": 9.004062563857333e-06,
      "loss": 4.4982,
      "step": 77600
    },
    {
      "epoch": 40.553235908141964,
      "grad_norm": 3.3751890659332275,
      "learning_rate": 8.978168981473618e-06,
      "loss": 4.5251,
      "step": 77700
    },
    {
      "epoch": 40.60542797494781,
      "grad_norm": 3.1471259593963623,
      "learning_rate": 8.952282320689518e-06,
      "loss": 4.5149,
      "step": 77800
    },
    {
      "epoch": 40.65762004175365,
      "grad_norm": 3.2929656505584717,
      "learning_rate": 8.926402756854096e-06,
      "loss": 4.5752,
      "step": 77900
    },
    {
      "epoch": 40.7098121085595,
      "grad_norm": 3.2114133834838867,
      "learning_rate": 8.900530465268316e-06,
      "loss": 4.6406,
      "step": 78000
    },
    {
      "epoch": 40.76200417536534,
      "grad_norm": 3.323957681655884,
      "learning_rate": 8.874665621183908e-06,
      "loss": 4.5873,
      "step": 78100
    },
    {
      "epoch": 40.81419624217119,
      "grad_norm": 3.055863857269287,
      "learning_rate": 8.84880839980215e-06,
      "loss": 4.6304,
      "step": 78200
    },
    {
      "epoch": 40.86638830897704,
      "grad_norm": 3.1699745655059814,
      "learning_rate": 8.82295897627267e-06,
      "loss": 4.6398,
      "step": 78300
    },
    {
      "epoch": 40.91858037578288,
      "grad_norm": 3.289513111114502,
      "learning_rate": 8.797117525692288e-06,
      "loss": 4.5941,
      "step": 78400
    },
    {
      "epoch": 40.970772442588725,
      "grad_norm": 3.2149226665496826,
      "learning_rate": 8.771284223103821e-06,
      "loss": 4.643,
      "step": 78500
    },
    {
      "epoch": 41.02296450939457,
      "grad_norm": 3.07816743850708,
      "learning_rate": 8.745459243494883e-06,
      "loss": 4.5449,
      "step": 78600
    },
    {
      "epoch": 41.07515657620042,
      "grad_norm": 6.044609069824219,
      "learning_rate": 8.719642761796718e-06,
      "loss": 4.4349,
      "step": 78700
    },
    {
      "epoch": 41.127348643006265,
      "grad_norm": 3.1620383262634277,
      "learning_rate": 8.693834952883005e-06,
      "loss": 4.4664,
      "step": 78800
    },
    {
      "epoch": 41.17954070981211,
      "grad_norm": 3.2579970359802246,
      "learning_rate": 8.668035991568679e-06,
      "loss": 4.5298,
      "step": 78900
    },
    {
      "epoch": 41.23173277661795,
      "grad_norm": 3.3446266651153564,
      "learning_rate": 8.642246052608736e-06,
      "loss": 4.5194,
      "step": 79000
    },
    {
      "epoch": 41.2839248434238,
      "grad_norm": 3.344750165939331,
      "learning_rate": 8.616465310697064e-06,
      "loss": 4.513,
      "step": 79100
    },
    {
      "epoch": 41.33611691022965,
      "grad_norm": 3.188372850418091,
      "learning_rate": 8.590693940465255e-06,
      "loss": 4.4794,
      "step": 79200
    },
    {
      "epoch": 41.38830897703549,
      "grad_norm": 3.3712823390960693,
      "learning_rate": 8.56493211648141e-06,
      "loss": 4.5867,
      "step": 79300
    },
    {
      "epoch": 41.44050104384134,
      "grad_norm": 3.283029079437256,
      "learning_rate": 8.539180013248974e-06,
      "loss": 4.5985,
      "step": 79400
    },
    {
      "epoch": 41.49269311064718,
      "grad_norm": 3.3011505603790283,
      "learning_rate": 8.51343780520555e-06,
      "loss": 4.5753,
      "step": 79500
    },
    {
      "epoch": 41.544885177453025,
      "grad_norm": 3.0619595050811768,
      "learning_rate": 8.4877056667217e-06,
      "loss": 4.5649,
      "step": 79600
    },
    {
      "epoch": 41.59707724425887,
      "grad_norm": 3.2573084831237793,
      "learning_rate": 8.461983772099786e-06,
      "loss": 4.5473,
      "step": 79700
    },
    {
      "epoch": 41.64926931106472,
      "grad_norm": 3.236307382583618,
      "learning_rate": 8.436272295572785e-06,
      "loss": 4.5626,
      "step": 79800
    },
    {
      "epoch": 41.701461377870565,
      "grad_norm": 3.220874071121216,
      "learning_rate": 8.410571411303103e-06,
      "loss": 4.5589,
      "step": 79900
    },
    {
      "epoch": 41.75365344467641,
      "grad_norm": 2.978512763977051,
      "learning_rate": 8.384881293381389e-06,
      "loss": 4.5395,
      "step": 80000
    },
    {
      "epoch": 41.805845511482254,
      "grad_norm": 3.376762866973877,
      "learning_rate": 8.359202115825369e-06,
      "loss": 4.617,
      "step": 80100
    },
    {
      "epoch": 41.8580375782881,
      "grad_norm": 3.264730215072632,
      "learning_rate": 8.333534052578666e-06,
      "loss": 4.5926,
      "step": 80200
    },
    {
      "epoch": 41.91022964509395,
      "grad_norm": 3.2152810096740723,
      "learning_rate": 8.30787727750961e-06,
      "loss": 4.5266,
      "step": 80300
    },
    {
      "epoch": 41.96242171189979,
      "grad_norm": 3.1900436878204346,
      "learning_rate": 8.282231964410075e-06,
      "loss": 4.5876,
      "step": 80400
    },
    {
      "epoch": 42.01461377870564,
      "grad_norm": 3.1503686904907227,
      "learning_rate": 8.256598286994291e-06,
      "loss": 4.5565,
      "step": 80500
    },
    {
      "epoch": 42.06680584551148,
      "grad_norm": 3.2888596057891846,
      "learning_rate": 8.23097641889767e-06,
      "loss": 4.4254,
      "step": 80600
    },
    {
      "epoch": 42.118997912317326,
      "grad_norm": 3.2910525798797607,
      "learning_rate": 8.205366533675636e-06,
      "loss": 4.3977,
      "step": 80700
    },
    {
      "epoch": 42.17118997912317,
      "grad_norm": 3.2667415142059326,
      "learning_rate": 8.17976880480244e-06,
      "loss": 4.4367,
      "step": 80800
    },
    {
      "epoch": 42.22338204592902,
      "grad_norm": 3.325573205947876,
      "learning_rate": 8.154183405669988e-06,
      "loss": 4.4532,
      "step": 80900
    },
    {
      "epoch": 42.275574112734866,
      "grad_norm": 3.245488166809082,
      "learning_rate": 8.12861050958667e-06,
      "loss": 4.499,
      "step": 81000
    },
    {
      "epoch": 42.32776617954071,
      "grad_norm": 3.2921559810638428,
      "learning_rate": 8.103050289776189e-06,
      "loss": 4.4862,
      "step": 81100
    },
    {
      "epoch": 42.379958246346554,
      "grad_norm": 3.480269432067871,
      "learning_rate": 8.077502919376377e-06,
      "loss": 4.5211,
      "step": 81200
    },
    {
      "epoch": 42.4321503131524,
      "grad_norm": 3.311441421508789,
      "learning_rate": 8.051968571438023e-06,
      "loss": 4.5339,
      "step": 81300
    },
    {
      "epoch": 42.48434237995825,
      "grad_norm": 3.2848060131073,
      "learning_rate": 8.026447418923712e-06,
      "loss": 4.5352,
      "step": 81400
    },
    {
      "epoch": 42.536534446764094,
      "grad_norm": 3.246840238571167,
      "learning_rate": 8.000939634706648e-06,
      "loss": 4.5878,
      "step": 81500
    },
    {
      "epoch": 42.58872651356994,
      "grad_norm": 3.344057559967041,
      "learning_rate": 7.975445391569477e-06,
      "loss": 4.5717,
      "step": 81600
    },
    {
      "epoch": 42.64091858037578,
      "grad_norm": 3.4058969020843506,
      "learning_rate": 7.949964862203124e-06,
      "loss": 4.6207,
      "step": 81700
    },
    {
      "epoch": 42.693110647181626,
      "grad_norm": 3.09869122505188,
      "learning_rate": 7.924498219205623e-06,
      "loss": 4.5568,
      "step": 81800
    },
    {
      "epoch": 42.74530271398747,
      "grad_norm": 3.260315179824829,
      "learning_rate": 7.89904563508094e-06,
      "loss": 4.5289,
      "step": 81900
    },
    {
      "epoch": 42.79749478079332,
      "grad_norm": 3.2506637573242188,
      "learning_rate": 7.873607282237813e-06,
      "loss": 4.5857,
      "step": 82000
    },
    {
      "epoch": 42.849686847599166,
      "grad_norm": 3.1892104148864746,
      "learning_rate": 7.848183332988588e-06,
      "loss": 4.6057,
      "step": 82100
    },
    {
      "epoch": 42.90187891440501,
      "grad_norm": 3.277332305908203,
      "learning_rate": 7.822773959548027e-06,
      "loss": 4.5611,
      "step": 82200
    },
    {
      "epoch": 42.954070981210855,
      "grad_norm": 3.1632885932922363,
      "learning_rate": 7.79737933403218e-06,
      "loss": 4.5786,
      "step": 82300
    },
    {
      "epoch": 43.0062630480167,
      "grad_norm": 3.2754411697387695,
      "learning_rate": 7.771999628457185e-06,
      "loss": 4.5655,
      "step": 82400
    },
    {
      "epoch": 43.05845511482255,
      "grad_norm": 3.2879526615142822,
      "learning_rate": 7.746635014738128e-06,
      "loss": 4.4062,
      "step": 82500
    },
    {
      "epoch": 43.110647181628394,
      "grad_norm": 3.5441277027130127,
      "learning_rate": 7.72128566468785e-06,
      "loss": 4.4557,
      "step": 82600
    },
    {
      "epoch": 43.16283924843424,
      "grad_norm": 3.2025232315063477,
      "learning_rate": 7.695951750015816e-06,
      "loss": 4.4475,
      "step": 82700
    },
    {
      "epoch": 43.21503131524008,
      "grad_norm": 3.2243423461914062,
      "learning_rate": 7.670633442326933e-06,
      "loss": 4.4784,
      "step": 82800
    },
    {
      "epoch": 43.26722338204593,
      "grad_norm": 3.101433277130127,
      "learning_rate": 7.645330913120384e-06,
      "loss": 4.502,
      "step": 82900
    },
    {
      "epoch": 43.31941544885177,
      "grad_norm": 3.281728744506836,
      "learning_rate": 7.620044333788475e-06,
      "loss": 4.5669,
      "step": 83000
    },
    {
      "epoch": 43.37160751565762,
      "grad_norm": 3.338454008102417,
      "learning_rate": 7.594773875615478e-06,
      "loss": 4.5615,
      "step": 83100
    },
    {
      "epoch": 43.42379958246347,
      "grad_norm": 3.2074191570281982,
      "learning_rate": 7.569519709776456e-06,
      "loss": 4.5349,
      "step": 83200
    },
    {
      "epoch": 43.47599164926931,
      "grad_norm": 3.267571449279785,
      "learning_rate": 7.544282007336121e-06,
      "loss": 4.491,
      "step": 83300
    },
    {
      "epoch": 43.528183716075155,
      "grad_norm": 3.141700029373169,
      "learning_rate": 7.51906093924766e-06,
      "loss": 4.468,
      "step": 83400
    },
    {
      "epoch": 43.580375782881,
      "grad_norm": 4.346724987030029,
      "learning_rate": 7.493856676351589e-06,
      "loss": 4.4705,
      "step": 83500
    },
    {
      "epoch": 43.63256784968685,
      "grad_norm": 3.441690444946289,
      "learning_rate": 7.468669389374583e-06,
      "loss": 4.5263,
      "step": 83600
    },
    {
      "epoch": 43.684759916492695,
      "grad_norm": 3.197270154953003,
      "learning_rate": 7.443499248928334e-06,
      "loss": 4.5719,
      "step": 83700
    },
    {
      "epoch": 43.73695198329854,
      "grad_norm": 3.2472951412200928,
      "learning_rate": 7.418346425508387e-06,
      "loss": 4.5559,
      "step": 83800
    },
    {
      "epoch": 43.78914405010438,
      "grad_norm": 3.182518243789673,
      "learning_rate": 7.3932110894929745e-06,
      "loss": 4.5424,
      "step": 83900
    },
    {
      "epoch": 43.84133611691023,
      "grad_norm": 3.142786979675293,
      "learning_rate": 7.368093411141892e-06,
      "loss": 4.5862,
      "step": 84000
    },
    {
      "epoch": 43.89352818371607,
      "grad_norm": 3.120370626449585,
      "learning_rate": 7.342993560595324e-06,
      "loss": 4.5567,
      "step": 84100
    },
    {
      "epoch": 43.94572025052192,
      "grad_norm": 3.1979284286499023,
      "learning_rate": 7.317911707872678e-06,
      "loss": 4.5454,
      "step": 84200
    },
    {
      "epoch": 43.99791231732777,
      "grad_norm": 3.333099603652954,
      "learning_rate": 7.292848022871466e-06,
      "loss": 4.5545,
      "step": 84300
    },
    {
      "epoch": 44.05010438413361,
      "grad_norm": 3.308379888534546,
      "learning_rate": 7.267802675366133e-06,
      "loss": 4.413,
      "step": 84400
    },
    {
      "epoch": 44.102296450939455,
      "grad_norm": 3.2592694759368896,
      "learning_rate": 7.242775835006908e-06,
      "loss": 4.4347,
      "step": 84500
    },
    {
      "epoch": 44.1544885177453,
      "grad_norm": 3.1483969688415527,
      "learning_rate": 7.217767671318656e-06,
      "loss": 4.4665,
      "step": 84600
    },
    {
      "epoch": 44.20668058455115,
      "grad_norm": 3.373645782470703,
      "learning_rate": 7.192778353699739e-06,
      "loss": 4.4264,
      "step": 84700
    },
    {
      "epoch": 44.258872651356995,
      "grad_norm": 3.1882903575897217,
      "learning_rate": 7.167808051420855e-06,
      "loss": 4.4362,
      "step": 84800
    },
    {
      "epoch": 44.31106471816284,
      "grad_norm": 3.159410238265991,
      "learning_rate": 7.142856933623898e-06,
      "loss": 4.5003,
      "step": 84900
    },
    {
      "epoch": 44.363256784968684,
      "grad_norm": 3.3212971687316895,
      "learning_rate": 7.117925169320813e-06,
      "loss": 4.4984,
      "step": 85000
    },
    {
      "epoch": 44.41544885177453,
      "grad_norm": 3.2427313327789307,
      "learning_rate": 7.093012927392453e-06,
      "loss": 4.5063,
      "step": 85100
    },
    {
      "epoch": 44.46764091858038,
      "grad_norm": 3.5494823455810547,
      "learning_rate": 7.068120376587419e-06,
      "loss": 4.4854,
      "step": 85200
    },
    {
      "epoch": 44.51983298538622,
      "grad_norm": 3.26603627204895,
      "learning_rate": 7.043247685520947e-06,
      "loss": 4.4432,
      "step": 85300
    },
    {
      "epoch": 44.57202505219207,
      "grad_norm": 3.3010356426239014,
      "learning_rate": 7.018395022673743e-06,
      "loss": 4.5502,
      "step": 85400
    },
    {
      "epoch": 44.62421711899791,
      "grad_norm": 3.2351317405700684,
      "learning_rate": 6.993562556390836e-06,
      "loss": 4.5473,
      "step": 85500
    },
    {
      "epoch": 44.676409185803756,
      "grad_norm": 3.1742279529571533,
      "learning_rate": 6.968750454880462e-06,
      "loss": 4.5089,
      "step": 85600
    },
    {
      "epoch": 44.7286012526096,
      "grad_norm": 3.0430800914764404,
      "learning_rate": 6.9439588862129094e-06,
      "loss": 4.595,
      "step": 85700
    },
    {
      "epoch": 44.78079331941545,
      "grad_norm": 3.4326186180114746,
      "learning_rate": 6.919188018319378e-06,
      "loss": 4.5852,
      "step": 85800
    },
    {
      "epoch": 44.832985386221296,
      "grad_norm": 3.2425031661987305,
      "learning_rate": 6.8944380189908524e-06,
      "loss": 4.5269,
      "step": 85900
    },
    {
      "epoch": 44.88517745302714,
      "grad_norm": 3.366008996963501,
      "learning_rate": 6.869709055876954e-06,
      "loss": 4.5883,
      "step": 86000
    },
    {
      "epoch": 44.937369519832984,
      "grad_norm": 3.1614410877227783,
      "learning_rate": 6.845001296484819e-06,
      "loss": 4.6259,
      "step": 86100
    },
    {
      "epoch": 44.98956158663883,
      "grad_norm": 3.206714630126953,
      "learning_rate": 6.820314908177942e-06,
      "loss": 4.5487,
      "step": 86200
    },
    {
      "epoch": 45.04175365344468,
      "grad_norm": 3.185245990753174,
      "learning_rate": 6.795650058175067e-06,
      "loss": 4.5096,
      "step": 86300
    },
    {
      "epoch": 45.093945720250524,
      "grad_norm": 3.4324569702148438,
      "learning_rate": 6.771006913549041e-06,
      "loss": 4.4351,
      "step": 86400
    },
    {
      "epoch": 45.14613778705637,
      "grad_norm": 3.536001443862915,
      "learning_rate": 6.746385641225681e-06,
      "loss": 4.4551,
      "step": 86500
    },
    {
      "epoch": 45.19832985386221,
      "grad_norm": 3.1084020137786865,
      "learning_rate": 6.7217864079826515e-06,
      "loss": 4.4222,
      "step": 86600
    },
    {
      "epoch": 45.25052192066806,
      "grad_norm": 3.2789697647094727,
      "learning_rate": 6.697209380448333e-06,
      "loss": 4.4351,
      "step": 86700
    },
    {
      "epoch": 45.3027139874739,
      "grad_norm": 3.5214719772338867,
      "learning_rate": 6.672654725100677e-06,
      "loss": 4.4786,
      "step": 86800
    },
    {
      "epoch": 45.35490605427975,
      "grad_norm": 3.4433741569519043,
      "learning_rate": 6.6481226082661045e-06,
      "loss": 4.4538,
      "step": 86900
    },
    {
      "epoch": 45.407098121085596,
      "grad_norm": 3.3464503288269043,
      "learning_rate": 6.623613196118368e-06,
      "loss": 4.487,
      "step": 87000
    },
    {
      "epoch": 45.45929018789144,
      "grad_norm": 3.333282947540283,
      "learning_rate": 6.599126654677423e-06,
      "loss": 4.5289,
      "step": 87100
    },
    {
      "epoch": 45.511482254697285,
      "grad_norm": 3.168361186981201,
      "learning_rate": 6.574663149808294e-06,
      "loss": 4.5082,
      "step": 87200
    },
    {
      "epoch": 45.56367432150313,
      "grad_norm": 3.309333324432373,
      "learning_rate": 6.5502228472199736e-06,
      "loss": 4.4773,
      "step": 87300
    },
    {
      "epoch": 45.61586638830898,
      "grad_norm": 3.125722885131836,
      "learning_rate": 6.525805912464291e-06,
      "loss": 4.5076,
      "step": 87400
    },
    {
      "epoch": 45.668058455114824,
      "grad_norm": 3.3097434043884277,
      "learning_rate": 6.501412510934776e-06,
      "loss": 4.5315,
      "step": 87500
    },
    {
      "epoch": 45.72025052192067,
      "grad_norm": 3.3462204933166504,
      "learning_rate": 6.477042807865561e-06,
      "loss": 4.5344,
      "step": 87600
    },
    {
      "epoch": 45.77244258872651,
      "grad_norm": 3.2815332412719727,
      "learning_rate": 6.452696968330253e-06,
      "loss": 4.5037,
      "step": 87700
    },
    {
      "epoch": 45.82463465553236,
      "grad_norm": 3.294661283493042,
      "learning_rate": 6.428375157240803e-06,
      "loss": 4.5891,
      "step": 87800
    },
    {
      "epoch": 45.8768267223382,
      "grad_norm": 3.49039363861084,
      "learning_rate": 6.404077539346409e-06,
      "loss": 4.612,
      "step": 87900
    },
    {
      "epoch": 45.92901878914405,
      "grad_norm": 3.219860315322876,
      "learning_rate": 6.379804279232391e-06,
      "loss": 4.5306,
      "step": 88000
    },
    {
      "epoch": 45.9812108559499,
      "grad_norm": 3.321727991104126,
      "learning_rate": 6.355555541319067e-06,
      "loss": 4.5472,
      "step": 88100
    },
    {
      "epoch": 46.03340292275574,
      "grad_norm": 3.2369699478149414,
      "learning_rate": 6.331331489860662e-06,
      "loss": 4.4733,
      "step": 88200
    },
    {
      "epoch": 46.085594989561585,
      "grad_norm": 3.374147415161133,
      "learning_rate": 6.307132288944174e-06,
      "loss": 4.45,
      "step": 88300
    },
    {
      "epoch": 46.13778705636743,
      "grad_norm": 3.196178674697876,
      "learning_rate": 6.282958102488271e-06,
      "loss": 4.4969,
      "step": 88400
    },
    {
      "epoch": 46.18997912317328,
      "grad_norm": 3.2412734031677246,
      "learning_rate": 6.258809094242177e-06,
      "loss": 4.4347,
      "step": 88500
    },
    {
      "epoch": 46.242171189979125,
      "grad_norm": 3.179765224456787,
      "learning_rate": 6.234685427784571e-06,
      "loss": 4.4679,
      "step": 88600
    },
    {
      "epoch": 46.29436325678497,
      "grad_norm": 3.2833282947540283,
      "learning_rate": 6.210587266522474e-06,
      "loss": 4.3997,
      "step": 88700
    },
    {
      "epoch": 46.34655532359081,
      "grad_norm": 2.971860408782959,
      "learning_rate": 6.186514773690138e-06,
      "loss": 4.4836,
      "step": 88800
    },
    {
      "epoch": 46.39874739039666,
      "grad_norm": 3.14001727104187,
      "learning_rate": 6.162468112347944e-06,
      "loss": 4.4647,
      "step": 88900
    },
    {
      "epoch": 46.45093945720251,
      "grad_norm": 3.2717671394348145,
      "learning_rate": 6.138447445381303e-06,
      "loss": 4.4694,
      "step": 89000
    },
    {
      "epoch": 46.50313152400835,
      "grad_norm": 3.1794919967651367,
      "learning_rate": 6.114452935499542e-06,
      "loss": 4.4697,
      "step": 89100
    },
    {
      "epoch": 46.5553235908142,
      "grad_norm": 3.1037180423736572,
      "learning_rate": 6.090484745234811e-06,
      "loss": 4.4924,
      "step": 89200
    },
    {
      "epoch": 46.60751565762004,
      "grad_norm": 3.168876886367798,
      "learning_rate": 6.066543036940975e-06,
      "loss": 4.486,
      "step": 89300
    },
    {
      "epoch": 46.659707724425886,
      "grad_norm": 3.0188348293304443,
      "learning_rate": 6.042627972792516e-06,
      "loss": 4.4932,
      "step": 89400
    },
    {
      "epoch": 46.71189979123173,
      "grad_norm": 3.1282005310058594,
      "learning_rate": 6.018739714783442e-06,
      "loss": 4.4976,
      "step": 89500
    },
    {
      "epoch": 46.76409185803758,
      "grad_norm": 3.118589401245117,
      "learning_rate": 5.9948784247261805e-06,
      "loss": 4.5491,
      "step": 89600
    },
    {
      "epoch": 46.816283924843425,
      "grad_norm": 3.3138620853424072,
      "learning_rate": 5.971044264250489e-06,
      "loss": 4.5985,
      "step": 89700
    },
    {
      "epoch": 46.86847599164927,
      "grad_norm": 3.4257967472076416,
      "learning_rate": 5.9472373948023455e-06,
      "loss": 4.6313,
      "step": 89800
    },
    {
      "epoch": 46.920668058455114,
      "grad_norm": 3.221813440322876,
      "learning_rate": 5.923457977642875e-06,
      "loss": 4.4652,
      "step": 89900
    },
    {
      "epoch": 46.97286012526096,
      "grad_norm": 3.316239356994629,
      "learning_rate": 5.899706173847255e-06,
      "loss": 4.5203,
      "step": 90000
    },
    {
      "epoch": 47.02505219206681,
      "grad_norm": 3.2465481758117676,
      "learning_rate": 5.8759821443036e-06,
      "loss": 4.5039,
      "step": 90100
    },
    {
      "epoch": 47.07724425887265,
      "grad_norm": 3.4854326248168945,
      "learning_rate": 5.852286049711906e-06,
      "loss": 4.4285,
      "step": 90200
    },
    {
      "epoch": 47.1294363256785,
      "grad_norm": 3.1236460208892822,
      "learning_rate": 5.828618050582937e-06,
      "loss": 4.4228,
      "step": 90300
    },
    {
      "epoch": 47.18162839248434,
      "grad_norm": 3.1262240409851074,
      "learning_rate": 5.804978307237148e-06,
      "loss": 4.4319,
      "step": 90400
    },
    {
      "epoch": 47.233820459290186,
      "grad_norm": 3.4076621532440186,
      "learning_rate": 5.781366979803593e-06,
      "loss": 4.4811,
      "step": 90500
    },
    {
      "epoch": 47.28601252609603,
      "grad_norm": 3.0574755668640137,
      "learning_rate": 5.757784228218852e-06,
      "loss": 4.4774,
      "step": 90600
    },
    {
      "epoch": 47.33820459290188,
      "grad_norm": 3.42948842048645,
      "learning_rate": 5.734230212225945e-06,
      "loss": 4.463,
      "step": 90700
    },
    {
      "epoch": 47.390396659707726,
      "grad_norm": 3.212238073348999,
      "learning_rate": 5.710705091373221e-06,
      "loss": 4.4142,
      "step": 90800
    },
    {
      "epoch": 47.44258872651357,
      "grad_norm": 3.2007744312286377,
      "learning_rate": 5.687209025013329e-06,
      "loss": 4.5069,
      "step": 90900
    },
    {
      "epoch": 47.494780793319414,
      "grad_norm": 3.265632152557373,
      "learning_rate": 5.663742172302095e-06,
      "loss": 4.471,
      "step": 91000
    },
    {
      "epoch": 47.54697286012526,
      "grad_norm": 3.3656880855560303,
      "learning_rate": 5.640304692197459e-06,
      "loss": 4.4906,
      "step": 91100
    },
    {
      "epoch": 47.59916492693111,
      "grad_norm": 3.175313711166382,
      "learning_rate": 5.6168967434584135e-06,
      "loss": 4.4439,
      "step": 91200
    },
    {
      "epoch": 47.651356993736954,
      "grad_norm": 3.1519227027893066,
      "learning_rate": 5.5935184846439e-06,
      "loss": 4.4778,
      "step": 91300
    },
    {
      "epoch": 47.7035490605428,
      "grad_norm": 3.3495171070098877,
      "learning_rate": 5.570170074111747e-06,
      "loss": 4.491,
      "step": 91400
    },
    {
      "epoch": 47.75574112734864,
      "grad_norm": 3.325754165649414,
      "learning_rate": 5.546851670017615e-06,
      "loss": 4.5369,
      "step": 91500
    },
    {
      "epoch": 47.80793319415449,
      "grad_norm": 3.23228120803833,
      "learning_rate": 5.5235634303138965e-06,
      "loss": 4.5244,
      "step": 91600
    },
    {
      "epoch": 47.86012526096033,
      "grad_norm": 3.2409474849700928,
      "learning_rate": 5.500305512748654e-06,
      "loss": 4.5143,
      "step": 91700
    },
    {
      "epoch": 47.91231732776618,
      "grad_norm": 3.3127620220184326,
      "learning_rate": 5.4770780748645745e-06,
      "loss": 4.5486,
      "step": 91800
    },
    {
      "epoch": 47.964509394572026,
      "grad_norm": 3.160797119140625,
      "learning_rate": 5.453881273997863e-06,
      "loss": 4.5742,
      "step": 91900
    },
    {
      "epoch": 48.01670146137787,
      "grad_norm": 2.9640064239501953,
      "learning_rate": 5.430715267277222e-06,
      "loss": 4.5045,
      "step": 92000
    },
    {
      "epoch": 48.068893528183715,
      "grad_norm": 3.2151315212249756,
      "learning_rate": 5.4075802116227336e-06,
      "loss": 4.3734,
      "step": 92100
    },
    {
      "epoch": 48.12108559498956,
      "grad_norm": 3.0197174549102783,
      "learning_rate": 5.3844762637448476e-06,
      "loss": 4.4419,
      "step": 92200
    },
    {
      "epoch": 48.17327766179541,
      "grad_norm": 3.281346082687378,
      "learning_rate": 5.361403580143303e-06,
      "loss": 4.4755,
      "step": 92300
    },
    {
      "epoch": 48.225469728601254,
      "grad_norm": 3.2571749687194824,
      "learning_rate": 5.3383623171060324e-06,
      "loss": 4.4572,
      "step": 92400
    },
    {
      "epoch": 48.2776617954071,
      "grad_norm": 3.0804710388183594,
      "learning_rate": 5.315352630708171e-06,
      "loss": 4.4154,
      "step": 92500
    },
    {
      "epoch": 48.32985386221294,
      "grad_norm": 3.3725178241729736,
      "learning_rate": 5.2923746768109396e-06,
      "loss": 4.4391,
      "step": 92600
    },
    {
      "epoch": 48.38204592901879,
      "grad_norm": 3.260333776473999,
      "learning_rate": 5.269428611060614e-06,
      "loss": 4.4929,
      "step": 92700
    },
    {
      "epoch": 48.43423799582463,
      "grad_norm": 3.3036320209503174,
      "learning_rate": 5.246514588887483e-06,
      "loss": 4.4809,
      "step": 92800
    },
    {
      "epoch": 48.48643006263048,
      "grad_norm": 3.1326589584350586,
      "learning_rate": 5.223632765504764e-06,
      "loss": 4.4528,
      "step": 92900
    },
    {
      "epoch": 48.53862212943633,
      "grad_norm": 3.2402865886688232,
      "learning_rate": 5.200783295907574e-06,
      "loss": 4.4882,
      "step": 93000
    },
    {
      "epoch": 48.59081419624217,
      "grad_norm": 3.239276885986328,
      "learning_rate": 5.177966334871883e-06,
      "loss": 4.4503,
      "step": 93100
    },
    {
      "epoch": 48.643006263048015,
      "grad_norm": 3.2655744552612305,
      "learning_rate": 5.155182036953443e-06,
      "loss": 4.4976,
      "step": 93200
    },
    {
      "epoch": 48.69519832985386,
      "grad_norm": 2.931408643722534,
      "learning_rate": 5.1324305564867725e-06,
      "loss": 4.5389,
      "step": 93300
    },
    {
      "epoch": 48.74739039665971,
      "grad_norm": 3.3621666431427,
      "learning_rate": 5.10971204758407e-06,
      "loss": 4.5297,
      "step": 93400
    },
    {
      "epoch": 48.799582463465555,
      "grad_norm": 3.231532573699951,
      "learning_rate": 5.087026664134212e-06,
      "loss": 4.4528,
      "step": 93500
    },
    {
      "epoch": 48.8517745302714,
      "grad_norm": 3.329585552215576,
      "learning_rate": 5.064374559801699e-06,
      "loss": 4.5585,
      "step": 93600
    },
    {
      "epoch": 48.90396659707724,
      "grad_norm": 3.0015597343444824,
      "learning_rate": 5.041755888025578e-06,
      "loss": 4.4593,
      "step": 93700
    },
    {
      "epoch": 48.95615866388309,
      "grad_norm": 3.4269697666168213,
      "learning_rate": 5.019170802018465e-06,
      "loss": 4.5121,
      "step": 93800
    },
    {
      "epoch": 49.00835073068894,
      "grad_norm": 3.525066614151001,
      "learning_rate": 4.996619454765458e-06,
      "loss": 4.4547,
      "step": 93900
    },
    {
      "epoch": 49.06054279749478,
      "grad_norm": 3.108778476715088,
      "learning_rate": 4.974101999023119e-06,
      "loss": 4.4417,
      "step": 94000
    },
    {
      "epoch": 49.11273486430063,
      "grad_norm": 3.7242910861968994,
      "learning_rate": 4.951618587318446e-06,
      "loss": 4.4248,
      "step": 94100
    },
    {
      "epoch": 49.16492693110647,
      "grad_norm": 3.6156907081604004,
      "learning_rate": 4.929169371947824e-06,
      "loss": 4.3912,
      "step": 94200
    },
    {
      "epoch": 49.217118997912316,
      "grad_norm": 3.250765562057495,
      "learning_rate": 4.90675450497601e-06,
      "loss": 4.442,
      "step": 94300
    },
    {
      "epoch": 49.26931106471816,
      "grad_norm": 3.2459418773651123,
      "learning_rate": 4.884374138235087e-06,
      "loss": 4.4811,
      "step": 94400
    },
    {
      "epoch": 49.32150313152401,
      "grad_norm": 3.20430588722229,
      "learning_rate": 4.862028423323442e-06,
      "loss": 4.4417,
      "step": 94500
    },
    {
      "epoch": 49.373695198329855,
      "grad_norm": 3.181990623474121,
      "learning_rate": 4.839717511604754e-06,
      "loss": 4.4243,
      "step": 94600
    },
    {
      "epoch": 49.4258872651357,
      "grad_norm": 3.3986563682556152,
      "learning_rate": 4.81744155420693e-06,
      "loss": 4.4732,
      "step": 94700
    },
    {
      "epoch": 49.478079331941544,
      "grad_norm": 3.244624376296997,
      "learning_rate": 4.795200702021128e-06,
      "loss": 4.471,
      "step": 94800
    },
    {
      "epoch": 49.53027139874739,
      "grad_norm": 3.664332389831543,
      "learning_rate": 4.772995105700714e-06,
      "loss": 4.4309,
      "step": 94900
    },
    {
      "epoch": 49.58246346555324,
      "grad_norm": 3.2450077533721924,
      "learning_rate": 4.750824915660216e-06,
      "loss": 4.4904,
      "step": 95000
    },
    {
      "epoch": 49.63465553235908,
      "grad_norm": 3.289350748062134,
      "learning_rate": 4.728690282074359e-06,
      "loss": 4.4153,
      "step": 95100
    },
    {
      "epoch": 49.68684759916493,
      "grad_norm": 3.2479708194732666,
      "learning_rate": 4.706591354877001e-06,
      "loss": 4.5165,
      "step": 95200
    },
    {
      "epoch": 49.73903966597077,
      "grad_norm": 3.337939739227295,
      "learning_rate": 4.684528283760133e-06,
      "loss": 4.4222,
      "step": 95300
    },
    {
      "epoch": 49.791231732776616,
      "grad_norm": 3.170816421508789,
      "learning_rate": 4.662501218172884e-06,
      "loss": 4.4788,
      "step": 95400
    },
    {
      "epoch": 49.84342379958246,
      "grad_norm": 3.304575204849243,
      "learning_rate": 4.6405103073204725e-06,
      "loss": 4.4815,
      "step": 95500
    },
    {
      "epoch": 49.89561586638831,
      "grad_norm": 3.3629064559936523,
      "learning_rate": 4.618555700163231e-06,
      "loss": 4.4892,
      "step": 95600
    },
    {
      "epoch": 49.947807933194156,
      "grad_norm": 3.237358570098877,
      "learning_rate": 4.5966375454155665e-06,
      "loss": 4.5504,
      "step": 95700
    },
    {
      "epoch": 50.0,
      "grad_norm": 3.0573933124542236,
      "learning_rate": 4.5747559915449725e-06,
      "loss": 4.5401,
      "step": 95800
    },
    {
      "epoch": 50.052192066805844,
      "grad_norm": 3.2484190464019775,
      "learning_rate": 4.552911186771026e-06,
      "loss": 4.3882,
      "step": 95900
    },
    {
      "epoch": 50.10438413361169,
      "grad_norm": 3.326246500015259,
      "learning_rate": 4.531103279064367e-06,
      "loss": 4.4138,
      "step": 96000
    },
    {
      "epoch": 50.15657620041754,
      "grad_norm": 3.4344844818115234,
      "learning_rate": 4.509332416145703e-06,
      "loss": 4.4223,
      "step": 96100
    },
    {
      "epoch": 50.208768267223384,
      "grad_norm": 3.2669310569763184,
      "learning_rate": 4.487598745484827e-06,
      "loss": 4.4559,
      "step": 96200
    },
    {
      "epoch": 50.26096033402923,
      "grad_norm": 3.2971649169921875,
      "learning_rate": 4.465902414299573e-06,
      "loss": 4.4359,
      "step": 96300
    },
    {
      "epoch": 50.31315240083507,
      "grad_norm": 3.1925179958343506,
      "learning_rate": 4.444243569554875e-06,
      "loss": 4.4468,
      "step": 96400
    },
    {
      "epoch": 50.36534446764092,
      "grad_norm": 2.9835047721862793,
      "learning_rate": 4.422622357961738e-06,
      "loss": 4.4066,
      "step": 96500
    },
    {
      "epoch": 50.41753653444676,
      "grad_norm": 3.304252862930298,
      "learning_rate": 4.401038925976235e-06,
      "loss": 4.3879,
      "step": 96600
    },
    {
      "epoch": 50.46972860125261,
      "grad_norm": 2.919935941696167,
      "learning_rate": 4.37949341979855e-06,
      "loss": 4.4106,
      "step": 96700
    },
    {
      "epoch": 50.521920668058456,
      "grad_norm": 3.1382553577423096,
      "learning_rate": 4.357985985371953e-06,
      "loss": 4.4826,
      "step": 96800
    },
    {
      "epoch": 50.5741127348643,
      "grad_norm": 3.213469982147217,
      "learning_rate": 4.336516768381842e-06,
      "loss": 4.4928,
      "step": 96900
    },
    {
      "epoch": 50.626304801670145,
      "grad_norm": 3.210879325866699,
      "learning_rate": 4.315085914254728e-06,
      "loss": 4.4863,
      "step": 97000
    },
    {
      "epoch": 50.67849686847599,
      "grad_norm": 3.266916513442993,
      "learning_rate": 4.293693568157265e-06,
      "loss": 4.4392,
      "step": 97100
    },
    {
      "epoch": 50.73068893528184,
      "grad_norm": 3.073728561401367,
      "learning_rate": 4.272339874995271e-06,
      "loss": 4.4987,
      "step": 97200
    },
    {
      "epoch": 50.782881002087684,
      "grad_norm": 3.141202211380005,
      "learning_rate": 4.25102497941273e-06,
      "loss": 4.4987,
      "step": 97300
    },
    {
      "epoch": 50.83507306889353,
      "grad_norm": 3.349378824234009,
      "learning_rate": 4.229749025790827e-06,
      "loss": 4.4411,
      "step": 97400
    },
    {
      "epoch": 50.88726513569937,
      "grad_norm": 3.3860037326812744,
      "learning_rate": 4.208512158246973e-06,
      "loss": 4.4754,
      "step": 97500
    },
    {
      "epoch": 50.93945720250522,
      "grad_norm": 3.135746955871582,
      "learning_rate": 4.187314520633797e-06,
      "loss": 4.5069,
      "step": 97600
    },
    {
      "epoch": 50.99164926931106,
      "grad_norm": 3.164538860321045,
      "learning_rate": 4.166156256538222e-06,
      "loss": 4.545,
      "step": 97700
    },
    {
      "epoch": 51.04384133611691,
      "grad_norm": 3.161367893218994,
      "learning_rate": 4.145037509280454e-06,
      "loss": 4.3905,
      "step": 97800
    },
    {
      "epoch": 51.09603340292276,
      "grad_norm": 3.1691722869873047,
      "learning_rate": 4.123958421913023e-06,
      "loss": 4.4549,
      "step": 97900
    },
    {
      "epoch": 51.1482254697286,
      "grad_norm": 3.1284656524658203,
      "learning_rate": 4.102919137219813e-06,
      "loss": 4.3758,
      "step": 98000
    },
    {
      "epoch": 51.200417536534445,
      "grad_norm": 3.1004347801208496,
      "learning_rate": 4.0819197977150936e-06,
      "loss": 4.4165,
      "step": 98100
    },
    {
      "epoch": 51.25260960334029,
      "grad_norm": 3.371586799621582,
      "learning_rate": 4.060960545642569e-06,
      "loss": 4.4077,
      "step": 98200
    },
    {
      "epoch": 51.30480167014614,
      "grad_norm": 3.2848827838897705,
      "learning_rate": 4.040041522974386e-06,
      "loss": 4.4257,
      "step": 98300
    },
    {
      "epoch": 51.356993736951985,
      "grad_norm": 3.149406671524048,
      "learning_rate": 4.019162871410197e-06,
      "loss": 4.4727,
      "step": 98400
    },
    {
      "epoch": 51.40918580375783,
      "grad_norm": 3.134441375732422,
      "learning_rate": 3.998324732376197e-06,
      "loss": 4.4708,
      "step": 98500
    },
    {
      "epoch": 51.46137787056367,
      "grad_norm": 3.34604811668396,
      "learning_rate": 3.977527247024149e-06,
      "loss": 4.4834,
      "step": 98600
    },
    {
      "epoch": 51.51356993736952,
      "grad_norm": 3.0460991859436035,
      "learning_rate": 3.9567705562304415e-06,
      "loss": 4.3959,
      "step": 98700
    },
    {
      "epoch": 51.56576200417537,
      "grad_norm": 3.2312395572662354,
      "learning_rate": 3.936054800595147e-06,
      "loss": 4.4485,
      "step": 98800
    },
    {
      "epoch": 51.61795407098121,
      "grad_norm": 3.456932783126831,
      "learning_rate": 3.915380120441028e-06,
      "loss": 4.4973,
      "step": 98900
    },
    {
      "epoch": 51.67014613778706,
      "grad_norm": 3.2896335124969482,
      "learning_rate": 3.894746655812635e-06,
      "loss": 4.4044,
      "step": 99000
    },
    {
      "epoch": 51.7223382045929,
      "grad_norm": 3.3057544231414795,
      "learning_rate": 3.8741545464753305e-06,
      "loss": 4.436,
      "step": 99100
    },
    {
      "epoch": 51.774530271398746,
      "grad_norm": 3.387346029281616,
      "learning_rate": 3.853603931914344e-06,
      "loss": 4.4355,
      "step": 99200
    },
    {
      "epoch": 51.82672233820459,
      "grad_norm": 3.250291109085083,
      "learning_rate": 3.833094951333832e-06,
      "loss": 4.5151,
      "step": 99300
    },
    {
      "epoch": 51.87891440501044,
      "grad_norm": 3.3933169841766357,
      "learning_rate": 3.8126277436559335e-06,
      "loss": 4.4983,
      "step": 99400
    },
    {
      "epoch": 51.931106471816285,
      "grad_norm": 3.2932355403900146,
      "learning_rate": 3.792202447519835e-06,
      "loss": 4.4612,
      "step": 99500
    },
    {
      "epoch": 51.98329853862213,
      "grad_norm": 3.2218029499053955,
      "learning_rate": 3.7718192012808195e-06,
      "loss": 4.4628,
      "step": 99600
    },
    {
      "epoch": 52.035490605427974,
      "grad_norm": 3.2475991249084473,
      "learning_rate": 3.751478143009334e-06,
      "loss": 4.4079,
      "step": 99700
    },
    {
      "epoch": 52.08768267223382,
      "grad_norm": 3.1428027153015137,
      "learning_rate": 3.7311794104900666e-06,
      "loss": 4.4329,
      "step": 99800
    },
    {
      "epoch": 52.13987473903967,
      "grad_norm": 3.224588394165039,
      "learning_rate": 3.7109231412209933e-06,
      "loss": 4.4199,
      "step": 99900
    },
    {
      "epoch": 52.19206680584551,
      "grad_norm": 3.1342926025390625,
      "learning_rate": 3.6907094724124527e-06,
      "loss": 4.4729,
      "step": 100000
    },
    {
      "epoch": 52.24425887265136,
      "grad_norm": 3.2512524127960205,
      "learning_rate": 3.670538540986234e-06,
      "loss": 4.3852,
      "step": 100100
    },
    {
      "epoch": 52.2964509394572,
      "grad_norm": 3.0808465480804443,
      "learning_rate": 3.65041048357462e-06,
      "loss": 4.3741,
      "step": 100200
    },
    {
      "epoch": 52.348643006263046,
      "grad_norm": 3.2574093341827393,
      "learning_rate": 3.630325436519484e-06,
      "loss": 4.4355,
      "step": 100300
    },
    {
      "epoch": 52.40083507306889,
      "grad_norm": 3.190122604370117,
      "learning_rate": 3.6102835358713628e-06,
      "loss": 4.4365,
      "step": 100400
    },
    {
      "epoch": 52.45302713987474,
      "grad_norm": 3.4824938774108887,
      "learning_rate": 3.5902849173885226e-06,
      "loss": 4.4581,
      "step": 100500
    },
    {
      "epoch": 52.505219206680586,
      "grad_norm": 3.0919711589813232,
      "learning_rate": 3.5703297165360505e-06,
      "loss": 4.4076,
      "step": 100600
    },
    {
      "epoch": 52.55741127348643,
      "grad_norm": 3.2161219120025635,
      "learning_rate": 3.5504180684849444e-06,
      "loss": 4.3956,
      "step": 100700
    },
    {
      "epoch": 52.609603340292274,
      "grad_norm": 3.6023752689361572,
      "learning_rate": 3.5305501081111748e-06,
      "loss": 4.5229,
      "step": 100800
    },
    {
      "epoch": 52.66179540709812,
      "grad_norm": 3.150956392288208,
      "learning_rate": 3.5107259699947904e-06,
      "loss": 4.4718,
      "step": 100900
    },
    {
      "epoch": 52.71398747390397,
      "grad_norm": 3.3395445346832275,
      "learning_rate": 3.4909457884189933e-06,
      "loss": 4.4537,
      "step": 101000
    },
    {
      "epoch": 52.766179540709814,
      "grad_norm": 3.1148154735565186,
      "learning_rate": 3.471209697369251e-06,
      "loss": 4.446,
      "step": 101100
    },
    {
      "epoch": 52.81837160751566,
      "grad_norm": 3.1008286476135254,
      "learning_rate": 3.4515178305323606e-06,
      "loss": 4.4771,
      "step": 101200
    },
    {
      "epoch": 52.8705636743215,
      "grad_norm": 3.101663112640381,
      "learning_rate": 3.431870321295556e-06,
      "loss": 4.4955,
      "step": 101300
    },
    {
      "epoch": 52.92275574112735,
      "grad_norm": 3.1444129943847656,
      "learning_rate": 3.4122673027456186e-06,
      "loss": 4.4195,
      "step": 101400
    },
    {
      "epoch": 52.97494780793319,
      "grad_norm": 3.2628588676452637,
      "learning_rate": 3.3927089076679485e-06,
      "loss": 4.4005,
      "step": 101500
    },
    {
      "epoch": 53.02713987473904,
      "grad_norm": 3.359774112701416,
      "learning_rate": 3.3731952685456827e-06,
      "loss": 4.4521,
      "step": 101600
    },
    {
      "epoch": 53.079331941544886,
      "grad_norm": 3.318410873413086,
      "learning_rate": 3.3537265175587983e-06,
      "loss": 4.3681,
      "step": 101700
    },
    {
      "epoch": 53.13152400835073,
      "grad_norm": 3.1052780151367188,
      "learning_rate": 3.3343027865832077e-06,
      "loss": 4.4384,
      "step": 101800
    },
    {
      "epoch": 53.183716075156575,
      "grad_norm": 3.281440019607544,
      "learning_rate": 3.314924207189868e-06,
      "loss": 4.4012,
      "step": 101900
    },
    {
      "epoch": 53.23590814196242,
      "grad_norm": 3.1816930770874023,
      "learning_rate": 3.295590910643903e-06,
      "loss": 4.4194,
      "step": 102000
    },
    {
      "epoch": 53.28810020876827,
      "grad_norm": 3.0415964126586914,
      "learning_rate": 3.2763030279036913e-06,
      "loss": 4.4051,
      "step": 102100
    },
    {
      "epoch": 53.340292275574114,
      "grad_norm": 3.296527624130249,
      "learning_rate": 3.2570606896199965e-06,
      "loss": 4.4448,
      "step": 102200
    },
    {
      "epoch": 53.39248434237996,
      "grad_norm": 3.3429009914398193,
      "learning_rate": 3.2378640261350726e-06,
      "loss": 4.3948,
      "step": 102300
    },
    {
      "epoch": 53.4446764091858,
      "grad_norm": 3.1063966751098633,
      "learning_rate": 3.218713167481793e-06,
      "loss": 4.4373,
      "step": 102400
    },
    {
      "epoch": 53.49686847599165,
      "grad_norm": 3.1754026412963867,
      "learning_rate": 3.1996082433827557e-06,
      "loss": 4.4408,
      "step": 102500
    },
    {
      "epoch": 53.54906054279749,
      "grad_norm": 3.0735089778900146,
      "learning_rate": 3.1805493832494084e-06,
      "loss": 4.3974,
      "step": 102600
    },
    {
      "epoch": 53.60125260960334,
      "grad_norm": 3.071990966796875,
      "learning_rate": 3.161536716181185e-06,
      "loss": 4.4526,
      "step": 102700
    },
    {
      "epoch": 53.65344467640919,
      "grad_norm": 3.362492799758911,
      "learning_rate": 3.1425703709646092e-06,
      "loss": 4.4784,
      "step": 102800
    },
    {
      "epoch": 53.70563674321503,
      "grad_norm": 3.231144428253174,
      "learning_rate": 3.123650476072435e-06,
      "loss": 4.4648,
      "step": 102900
    },
    {
      "epoch": 53.757828810020875,
      "grad_norm": 3.834980010986328,
      "learning_rate": 3.1047771596627852e-06,
      "loss": 4.3903,
      "step": 103000
    },
    {
      "epoch": 53.81002087682672,
      "grad_norm": 3.2541871070861816,
      "learning_rate": 3.0859505495782603e-06,
      "loss": 4.4042,
      "step": 103100
    },
    {
      "epoch": 53.86221294363257,
      "grad_norm": 3.1534340381622314,
      "learning_rate": 3.0671707733450874e-06,
      "loss": 4.4413,
      "step": 103200
    },
    {
      "epoch": 53.914405010438415,
      "grad_norm": 3.2321739196777344,
      "learning_rate": 3.048437958172262e-06,
      "loss": 4.4891,
      "step": 103300
    },
    {
      "epoch": 53.96659707724426,
      "grad_norm": 3.2375853061676025,
      "learning_rate": 3.029752230950672e-06,
      "loss": 4.4655,
      "step": 103400
    },
    {
      "epoch": 54.0187891440501,
      "grad_norm": 3.265249013900757,
      "learning_rate": 3.011113718252244e-06,
      "loss": 4.4454,
      "step": 103500
    },
    {
      "epoch": 54.07098121085595,
      "grad_norm": 3.2073631286621094,
      "learning_rate": 2.9925225463290863e-06,
      "loss": 4.3922,
      "step": 103600
    },
    {
      "epoch": 54.1231732776618,
      "grad_norm": 3.3891186714172363,
      "learning_rate": 2.9739788411126368e-06,
      "loss": 4.4575,
      "step": 103700
    },
    {
      "epoch": 54.17536534446764,
      "grad_norm": 3.320523500442505,
      "learning_rate": 2.9554827282128153e-06,
      "loss": 4.4345,
      "step": 103800
    },
    {
      "epoch": 54.22755741127349,
      "grad_norm": 3.2545557022094727,
      "learning_rate": 2.9370343329171415e-06,
      "loss": 4.4326,
      "step": 103900
    },
    {
      "epoch": 54.27974947807933,
      "grad_norm": 3.0799663066864014,
      "learning_rate": 2.918633780189931e-06,
      "loss": 4.3642,
      "step": 104000
    },
    {
      "epoch": 54.331941544885176,
      "grad_norm": 3.172189235687256,
      "learning_rate": 2.900281194671418e-06,
      "loss": 4.4513,
      "step": 104100
    },
    {
      "epoch": 54.38413361169102,
      "grad_norm": 3.2624855041503906,
      "learning_rate": 2.8819767006769185e-06,
      "loss": 4.4794,
      "step": 104200
    },
    {
      "epoch": 54.43632567849687,
      "grad_norm": 3.149940252304077,
      "learning_rate": 2.863720422195997e-06,
      "loss": 4.3978,
      "step": 104300
    },
    {
      "epoch": 54.488517745302715,
      "grad_norm": 3.1629157066345215,
      "learning_rate": 2.8455124828916147e-06,
      "loss": 4.397,
      "step": 104400
    },
    {
      "epoch": 54.54070981210856,
      "grad_norm": 3.322030544281006,
      "learning_rate": 2.8273530060992916e-06,
      "loss": 4.4551,
      "step": 104500
    },
    {
      "epoch": 54.592901878914404,
      "grad_norm": 3.3640971183776855,
      "learning_rate": 2.8092421148262893e-06,
      "loss": 4.3828,
      "step": 104600
    },
    {
      "epoch": 54.64509394572025,
      "grad_norm": 3.499079465866089,
      "learning_rate": 2.791179931750754e-06,
      "loss": 4.3979,
      "step": 104700
    },
    {
      "epoch": 54.6972860125261,
      "grad_norm": 3.278856039047241,
      "learning_rate": 2.773166579220895e-06,
      "loss": 4.4336,
      "step": 104800
    },
    {
      "epoch": 54.74947807933194,
      "grad_norm": 3.2604551315307617,
      "learning_rate": 2.7552021792541673e-06,
      "loss": 4.4933,
      "step": 104900
    },
    {
      "epoch": 54.80167014613779,
      "grad_norm": 3.280587673187256,
      "learning_rate": 2.737286853536424e-06,
      "loss": 4.4422,
      "step": 105000
    },
    {
      "epoch": 54.85386221294363,
      "grad_norm": 3.2755682468414307,
      "learning_rate": 2.7194207234211156e-06,
      "loss": 4.4445,
      "step": 105100
    },
    {
      "epoch": 54.906054279749476,
      "grad_norm": 3.2728302478790283,
      "learning_rate": 2.7016039099284343e-06,
      "loss": 4.4537,
      "step": 105200
    },
    {
      "epoch": 54.95824634655532,
      "grad_norm": 3.2179126739501953,
      "learning_rate": 2.683836533744535e-06,
      "loss": 4.4193,
      "step": 105300
    },
    {
      "epoch": 55.01043841336117,
      "grad_norm": 3.022108554840088,
      "learning_rate": 2.666118715220697e-06,
      "loss": 4.4705,
      "step": 105400
    },
    {
      "epoch": 55.062630480167016,
      "grad_norm": 3.3568358421325684,
      "learning_rate": 2.648450574372491e-06,
      "loss": 4.4134,
      "step": 105500
    },
    {
      "epoch": 55.11482254697286,
      "grad_norm": 3.1804776191711426,
      "learning_rate": 2.630832230879008e-06,
      "loss": 4.3972,
      "step": 105600
    },
    {
      "epoch": 55.167014613778704,
      "grad_norm": 3.387493848800659,
      "learning_rate": 2.6132638040820114e-06,
      "loss": 4.4263,
      "step": 105700
    },
    {
      "epoch": 55.21920668058455,
      "grad_norm": 3.1838877201080322,
      "learning_rate": 2.5957454129851433e-06,
      "loss": 4.44,
      "step": 105800
    },
    {
      "epoch": 55.2713987473904,
      "grad_norm": 3.214805841445923,
      "learning_rate": 2.578277176253128e-06,
      "loss": 4.3721,
      "step": 105900
    },
    {
      "epoch": 55.323590814196244,
      "grad_norm": 3.226064682006836,
      "learning_rate": 2.560859212210948e-06,
      "loss": 4.4561,
      "step": 106000
    },
    {
      "epoch": 55.37578288100209,
      "grad_norm": 3.301227331161499,
      "learning_rate": 2.543491638843053e-06,
      "loss": 4.3775,
      "step": 106100
    },
    {
      "epoch": 55.42797494780793,
      "grad_norm": 3.257997989654541,
      "learning_rate": 2.526174573792568e-06,
      "loss": 4.4036,
      "step": 106200
    },
    {
      "epoch": 55.48016701461378,
      "grad_norm": 3.2451565265655518,
      "learning_rate": 2.508908134360478e-06,
      "loss": 4.4274,
      "step": 106300
    },
    {
      "epoch": 55.53235908141962,
      "grad_norm": 3.2217695713043213,
      "learning_rate": 2.4916924375048602e-06,
      "loss": 4.4657,
      "step": 106400
    },
    {
      "epoch": 55.58455114822547,
      "grad_norm": 3.0776236057281494,
      "learning_rate": 2.474527599840052e-06,
      "loss": 4.428,
      "step": 106500
    },
    {
      "epoch": 55.636743215031316,
      "grad_norm": 3.103792905807495,
      "learning_rate": 2.4574137376359055e-06,
      "loss": 4.4402,
      "step": 106600
    },
    {
      "epoch": 55.68893528183716,
      "grad_norm": 3.1096203327178955,
      "learning_rate": 2.4403509668169745e-06,
      "loss": 4.4568,
      "step": 106700
    },
    {
      "epoch": 55.741127348643005,
      "grad_norm": 3.2854485511779785,
      "learning_rate": 2.423339402961723e-06,
      "loss": 4.4329,
      "step": 106800
    },
    {
      "epoch": 55.79331941544885,
      "grad_norm": 3.3195159435272217,
      "learning_rate": 2.4063791613017683e-06,
      "loss": 4.4244,
      "step": 106900
    },
    {
      "epoch": 55.8455114822547,
      "grad_norm": 3.4947519302368164,
      "learning_rate": 2.3894703567210777e-06,
      "loss": 4.4386,
      "step": 107000
    },
    {
      "epoch": 55.897703549060545,
      "grad_norm": 3.211683511734009,
      "learning_rate": 2.3726131037551923e-06,
      "loss": 4.4478,
      "step": 107100
    },
    {
      "epoch": 55.94989561586639,
      "grad_norm": 3.1259520053863525,
      "learning_rate": 2.3558075165904727e-06,
      "loss": 4.4278,
      "step": 107200
    },
    {
      "epoch": 56.00208768267223,
      "grad_norm": 3.1742148399353027,
      "learning_rate": 2.3390537090632927e-06,
      "loss": 4.3879,
      "step": 107300
    },
    {
      "epoch": 56.05427974947808,
      "grad_norm": 3.0407867431640625,
      "learning_rate": 2.322351794659299e-06,
      "loss": 4.3716,
      "step": 107400
    },
    {
      "epoch": 56.10647181628392,
      "grad_norm": 3.218675374984741,
      "learning_rate": 2.30570188651262e-06,
      "loss": 4.3776,
      "step": 107500
    },
    {
      "epoch": 56.15866388308977,
      "grad_norm": 3.0414319038391113,
      "learning_rate": 2.2891040974051094e-06,
      "loss": 4.325,
      "step": 107600
    },
    {
      "epoch": 56.21085594989562,
      "grad_norm": 3.282240152359009,
      "learning_rate": 2.272558539765588e-06,
      "loss": 4.4066,
      "step": 107700
    },
    {
      "epoch": 56.26304801670146,
      "grad_norm": 3.224905490875244,
      "learning_rate": 2.25606532566906e-06,
      "loss": 4.4066,
      "step": 107800
    },
    {
      "epoch": 56.315240083507305,
      "grad_norm": 3.111224412918091,
      "learning_rate": 2.239624566835984e-06,
      "loss": 4.4525,
      "step": 107900
    },
    {
      "epoch": 56.36743215031315,
      "grad_norm": 3.133042097091675,
      "learning_rate": 2.223236374631502e-06,
      "loss": 4.3686,
      "step": 108000
    },
    {
      "epoch": 56.419624217119,
      "grad_norm": 3.0504043102264404,
      "learning_rate": 2.2069008600646648e-06,
      "loss": 4.4043,
      "step": 108100
    },
    {
      "epoch": 56.471816283924845,
      "grad_norm": 3.3489105701446533,
      "learning_rate": 2.190618133787723e-06,
      "loss": 4.4207,
      "step": 108200
    },
    {
      "epoch": 56.52400835073069,
      "grad_norm": 3.097414970397949,
      "learning_rate": 2.1743883060953395e-06,
      "loss": 4.4706,
      "step": 108300
    },
    {
      "epoch": 56.57620041753653,
      "grad_norm": 3.250032663345337,
      "learning_rate": 2.158211486923859e-06,
      "loss": 4.4169,
      "step": 108400
    },
    {
      "epoch": 56.62839248434238,
      "grad_norm": 3.3911478519439697,
      "learning_rate": 2.1420877858505683e-06,
      "loss": 4.4163,
      "step": 108500
    },
    {
      "epoch": 56.68058455114823,
      "grad_norm": 3.4038777351379395,
      "learning_rate": 2.1260173120929375e-06,
      "loss": 4.4359,
      "step": 108600
    },
    {
      "epoch": 56.73277661795407,
      "grad_norm": 3.326340675354004,
      "learning_rate": 2.110000174507898e-06,
      "loss": 4.4087,
      "step": 108700
    },
    {
      "epoch": 56.78496868475992,
      "grad_norm": 3.0976438522338867,
      "learning_rate": 2.094036481591091e-06,
      "loss": 4.4382,
      "step": 108800
    },
    {
      "epoch": 56.83716075156576,
      "grad_norm": 3.089731454849243,
      "learning_rate": 2.0781263414761365e-06,
      "loss": 4.4675,
      "step": 108900
    },
    {
      "epoch": 56.889352818371606,
      "grad_norm": 3.2069602012634277,
      "learning_rate": 2.062269861933913e-06,
      "loss": 4.4687,
      "step": 109000
    },
    {
      "epoch": 56.94154488517745,
      "grad_norm": 3.1428158283233643,
      "learning_rate": 2.046467150371807e-06,
      "loss": 4.4621,
      "step": 109100
    },
    {
      "epoch": 56.9937369519833,
      "grad_norm": 3.3531832695007324,
      "learning_rate": 2.0307183138329965e-06,
      "loss": 4.4238,
      "step": 109200
    },
    {
      "epoch": 57.045929018789145,
      "grad_norm": 3.1558687686920166,
      "learning_rate": 2.0150234589957363e-06,
      "loss": 4.3534,
      "step": 109300
    },
    {
      "epoch": 57.09812108559499,
      "grad_norm": 3.1419827938079834,
      "learning_rate": 1.9993826921726043e-06,
      "loss": 4.3255,
      "step": 109400
    },
    {
      "epoch": 57.150313152400834,
      "grad_norm": 3.2217345237731934,
      "learning_rate": 1.9837961193098175e-06,
      "loss": 4.4093,
      "step": 109500
    },
    {
      "epoch": 57.20250521920668,
      "grad_norm": 3.3096816539764404,
      "learning_rate": 1.968263845986499e-06,
      "loss": 4.4089,
      "step": 109600
    },
    {
      "epoch": 57.25469728601253,
      "grad_norm": 3.279534101486206,
      "learning_rate": 1.9527859774139446e-06,
      "loss": 4.4243,
      "step": 109700
    },
    {
      "epoch": 57.306889352818374,
      "grad_norm": 3.477043867111206,
      "learning_rate": 1.937362618434948e-06,
      "loss": 4.4249,
      "step": 109800
    },
    {
      "epoch": 57.35908141962422,
      "grad_norm": 3.0834524631500244,
      "learning_rate": 1.9219938735230535e-06,
      "loss": 4.4167,
      "step": 109900
    },
    {
      "epoch": 57.41127348643006,
      "grad_norm": 3.2862627506256104,
      "learning_rate": 1.9066798467818792e-06,
      "loss": 4.4086,
      "step": 110000
    },
    {
      "epoch": 57.463465553235906,
      "grad_norm": 3.1387505531311035,
      "learning_rate": 1.891420641944387e-06,
      "loss": 4.4417,
      "step": 110100
    },
    {
      "epoch": 57.51565762004175,
      "grad_norm": 3.2285757064819336,
      "learning_rate": 1.8762163623721918e-06,
      "loss": 4.4594,
      "step": 110200
    },
    {
      "epoch": 57.5678496868476,
      "grad_norm": 3.190354585647583,
      "learning_rate": 1.8610671110548662e-06,
      "loss": 4.4811,
      "step": 110300
    },
    {
      "epoch": 57.620041753653446,
      "grad_norm": 3.2844088077545166,
      "learning_rate": 1.8459729906092306e-06,
      "loss": 4.3653,
      "step": 110400
    },
    {
      "epoch": 57.67223382045929,
      "grad_norm": 3.2633607387542725,
      "learning_rate": 1.8309341032786644e-06,
      "loss": 4.3574,
      "step": 110500
    },
    {
      "epoch": 57.724425887265134,
      "grad_norm": 3.2478981018066406,
      "learning_rate": 1.8159505509324205e-06,
      "loss": 4.4446,
      "step": 110600
    },
    {
      "epoch": 57.77661795407098,
      "grad_norm": 3.457202672958374,
      "learning_rate": 1.8010224350649153e-06,
      "loss": 4.431,
      "step": 110700
    },
    {
      "epoch": 57.82881002087683,
      "grad_norm": 3.1657984256744385,
      "learning_rate": 1.786149856795063e-06,
      "loss": 4.4033,
      "step": 110800
    },
    {
      "epoch": 57.881002087682674,
      "grad_norm": 3.2258048057556152,
      "learning_rate": 1.7713329168655845e-06,
      "loss": 4.4406,
      "step": 110900
    },
    {
      "epoch": 57.93319415448852,
      "grad_norm": 3.3820555210113525,
      "learning_rate": 1.7565717156423134e-06,
      "loss": 4.3631,
      "step": 111000
    },
    {
      "epoch": 57.98538622129436,
      "grad_norm": 3.294011116027832,
      "learning_rate": 1.7418663531135283e-06,
      "loss": 4.414,
      "step": 111100
    },
    {
      "epoch": 58.03757828810021,
      "grad_norm": 3.2754335403442383,
      "learning_rate": 1.7272169288892704e-06,
      "loss": 4.4455,
      "step": 111200
    },
    {
      "epoch": 58.08977035490605,
      "grad_norm": 3.205902099609375,
      "learning_rate": 1.7126235422006765e-06,
      "loss": 4.3702,
      "step": 111300
    },
    {
      "epoch": 58.1419624217119,
      "grad_norm": 3.312197208404541,
      "learning_rate": 1.6980862918992947e-06,
      "loss": 4.3335,
      "step": 111400
    },
    {
      "epoch": 58.194154488517746,
      "grad_norm": 3.1924710273742676,
      "learning_rate": 1.6836052764564192e-06,
      "loss": 4.4349,
      "step": 111500
    },
    {
      "epoch": 58.24634655532359,
      "grad_norm": 3.269044876098633,
      "learning_rate": 1.6691805939624328e-06,
      "loss": 4.3728,
      "step": 111600
    },
    {
      "epoch": 58.298538622129435,
      "grad_norm": 3.3123700618743896,
      "learning_rate": 1.6548123421261276e-06,
      "loss": 4.3774,
      "step": 111700
    },
    {
      "epoch": 58.35073068893528,
      "grad_norm": 3.0455307960510254,
      "learning_rate": 1.6405006182740502e-06,
      "loss": 4.3699,
      "step": 111800
    },
    {
      "epoch": 58.40292275574113,
      "grad_norm": 4.37297248840332,
      "learning_rate": 1.6262455193498515e-06,
      "loss": 4.3882,
      "step": 111900
    },
    {
      "epoch": 58.455114822546975,
      "grad_norm": 3.249516725540161,
      "learning_rate": 1.6120471419136042e-06,
      "loss": 4.4045,
      "step": 112000
    },
    {
      "epoch": 58.50730688935282,
      "grad_norm": 3.17067551612854,
      "learning_rate": 1.5979055821411804e-06,
      "loss": 4.4513,
      "step": 112100
    },
    {
      "epoch": 58.55949895615866,
      "grad_norm": 3.1830646991729736,
      "learning_rate": 1.5838209358235812e-06,
      "loss": 4.4899,
      "step": 112200
    },
    {
      "epoch": 58.61169102296451,
      "grad_norm": 3.1230437755584717,
      "learning_rate": 1.5697932983662878e-06,
      "loss": 4.4682,
      "step": 112300
    },
    {
      "epoch": 58.66388308977035,
      "grad_norm": 3.1956868171691895,
      "learning_rate": 1.5558227647886226e-06,
      "loss": 4.3823,
      "step": 112400
    },
    {
      "epoch": 58.7160751565762,
      "grad_norm": 3.331293821334839,
      "learning_rate": 1.5419094297230985e-06,
      "loss": 4.3935,
      "step": 112500
    },
    {
      "epoch": 58.76826722338205,
      "grad_norm": 3.3238327503204346,
      "learning_rate": 1.5280533874147885e-06,
      "loss": 4.4348,
      "step": 112600
    },
    {
      "epoch": 58.82045929018789,
      "grad_norm": 3.228762626647949,
      "learning_rate": 1.5142547317206745e-06,
      "loss": 4.4146,
      "step": 112700
    },
    {
      "epoch": 58.872651356993735,
      "grad_norm": 2.9994544982910156,
      "learning_rate": 1.5005135561090167e-06,
      "loss": 4.4086,
      "step": 112800
    },
    {
      "epoch": 58.92484342379958,
      "grad_norm": 3.1779556274414062,
      "learning_rate": 1.4868299536587272e-06,
      "loss": 4.43,
      "step": 112900
    },
    {
      "epoch": 58.97703549060543,
      "grad_norm": 3.1306586265563965,
      "learning_rate": 1.4732040170587292e-06,
      "loss": 4.4303,
      "step": 113000
    },
    {
      "epoch": 59.029227557411275,
      "grad_norm": 3.0221331119537354,
      "learning_rate": 1.4596358386073317e-06,
      "loss": 4.3581,
      "step": 113100
    },
    {
      "epoch": 59.08141962421712,
      "grad_norm": 3.374234437942505,
      "learning_rate": 1.4461255102116113e-06,
      "loss": 4.4316,
      "step": 113200
    },
    {
      "epoch": 59.13361169102296,
      "grad_norm": 3.449317693710327,
      "learning_rate": 1.4326731233867807e-06,
      "loss": 4.3866,
      "step": 113300
    },
    {
      "epoch": 59.18580375782881,
      "grad_norm": 3.133495807647705,
      "learning_rate": 1.4192787692555709e-06,
      "loss": 4.3492,
      "step": 113400
    },
    {
      "epoch": 59.23799582463466,
      "grad_norm": 3.16495943069458,
      "learning_rate": 1.4059425385476221e-06,
      "loss": 4.4107,
      "step": 113500
    },
    {
      "epoch": 59.2901878914405,
      "grad_norm": 3.173818349838257,
      "learning_rate": 1.3926645215988565e-06,
      "loss": 4.3409,
      "step": 113600
    },
    {
      "epoch": 59.34237995824635,
      "grad_norm": 3.0703349113464355,
      "learning_rate": 1.3794448083508726e-06,
      "loss": 4.397,
      "step": 113700
    },
    {
      "epoch": 59.39457202505219,
      "grad_norm": 3.154353618621826,
      "learning_rate": 1.3662834883503406e-06,
      "loss": 4.4367,
      "step": 113800
    },
    {
      "epoch": 59.446764091858036,
      "grad_norm": 3.258897542953491,
      "learning_rate": 1.353180650748388e-06,
      "loss": 4.4731,
      "step": 113900
    },
    {
      "epoch": 59.49895615866388,
      "grad_norm": 3.2224416732788086,
      "learning_rate": 1.340136384299996e-06,
      "loss": 4.448,
      "step": 114000
    },
    {
      "epoch": 59.55114822546973,
      "grad_norm": 3.1506941318511963,
      "learning_rate": 1.327150777363404e-06,
      "loss": 4.4005,
      "step": 114100
    },
    {
      "epoch": 59.603340292275576,
      "grad_norm": 3.10516357421875,
      "learning_rate": 1.314223917899512e-06,
      "loss": 4.3742,
      "step": 114200
    },
    {
      "epoch": 59.65553235908142,
      "grad_norm": 3.0689477920532227,
      "learning_rate": 1.3013558934712744e-06,
      "loss": 4.4257,
      "step": 114300
    },
    {
      "epoch": 59.707724425887264,
      "grad_norm": 3.2605538368225098,
      "learning_rate": 1.2885467912431137e-06,
      "loss": 4.4395,
      "step": 114400
    },
    {
      "epoch": 59.75991649269311,
      "grad_norm": 3.199510335922241,
      "learning_rate": 1.2757966979803372e-06,
      "loss": 4.3937,
      "step": 114500
    },
    {
      "epoch": 59.81210855949896,
      "grad_norm": 3.047769069671631,
      "learning_rate": 1.2631057000485337e-06,
      "loss": 4.3954,
      "step": 114600
    },
    {
      "epoch": 59.864300626304804,
      "grad_norm": 3.295541763305664,
      "learning_rate": 1.2504738834129971e-06,
      "loss": 4.4215,
      "step": 114700
    },
    {
      "epoch": 59.91649269311065,
      "grad_norm": 3.1387600898742676,
      "learning_rate": 1.2379013336381518e-06,
      "loss": 4.3535,
      "step": 114800
    },
    {
      "epoch": 59.96868475991649,
      "grad_norm": 3.22890305519104,
      "learning_rate": 1.2253881358869557e-06,
      "loss": 4.4299,
      "step": 114900
    },
    {
      "epoch": 60.020876826722336,
      "grad_norm": 3.2740581035614014,
      "learning_rate": 1.2129343749203358e-06,
      "loss": 4.4301,
      "step": 115000
    },
    {
      "epoch": 60.07306889352818,
      "grad_norm": 3.245979070663452,
      "learning_rate": 1.2005401350966151e-06,
      "loss": 4.3546,
      "step": 115100
    },
    {
      "epoch": 60.12526096033403,
      "grad_norm": 3.213798999786377,
      "learning_rate": 1.1882055003709292e-06,
      "loss": 4.4086,
      "step": 115200
    },
    {
      "epoch": 60.177453027139876,
      "grad_norm": 3.149244785308838,
      "learning_rate": 1.1759305542946708e-06,
      "loss": 4.4013,
      "step": 115300
    },
    {
      "epoch": 60.22964509394572,
      "grad_norm": 2.961613893508911,
      "learning_rate": 1.1637153800149138e-06,
      "loss": 4.4112,
      "step": 115400
    },
    {
      "epoch": 60.281837160751564,
      "grad_norm": 3.0923314094543457,
      "learning_rate": 1.151560060273862e-06,
      "loss": 4.4012,
      "step": 115500
    },
    {
      "epoch": 60.33402922755741,
      "grad_norm": 3.3207435607910156,
      "learning_rate": 1.139464677408274e-06,
      "loss": 4.3737,
      "step": 115600
    },
    {
      "epoch": 60.38622129436326,
      "grad_norm": 3.1329660415649414,
      "learning_rate": 1.1274293133489133e-06,
      "loss": 4.4373,
      "step": 115700
    },
    {
      "epoch": 60.438413361169104,
      "grad_norm": 3.2257823944091797,
      "learning_rate": 1.1154540496199996e-06,
      "loss": 4.4283,
      "step": 115800
    },
    {
      "epoch": 60.49060542797495,
      "grad_norm": 3.2347984313964844,
      "learning_rate": 1.1035389673386399e-06,
      "loss": 4.4091,
      "step": 115900
    },
    {
      "epoch": 60.54279749478079,
      "grad_norm": 3.327199935913086,
      "learning_rate": 1.0916841472142937e-06,
      "loss": 4.3872,
      "step": 116000
    },
    {
      "epoch": 60.59498956158664,
      "grad_norm": 3.185925006866455,
      "learning_rate": 1.0798896695482242e-06,
      "loss": 4.4062,
      "step": 116100
    },
    {
      "epoch": 60.64718162839248,
      "grad_norm": 3.046870708465576,
      "learning_rate": 1.0681556142329475e-06,
      "loss": 4.3815,
      "step": 116200
    },
    {
      "epoch": 60.69937369519833,
      "grad_norm": 3.1505839824676514,
      "learning_rate": 1.0564820607516934e-06,
      "loss": 4.3973,
      "step": 116300
    },
    {
      "epoch": 60.75156576200418,
      "grad_norm": 3.4194107055664062,
      "learning_rate": 1.0448690881778767e-06,
      "loss": 4.3435,
      "step": 116400
    },
    {
      "epoch": 60.80375782881002,
      "grad_norm": 3.316520929336548,
      "learning_rate": 1.0333167751745465e-06,
      "loss": 4.4211,
      "step": 116500
    },
    {
      "epoch": 60.855949895615865,
      "grad_norm": 3.2412967681884766,
      "learning_rate": 1.0218251999938656e-06,
      "loss": 4.3812,
      "step": 116600
    },
    {
      "epoch": 60.90814196242171,
      "grad_norm": 3.191735029220581,
      "learning_rate": 1.0103944404765698e-06,
      "loss": 4.3976,
      "step": 116700
    },
    {
      "epoch": 60.96033402922756,
      "grad_norm": 3.2778375148773193,
      "learning_rate": 9.990245740514548e-07,
      "loss": 4.3672,
      "step": 116800
    },
    {
      "epoch": 61.012526096033405,
      "grad_norm": 3.383589744567871,
      "learning_rate": 9.877156777348374e-07,
      "loss": 4.4687,
      "step": 116900
    },
    {
      "epoch": 61.06471816283925,
      "grad_norm": 3.1808881759643555,
      "learning_rate": 9.7646782813004e-07,
      "loss": 4.3623,
      "step": 117000
    },
    {
      "epoch": 61.11691022964509,
      "grad_norm": 3.2184677124023438,
      "learning_rate": 9.652811014268759e-07,
      "loss": 4.373,
      "step": 117100
    },
    {
      "epoch": 61.16910229645094,
      "grad_norm": 3.0128891468048096,
      "learning_rate": 9.541555734011243e-07,
      "loss": 4.3856,
      "step": 117200
    },
    {
      "epoch": 61.22129436325679,
      "grad_norm": 3.4021241664886475,
      "learning_rate": 9.430913194140212e-07,
      "loss": 4.3794,
      "step": 117300
    },
    {
      "epoch": 61.27348643006263,
      "grad_norm": 3.2642099857330322,
      "learning_rate": 9.320884144117559e-07,
      "loss": 4.3956,
      "step": 117400
    },
    {
      "epoch": 61.32567849686848,
      "grad_norm": 3.1024746894836426,
      "learning_rate": 9.211469329249479e-07,
      "loss": 4.3639,
      "step": 117500
    },
    {
      "epoch": 61.37787056367432,
      "grad_norm": 10.847796440124512,
      "learning_rate": 9.102669490681559e-07,
      "loss": 4.3752,
      "step": 117600
    },
    {
      "epoch": 61.430062630480165,
      "grad_norm": 3.2028162479400635,
      "learning_rate": 8.994485365393712e-07,
      "loss": 4.382,
      "step": 117700
    },
    {
      "epoch": 61.48225469728601,
      "grad_norm": 3.2717087268829346,
      "learning_rate": 8.886917686195173e-07,
      "loss": 4.416,
      "step": 117800
    },
    {
      "epoch": 61.53444676409186,
      "grad_norm": 3.2407050132751465,
      "learning_rate": 8.779967181719484e-07,
      "loss": 4.4362,
      "step": 117900
    },
    {
      "epoch": 61.586638830897705,
      "grad_norm": 3.211585283279419,
      "learning_rate": 8.673634576419732e-07,
      "loss": 4.3952,
      "step": 118000
    },
    {
      "epoch": 61.63883089770355,
      "grad_norm": 3.065802812576294,
      "learning_rate": 8.567920590563417e-07,
      "loss": 4.4278,
      "step": 118100
    },
    {
      "epoch": 61.69102296450939,
      "grad_norm": 3.0537145137786865,
      "learning_rate": 8.462825940227793e-07,
      "loss": 4.4011,
      "step": 118200
    },
    {
      "epoch": 61.74321503131524,
      "grad_norm": 3.1845438480377197,
      "learning_rate": 8.358351337294746e-07,
      "loss": 4.4376,
      "step": 118300
    },
    {
      "epoch": 61.79540709812109,
      "grad_norm": 3.334476947784424,
      "learning_rate": 8.254497489446289e-07,
      "loss": 4.4417,
      "step": 118400
    },
    {
      "epoch": 61.84759916492693,
      "grad_norm": 3.3680574893951416,
      "learning_rate": 8.151265100159589e-07,
      "loss": 4.3279,
      "step": 118500
    },
    {
      "epoch": 61.89979123173278,
      "grad_norm": 3.110633373260498,
      "learning_rate": 8.0486548687021e-07,
      "loss": 4.3791,
      "step": 118600
    },
    {
      "epoch": 61.95198329853862,
      "grad_norm": 3.3188910484313965,
      "learning_rate": 7.946667490127114e-07,
      "loss": 4.4092,
      "step": 118700
    },
    {
      "epoch": 62.004175365344466,
      "grad_norm": 3.1668291091918945,
      "learning_rate": 7.845303655268787e-07,
      "loss": 4.3499,
      "step": 118800
    },
    {
      "epoch": 62.05636743215031,
      "grad_norm": 3.267076015472412,
      "learning_rate": 7.744564050737579e-07,
      "loss": 4.3344,
      "step": 118900
    },
    {
      "epoch": 62.10855949895616,
      "grad_norm": 3.2428040504455566,
      "learning_rate": 7.644449358915628e-07,
      "loss": 4.3281,
      "step": 119000
    },
    {
      "epoch": 62.160751565762006,
      "grad_norm": 3.216313362121582,
      "learning_rate": 7.544960257952072e-07,
      "loss": 4.4485,
      "step": 119100
    },
    {
      "epoch": 62.21294363256785,
      "grad_norm": 3.2465121746063232,
      "learning_rate": 7.446097421758414e-07,
      "loss": 4.3914,
      "step": 119200
    },
    {
      "epoch": 62.265135699373694,
      "grad_norm": 3.2375411987304688,
      "learning_rate": 7.347861520004096e-07,
      "loss": 4.4016,
      "step": 119300
    },
    {
      "epoch": 62.31732776617954,
      "grad_norm": 3.319035768508911,
      "learning_rate": 7.250253218111813e-07,
      "loss": 4.4269,
      "step": 119400
    },
    {
      "epoch": 62.36951983298539,
      "grad_norm": 3.1512606143951416,
      "learning_rate": 7.153273177253151e-07,
      "loss": 4.4163,
      "step": 119500
    },
    {
      "epoch": 62.421711899791234,
      "grad_norm": 3.186035633087158,
      "learning_rate": 7.056922054343929e-07,
      "loss": 4.4144,
      "step": 119600
    },
    {
      "epoch": 62.47390396659708,
      "grad_norm": 3.2413620948791504,
      "learning_rate": 6.961200502039911e-07,
      "loss": 4.3908,
      "step": 119700
    },
    {
      "epoch": 62.52609603340292,
      "grad_norm": 3.295257091522217,
      "learning_rate": 6.86610916873236e-07,
      "loss": 4.4015,
      "step": 119800
    },
    {
      "epoch": 62.578288100208766,
      "grad_norm": 3.19771409034729,
      "learning_rate": 6.771648698543509e-07,
      "loss": 4.4232,
      "step": 119900
    },
    {
      "epoch": 62.63048016701461,
      "grad_norm": 3.173701286315918,
      "learning_rate": 6.677819731322388e-07,
      "loss": 4.3856,
      "step": 120000
    },
    {
      "epoch": 62.68267223382046,
      "grad_norm": 3.268415689468384,
      "learning_rate": 6.584622902640369e-07,
      "loss": 4.4094,
      "step": 120100
    },
    {
      "epoch": 62.734864300626306,
      "grad_norm": 3.352250099182129,
      "learning_rate": 6.492058843786875e-07,
      "loss": 4.3893,
      "step": 120200
    },
    {
      "epoch": 62.78705636743215,
      "grad_norm": 3.170158624649048,
      "learning_rate": 6.400128181765175e-07,
      "loss": 4.3257,
      "step": 120300
    },
    {
      "epoch": 62.839248434237994,
      "grad_norm": 3.1082255840301514,
      "learning_rate": 6.308831539288019e-07,
      "loss": 4.4197,
      "step": 120400
    },
    {
      "epoch": 62.89144050104384,
      "grad_norm": 2.9474034309387207,
      "learning_rate": 6.218169534773522e-07,
      "loss": 4.393,
      "step": 120500
    },
    {
      "epoch": 62.94363256784969,
      "grad_norm": 3.1827049255371094,
      "learning_rate": 6.128142782340951e-07,
      "loss": 4.363,
      "step": 120600
    },
    {
      "epoch": 62.995824634655534,
      "grad_norm": 3.3156471252441406,
      "learning_rate": 6.038751891806493e-07,
      "loss": 4.4388,
      "step": 120700
    },
    {
      "epoch": 63.04801670146138,
      "grad_norm": 3.354933738708496,
      "learning_rate": 5.94999746867928e-07,
      "loss": 4.3905,
      "step": 120800
    },
    {
      "epoch": 63.10020876826722,
      "grad_norm": 3.3038523197174072,
      "learning_rate": 5.861880114157071e-07,
      "loss": 4.3962,
      "step": 120900
    },
    {
      "epoch": 63.15240083507307,
      "grad_norm": 3.066608190536499,
      "learning_rate": 5.774400425122395e-07,
      "loss": 4.4046,
      "step": 121000
    },
    {
      "epoch": 63.20459290187891,
      "grad_norm": 3.8322110176086426,
      "learning_rate": 5.687558994138387e-07,
      "loss": 4.4363,
      "step": 121100
    },
    {
      "epoch": 63.25678496868476,
      "grad_norm": 3.237347364425659,
      "learning_rate": 5.601356409444747e-07,
      "loss": 4.4095,
      "step": 121200
    },
    {
      "epoch": 63.30897703549061,
      "grad_norm": 3.271240234375,
      "learning_rate": 5.515793254953893e-07,
      "loss": 4.4305,
      "step": 121300
    },
    {
      "epoch": 63.36116910229645,
      "grad_norm": 3.2159111499786377,
      "learning_rate": 5.430870110246866e-07,
      "loss": 4.3963,
      "step": 121400
    },
    {
      "epoch": 63.413361169102295,
      "grad_norm": 3.363537073135376,
      "learning_rate": 5.346587550569449e-07,
      "loss": 4.3758,
      "step": 121500
    },
    {
      "epoch": 63.46555323590814,
      "grad_norm": 3.2479774951934814,
      "learning_rate": 5.26294614682833e-07,
      "loss": 4.4016,
      "step": 121600
    },
    {
      "epoch": 63.51774530271399,
      "grad_norm": 3.2945213317871094,
      "learning_rate": 5.179946465587127e-07,
      "loss": 4.4266,
      "step": 121700
    },
    {
      "epoch": 63.569937369519835,
      "grad_norm": 3.2565982341766357,
      "learning_rate": 5.097589069062658e-07,
      "loss": 4.2569,
      "step": 121800
    },
    {
      "epoch": 63.62212943632568,
      "grad_norm": 3.3159611225128174,
      "learning_rate": 5.015874515121066e-07,
      "loss": 4.4154,
      "step": 121900
    },
    {
      "epoch": 63.67432150313152,
      "grad_norm": 3.038914442062378,
      "learning_rate": 4.934803357274031e-07,
      "loss": 4.4454,
      "step": 122000
    },
    {
      "epoch": 63.72651356993737,
      "grad_norm": 3.2616660594940186,
      "learning_rate": 4.854376144675088e-07,
      "loss": 4.2962,
      "step": 122100
    },
    {
      "epoch": 63.77870563674321,
      "grad_norm": 3.262991428375244,
      "learning_rate": 4.774593422115836e-07,
      "loss": 4.3778,
      "step": 122200
    },
    {
      "epoch": 63.83089770354906,
      "grad_norm": 3.2020227909088135,
      "learning_rate": 4.69545573002228e-07,
      "loss": 4.3381,
      "step": 122300
    },
    {
      "epoch": 63.88308977035491,
      "grad_norm": 3.34002423286438,
      "learning_rate": 4.6169636044511967e-07,
      "loss": 4.4033,
      "step": 122400
    },
    {
      "epoch": 63.93528183716075,
      "grad_norm": 3.355700969696045,
      "learning_rate": 4.539117577086416e-07,
      "loss": 4.3254,
      "step": 122500
    },
    {
      "epoch": 63.987473903966595,
      "grad_norm": 3.287605047225952,
      "learning_rate": 4.4619181752353357e-07,
      "loss": 4.3694,
      "step": 122600
    },
    {
      "epoch": 64.03966597077245,
      "grad_norm": 3.0801992416381836,
      "learning_rate": 4.3853659218252907e-07,
      "loss": 4.4066,
      "step": 122700
    },
    {
      "epoch": 64.09185803757829,
      "grad_norm": 3.4844119548797607,
      "learning_rate": 4.309461335399956e-07,
      "loss": 4.3497,
      "step": 122800
    },
    {
      "epoch": 64.14405010438414,
      "grad_norm": 3.3279507160186768,
      "learning_rate": 4.234204930115937e-07,
      "loss": 4.4269,
      "step": 122900
    },
    {
      "epoch": 64.19624217118998,
      "grad_norm": 3.0521090030670166,
      "learning_rate": 4.159597215739231e-07,
      "loss": 4.4002,
      "step": 123000
    },
    {
      "epoch": 64.24843423799582,
      "grad_norm": 3.2915163040161133,
      "learning_rate": 4.0856386976417804e-07,
      "loss": 4.3458,
      "step": 123100
    },
    {
      "epoch": 64.30062630480167,
      "grad_norm": 3.2424261569976807,
      "learning_rate": 4.01232987679806e-07,
      "loss": 4.3349,
      "step": 123200
    },
    {
      "epoch": 64.35281837160751,
      "grad_norm": 3.257749557495117,
      "learning_rate": 3.9396712497816516e-07,
      "loss": 4.3836,
      "step": 123300
    },
    {
      "epoch": 64.40501043841336,
      "grad_norm": 3.3166801929473877,
      "learning_rate": 3.8676633087619485e-07,
      "loss": 4.3416,
      "step": 123400
    },
    {
      "epoch": 64.4572025052192,
      "grad_norm": 3.069287061691284,
      "learning_rate": 3.796306541500738e-07,
      "loss": 4.3242,
      "step": 123500
    },
    {
      "epoch": 64.50939457202506,
      "grad_norm": 3.3456168174743652,
      "learning_rate": 3.725601431348924e-07,
      "loss": 4.3929,
      "step": 123600
    },
    {
      "epoch": 64.5615866388309,
      "grad_norm": 3.0881423950195312,
      "learning_rate": 3.6555484572433407e-07,
      "loss": 4.3646,
      "step": 123700
    },
    {
      "epoch": 64.61377870563675,
      "grad_norm": 3.1632232666015625,
      "learning_rate": 3.5861480937033454e-07,
      "loss": 4.4418,
      "step": 123800
    },
    {
      "epoch": 64.66597077244259,
      "grad_norm": 3.0936648845672607,
      "learning_rate": 3.5174008108277426e-07,
      "loss": 4.4037,
      "step": 123900
    },
    {
      "epoch": 64.71816283924844,
      "grad_norm": 3.246943473815918,
      "learning_rate": 3.4493070742915525e-07,
      "loss": 4.384,
      "step": 124000
    }
  ],
  "logging_steps": 100,
  "max_steps": 134120,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 70,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": true,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 256,
  "trial_name": null,
  "trial_params": null
}
